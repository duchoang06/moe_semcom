nohup: ignoring input
Training started at 2025-06-20 04:46:01
Training detail: learning rate: 1e-4, weight decay: 5e-3, total epochs: 250, batch size: 128, lambda_moe_lb: 0.0002, seed: 2006
Starting training...

 --- Epoch 1
Task: Classification | Acc: 66.94% | Avg Loss: 0.5921
Task: Reconstruction | Avg Loss: 6.3244 
MoE Balancing Loss: 12281.4925
Mutual Information | Avg Loss: -0.00001
Total Loss: 5.9199
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276507.44
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276279.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276163.62
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276277.44
Time elapsed: 0:07:48.252735

 --- Epoch 2
Task: Classification | Acc: 84.70% | Avg Loss: 0.3627
Task: Reconstruction | Avg Loss: 5.6607 
MoE Balancing Loss: 12270.3683
Mutual Information | Avg Loss: -0.00026
Total Loss: 5.3274
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276993.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276662.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276767.91
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276849.44
Time elapsed: 0:15:36.557963

 --- Epoch 3
Task: Classification | Acc: 85.94% | Avg Loss: 0.3316
Task: Reconstruction | Avg Loss: 5.3332 
MoE Balancing Loss: 12270.0184
Mutual Information | Avg Loss: -0.00100
Total Loss: 5.4235
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278723.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277557.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278177.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278216.50
Time elapsed: 0:23:25.597315

 --- Epoch 4
Task: Classification | Acc: 86.97% | Avg Loss: 0.3098
Task: Reconstruction | Avg Loss: 5.1544 
MoE Balancing Loss: 12270.9184
Mutual Information | Avg Loss: -0.00209
Total Loss: 5.2068
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277344.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276446.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276676.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277281.31
Time elapsed: 0:31:12.162075

 --- Epoch 5
Task: Classification | Acc: 87.38% | Avg Loss: 0.2978
Task: Reconstruction | Avg Loss: 4.9540 
MoE Balancing Loss: 12271.9114
Mutual Information | Avg Loss: -0.00312
Total Loss: 5.0447
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 273714.62
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275092.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274763.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275188.91
Time elapsed: 0:38:58.635195

 --- Epoch 6
Task: Classification | Acc: 88.29% | Avg Loss: 0.2857
Task: Reconstruction | Avg Loss: 4.8110 
MoE Balancing Loss: 12271.9707
Mutual Information | Avg Loss: -0.00377
Total Loss: 4.9865
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277724.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277559.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277160.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277661.16
Time elapsed: 0:46:45.460710

 --- Epoch 7
Task: Classification | Acc: 88.69% | Avg Loss: 0.2708
Task: Reconstruction | Avg Loss: 4.7066 
MoE Balancing Loss: 12272.1289
Mutual Information | Avg Loss: -0.00420
Total Loss: 4.8127
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275124.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276548.47
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276014.41
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276342.59
Time elapsed: 0:54:31.535663

 --- Epoch 8
Task: Classification | Acc: 89.42% | Avg Loss: 0.2571
Task: Reconstruction | Avg Loss: 4.6303 
MoE Balancing Loss: 12272.5062
Mutual Information | Avg Loss: -0.00435
Total Loss: 4.8256
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276792.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277773.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277402.06
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277706.66
Time elapsed: 1:02:17.691884

 --- Epoch 9
Task: Classification | Acc: 90.47% | Avg Loss: 0.2411
Task: Reconstruction | Avg Loss: 4.5501 
MoE Balancing Loss: 12274.0932
Mutual Information | Avg Loss: -0.00447
Total Loss: 4.8180
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275335.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275488.00
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275498.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275563.78
Time elapsed: 1:10:02.345268

 --- Epoch 10
Task: Classification | Acc: 90.75% | Avg Loss: 0.2294
Task: Reconstruction | Avg Loss: 4.4964 
MoE Balancing Loss: 12273.1332
Mutual Information | Avg Loss: -0.00452
Total Loss: 4.6630
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274955.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275442.91
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275544.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275958.69
Time elapsed: 1:17:47.661848
Example 1 ---
Original text: one from the heart.
Reconstructed text: one is the good.
Original IDs: [101, 2028, 2013, 1996, 2540, 1012, 102]
Predicted IDs: [101, 2028, 2003, 1996, 2204, 1012, 102]
BLEU Score: 0.6000
Example 2 ---
Original text: there is nothing outstanding about this film, but it is good enough and will likely be appreciated most by sailors and folks who know their way around a submarine.
Reconstructed text: is the film that in the movie, that ` ` n ', but if to care in the time to is the sense of the n.
Original IDs: [101, 2045, 2003, 2498, 5151, 2055, 2023, 2143, 1010, 2021, 2009, 2003, 2204, 2438, 1998, 2097, 3497, 2022, 12315, 2087, 2011, 11279, 1998, 12455, 2040, 2113, 2037, 2126, 2105, 1037, 6982, 1012, 102]
Predicted IDs: [101, 2003, 1996, 2143, 2008, 1999, 1996, 3185, 1010, 2008, 1036, 1036, 1050, 1005, 1010, 2021, 2065, 2000, 2729, 1999, 1996, 2051, 102, 102, 2000, 2003, 1996, 3168, 1997, 1996, 1050, 1012, 102]
BLEU Score: 0.1931
Example 3 ---
Original text: birthday girl is an amusing joy ride, with some surprisingly violent moments.
Reconstructed text: , that is a untriity and - a a recentness, and
Original IDs: [101, 5798, 2611, 2003, 2019, 19142, 6569, 4536, 1010, 2007, 2070, 10889, 6355, 5312, 1012, 102]
Predicted IDs: [101, 1010, 2008, 2003, 1037, 4895, 18886, 3012, 1998, 1011, 1037, 1037, 3522, 2791, 1010, 1998]
BLEU Score: 0.1411

 --- Epoch 11
Task: Classification | Acc: 91.18% | Avg Loss: 0.2258
Task: Reconstruction | Avg Loss: 4.4388 
MoE Balancing Loss: 12273.2371
Mutual Information | Avg Loss: -0.00458
Total Loss: 4.7211
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276300.38
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276634.44
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276235.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276296.84
Time elapsed: 1:25:33.512348

 --- Epoch 12
Task: Classification | Acc: 91.66% | Avg Loss: 0.2125
Task: Reconstruction | Avg Loss: 4.3814 
MoE Balancing Loss: 12275.0777
Mutual Information | Avg Loss: -0.00453
Total Loss: 4.9005
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276361.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277410.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276735.41
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277534.91
Time elapsed: 1:33:17.837663

 --- Epoch 13
Task: Classification | Acc: 91.99% | Avg Loss: 0.2065
Task: Reconstruction | Avg Loss: 4.3296 
MoE Balancing Loss: 12271.2698
Mutual Information | Avg Loss: -0.00453
Total Loss: 4.5636
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275745.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276939.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277189.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276889.31
Time elapsed: 1:41:01.945762

 --- Epoch 14
Task: Classification | Acc: 92.27% | Avg Loss: 0.1972
Task: Reconstruction | Avg Loss: 4.2537 
MoE Balancing Loss: 12274.6589
Mutual Information | Avg Loss: -0.00459
Total Loss: 4.6075
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276115.16
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275495.03
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275555.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275883.22
Time elapsed: 1:48:47.002743

 --- Epoch 15
Task: Classification | Acc: 92.40% | Avg Loss: 0.1977
Task: Reconstruction | Avg Loss: 4.2151 
MoE Balancing Loss: 12273.9512
Mutual Information | Avg Loss: -0.00460
Total Loss: 4.6190
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276812.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277335.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277124.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277011.41
Time elapsed: 1:56:31.292095

 --- Epoch 16
Task: Classification | Acc: 92.77% | Avg Loss: 0.1877
Task: Reconstruction | Avg Loss: 4.1574 
MoE Balancing Loss: 12273.5898
Mutual Information | Avg Loss: -0.00447
Total Loss: 4.4809
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277460.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276679.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277293.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277322.41
Time elapsed: 2:04:15.162587

 --- Epoch 17
Task: Classification | Acc: 92.99% | Avg Loss: 0.1825
Task: Reconstruction | Avg Loss: 4.1197 
MoE Balancing Loss: 12273.6258
Mutual Information | Avg Loss: -0.00446
Total Loss: 4.5051
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274738.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275191.06
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275534.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275925.91
Time elapsed: 2:11:58.895387

 --- Epoch 18
Task: Classification | Acc: 93.21% | Avg Loss: 0.1784
Task: Reconstruction | Avg Loss: 4.0829 
MoE Balancing Loss: 12272.9499
Mutual Information | Avg Loss: -0.00440
Total Loss: 4.5450
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276291.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275556.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276675.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275863.78
Time elapsed: 2:19:43.632239

 --- Epoch 19
Task: Classification | Acc: 93.54% | Avg Loss: 0.1743
Task: Reconstruction | Avg Loss: 4.0560 
MoE Balancing Loss: 12273.6048
Mutual Information | Avg Loss: -0.00427
Total Loss: 4.3614
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277182.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277532.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277290.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277678.47
Time elapsed: 2:27:27.479304

 --- Epoch 20
Task: Classification | Acc: 93.69% | Avg Loss: 0.1672
Task: Reconstruction | Avg Loss: 4.0058 
MoE Balancing Loss: 12273.2652
Mutual Information | Avg Loss: -0.00439
Total Loss: 4.4572
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275995.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276071.47
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276563.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276215.97
Time elapsed: 2:35:11.016138
Example 1 ---
Original text: passable entertainment, but it's the kind of motion picture that won't make much of a splash when it's released, and will not be remembered long afterwards.
Reconstructed text: funny, that isn't a funny., but a n'of the one of the grand part of the own.. the, and in a the film
Original IDs: [101, 3413, 3085, 4024, 1010, 2021, 2009, 1005, 1055, 1996, 2785, 1997, 4367, 3861, 2008, 24185, 1050, 1005, 1056, 2191, 2172, 1997, 1037, 17624, 2043, 2009, 1005, 1055, 2207, 1010, 1998, 2097, 2025, 2022, 4622, 2146, 5728, 1012, 102]
Predicted IDs: [101, 6057, 1010, 2008, 2003, 1050, 1005, 1056, 102, 1037, 6057, 102, 1012, 1010, 2021, 1037, 1050, 1005, 1997, 1996, 2028, 1997, 1996, 2882, 2112, 1997, 1996, 2219, 1012, 1012, 102, 1996, 1010, 1998, 1999, 1037, 1996, 2143, 102]
BLEU Score: 0.3096
Example 2 ---
Original text: the band's courage in the face of official repression is inspiring, especially for aging hippies ( this one included ).
Reconstructed text: if it's really in the film of the, a bad film in a own cliches and. the
Original IDs: [101, 1996, 2316, 1005, 1055, 8424, 1999, 1996, 2227, 1997, 2880, 22422, 2003, 18988, 1010, 2926, 2005, 12520, 5099, 13046, 1006, 2023, 2028, 2443, 1007, 1012, 102]
Predicted IDs: [101, 2065, 2009, 1005, 1055, 2428, 1999, 1996, 2143, 1997, 1996, 1010, 1037, 2919, 2143, 1999, 1037, 2219, 18856, 17322, 2015, 1998, 102, 1012, 102, 102, 1996]
BLEU Score: 0.3012
Example 3 ---
Original text: it's another video movie photographed like a film, with the bad lighting that's often written off as indie film naturalism.
Reconstructed text: ,'' s a films in the film that'and shooting pleass. the the ` emotionals.
Original IDs: [101, 2009, 1005, 1055, 2178, 2678, 3185, 16164, 2066, 1037, 2143, 1010, 2007, 1996, 2919, 7497, 2008, 1005, 1055, 2411, 2517, 2125, 2004, 10271, 2143, 3019, 2964, 1012, 102]
Predicted IDs: [101, 1010, 1005, 1005, 1055, 1037, 2143, 2015, 1999, 1996, 2143, 2008, 1005, 1998, 5008, 22512, 2015, 1012, 102, 1996, 102, 102, 1996, 1036, 6832, 102, 2015, 1012, 102]
BLEU Score: 0.1948

 --- Epoch 21
Task: Classification | Acc: 94.05% | Avg Loss: 0.1586
Task: Reconstruction | Avg Loss: 3.9653 
MoE Balancing Loss: 12275.0305
Mutual Information | Avg Loss: -0.00443
Total Loss: 4.4257
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276349.16
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275599.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276055.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275966.59
Time elapsed: 2:42:54.311799

 --- Epoch 22
Task: Classification | Acc: 93.90% | Avg Loss: 0.1634
Task: Reconstruction | Avg Loss: 3.9469 
MoE Balancing Loss: 12273.5649
Mutual Information | Avg Loss: -0.00437
Total Loss: 4.4482
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275649.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275939.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275533.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276252.59
Time elapsed: 2:50:38.018655

 --- Epoch 23
Task: Classification | Acc: 93.78% | Avg Loss: 0.1632
Task: Reconstruction | Avg Loss: 3.9000 
MoE Balancing Loss: 12274.3452
Mutual Information | Avg Loss: -0.00432
Total Loss: 4.5886
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277687.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277668.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278262.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277578.00
Time elapsed: 2:58:21.982196

 --- Epoch 24
Task: Classification | Acc: 94.21% | Avg Loss: 0.1552
Task: Reconstruction | Avg Loss: 3.8737 
MoE Balancing Loss: 12275.2588
Mutual Information | Avg Loss: -0.00418
Total Loss: 4.4242
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278687.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276324.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276943.03
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277220.25
Time elapsed: 3:06:05.721425

 --- Epoch 25
Task: Classification | Acc: 94.17% | Avg Loss: 0.1561
Task: Reconstruction | Avg Loss: 3.8274 
MoE Balancing Loss: 12276.3165
Mutual Information | Avg Loss: -0.00422
Total Loss: 4.4919
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278882.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277524.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277157.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276940.31
Time elapsed: 3:13:50.073042

 --- Epoch 26
Task: Classification | Acc: 94.46% | Avg Loss: 0.1485
Task: Reconstruction | Avg Loss: 3.8033 
MoE Balancing Loss: 12273.8063
Mutual Information | Avg Loss: -0.00423
Total Loss: 4.3711
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276665.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276389.81
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276457.12
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277263.59
Time elapsed: 3:21:34.559129

 --- Epoch 27
Task: Classification | Acc: 94.57% | Avg Loss: 0.1426
Task: Reconstruction | Avg Loss: 3.7670 
MoE Balancing Loss: 12272.8990
Mutual Information | Avg Loss: -0.00414
Total Loss: 4.3369
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275331.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275171.62
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275739.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275219.28
Time elapsed: 3:29:17.185923

 --- Epoch 28
Task: Classification | Acc: 94.70% | Avg Loss: 0.1402
Task: Reconstruction | Avg Loss: 3.7413 
MoE Balancing Loss: 12274.7850
Mutual Information | Avg Loss: -0.00406
Total Loss: 4.3517
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276650.06
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276489.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276760.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276436.00
Time elapsed: 3:37:00.912351

 --- Epoch 29
Task: Classification | Acc: 94.89% | Avg Loss: 0.1382
Task: Reconstruction | Avg Loss: 3.7055 
MoE Balancing Loss: 12273.3748
Mutual Information | Avg Loss: -0.00423
Total Loss: 4.4052
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276510.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276882.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276494.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277612.06
Time elapsed: 3:44:43.680705

 --- Epoch 30
Task: Classification | Acc: 94.99% | Avg Loss: 0.1366
Task: Reconstruction | Avg Loss: 3.6737 
MoE Balancing Loss: 12275.3129
Mutual Information | Avg Loss: -0.00404
Total Loss: 4.1151
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275754.25
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274976.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275546.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274974.97
Time elapsed: 3:52:25.817443
Example 1 ---
Original text: ultimately feels empty and unsatisfying, like swallowing a communion wafer without the wine.
Reconstructed text: filledly silly, sweetly retentious, - - as mythless plotted in the former.,
Original IDs: [101, 4821, 5683, 4064, 1998, 4895, 16846, 2483, 14116, 1010, 2066, 18468, 1037, 15661, 11333, 7512, 2302, 1996, 4511, 1012, 102]
Predicted IDs: [101, 3561, 2135, 10021, 1010, 22557, 2128, 6528, 20771, 1010, 1011, 1011, 2004, 10661, 3238, 27347, 1999, 1996, 2280, 1012, 1010]
BLEU Score: 0.1333
Example 2 ---
Original text: macdowell, whose wifty southern charm has anchored lighter affairs... brings an absolutely riveting conviction to her role.
Reconstructed text: a vivid,, a unoraing the portrait's bold..... a a asetting work of the film.
Original IDs: [101, 6097, 3527, 4381, 1010, 3005, 15536, 6199, 2100, 2670, 11084, 2038, 14453, 9442, 3821, 1012, 1012, 1012, 7545, 2019, 7078, 15544, 19510, 2075, 10652, 2000, 2014, 2535, 1012, 102]
Predicted IDs: [101, 1037, 14954, 1010, 1010, 1037, 4895, 6525, 2075, 1996, 6533, 1005, 1055, 7782, 1012, 1012, 1012, 1012, 1012, 1037, 1037, 1037, 21678, 2075, 2147, 1997, 1996, 2143, 1012, 102]
BLEU Score: 0.0999
Example 3 ---
Original text: i can take infantile humor... but this is the sort of infantile that makes you wonder about changing the director and writer's diapers.
Reconstructed text: is the unfly movie... that it's a the movie,, out you ton'the'and something of a exitr.
Original IDs: [101, 1045, 2064, 2202, 10527, 9463, 8562, 1012, 1012, 1012, 2021, 2023, 2003, 1996, 4066, 1997, 10527, 9463, 2008, 3084, 2017, 4687, 2055, 5278, 1996, 2472, 1998, 3213, 1005, 1055, 22939, 7347, 1012, 102]
Predicted IDs: [101, 2003, 1996, 4895, 10258, 2100, 3185, 1012, 1012, 1012, 2008, 2009, 1005, 1055, 1037, 1996, 3185, 1010, 1010, 2041, 2017, 2000, 1050, 1005, 1996, 1005, 1998, 2242, 1997, 1037, 6164, 2099, 1012, 102]
BLEU Score: 0.3378

 --- Epoch 31
Task: Classification | Acc: 95.03% | Avg Loss: 0.1348
Task: Reconstruction | Avg Loss: 3.6502 
MoE Balancing Loss: 12274.0661
Mutual Information | Avg Loss: -0.00406
Total Loss: 4.3034
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276081.06
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275333.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275131.41
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275368.91
Time elapsed: 4:00:07.599708

 --- Epoch 32
Task: Classification | Acc: 95.06% | Avg Loss: 0.1306
Task: Reconstruction | Avg Loss: 3.6128 
MoE Balancing Loss: 12272.4203
Mutual Information | Avg Loss: -0.00417
Total Loss: 4.2547
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278233.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277677.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277486.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278261.38
Time elapsed: 4:07:41.280751

 --- Epoch 33
Task: Classification | Acc: 95.26% | Avg Loss: 0.1298
Task: Reconstruction | Avg Loss: 3.5810 
MoE Balancing Loss: 12275.2166
Mutual Information | Avg Loss: -0.00404
Total Loss: 4.1292
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277084.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276088.81
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276125.94
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276355.56
Time elapsed: 4:17:08.331451

 --- Epoch 34
Task: Classification | Acc: 95.27% | Avg Loss: 0.1293
Task: Reconstruction | Avg Loss: 3.5543 
MoE Balancing Loss: 12271.6905
Mutual Information | Avg Loss: -0.00404
Total Loss: 4.4345
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277505.06
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277025.62
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277636.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277107.75
Time elapsed: 4:19:09.853107

 --- Epoch 35
Task: Classification | Acc: 95.17% | Avg Loss: 0.1300
Task: Reconstruction | Avg Loss: 3.5262 
MoE Balancing Loss: 12274.9985
Mutual Information | Avg Loss: -0.00390
Total Loss: 4.2022
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277660.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276712.56
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276456.66
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276473.00
Time elapsed: 4:29:59.032321

 --- Epoch 36
Task: Classification | Acc: 95.07% | Avg Loss: 0.1316
Task: Reconstruction | Avg Loss: 3.4962 
MoE Balancing Loss: 12274.8177
Mutual Information | Avg Loss: -0.00396
Total Loss: 4.2197
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278187.06
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276614.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276494.03
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277435.94
Time elapsed: 4:40:49.492396

 --- Epoch 37
Task: Classification | Acc: 95.72% | Avg Loss: 0.1183
Task: Reconstruction | Avg Loss: 3.4787 
MoE Balancing Loss: 12272.9811
Mutual Information | Avg Loss: -0.00388
Total Loss: 4.1346
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275234.84
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275428.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276137.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276462.91
Time elapsed: 4:51:40.780909

 --- Epoch 38
Task: Classification | Acc: 95.53% | Avg Loss: 0.1208
Task: Reconstruction | Avg Loss: 3.4607 
MoE Balancing Loss: 12271.8268
Mutual Information | Avg Loss: -0.00397
Total Loss: 4.0945
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277130.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277072.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276730.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276699.41
Time elapsed: 5:02:31.252677

 --- Epoch 39
Task: Classification | Acc: 95.60% | Avg Loss: 0.1172
Task: Reconstruction | Avg Loss: 3.4274 
MoE Balancing Loss: 12272.8380
Mutual Information | Avg Loss: -0.00395
Total Loss: 4.0648
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275048.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274929.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275862.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275083.19
Time elapsed: 5:13:06.085572

 --- Epoch 40
Task: Classification | Acc: 95.68% | Avg Loss: 0.1151
Task: Reconstruction | Avg Loss: 3.4010 
MoE Balancing Loss: 12274.3325
Mutual Information | Avg Loss: -0.00399
Total Loss: 4.1076
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277935.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277103.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276332.91
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277233.88
Time elapsed: 5:23:53.820951
Example 1 ---
Original text: in execution, this clever idea is far less funny than the original, killers from space.
Reconstructed text: a sweet, thatvey that is not been as as it think and for say but
Original IDs: [101, 1999, 7781, 1010, 2023, 12266, 2801, 2003, 2521, 2625, 6057, 2084, 1996, 2434, 1010, 15978, 2013, 2686, 1012, 102]
Predicted IDs: [101, 1037, 4086, 1010, 2008, 12417, 2008, 2003, 2025, 2042, 2004, 2004, 2009, 2228, 1998, 102, 2005, 2360, 102, 2021]
BLEU Score: 0.1103
Example 2 ---
Original text: the film's hackneyed message is not helped by the thin characterizations, nonexistent plot and pretentious visual style.
Reconstructed text: ( moore's unstitative to, - as the the film with a brayling and sense of uneat.
Original IDs: [101, 1996, 2143, 1005, 1055, 28425, 2098, 4471, 2003, 2025, 3271, 2011, 1996, 4857, 23191, 2015, 1010, 3904, 9048, 16173, 2102, 5436, 1998, 3653, 6528, 20771, 5107, 2806, 1012, 102]
Predicted IDs: [101, 1006, 5405, 1005, 1055, 4895, 3367, 29293, 2000, 1010, 1011, 2004, 1996, 1996, 2143, 2007, 1037, 11655, 8516, 2075, 1998, 3168, 1997, 4895, 5243, 2102, 102, 1012, 102, 102]
BLEU Score: 0.3495
Example 3 ---
Original text: it feels like an after - school special gussied up with some fancy special effects, and watching its rote plot points connect is about as exciting as gazing at an egg timer for 93 minutes.
Reconstructed text: is to like a high - director summer movie testss in in a brand role, but also have to of the the. to biting as.. wink for success.
Original IDs: [101, 2009, 5683, 2066, 2019, 2044, 1011, 2082, 2569, 12670, 11741, 2094, 2039, 2007, 2070, 11281, 2569, 3896, 1010, 1998, 3666, 2049, 18672, 2063, 5436, 2685, 7532, 2003, 2055, 2004, 10990, 2004, 16448, 2012, 2019, 8288, 25309, 2005, 6109, 2781, 1012, 102]
Predicted IDs: [101, 2003, 2000, 2066, 1037, 2152, 1011, 2472, 2621, 3185, 3231, 2015, 2015, 1999, 1999, 1037, 4435, 2535, 1010, 2021, 2036, 2031, 2000, 1997, 1996, 1996, 1012, 102, 102, 102, 102, 2000, 12344, 2004, 1012, 1012, 16837, 2005, 102, 3112, 1012, 102]
BLEU Score: 0.1871

 --- Epoch 41
Task: Classification | Acc: 95.74% | Avg Loss: 0.1129
Task: Reconstruction | Avg Loss: 3.3701 
MoE Balancing Loss: 12273.0132
Mutual Information | Avg Loss: -0.00386
Total Loss: 4.1730
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277869.38
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277614.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279563.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278656.84
Time elapsed: 5:34:43.190108

 --- Epoch 42
Task: Classification | Acc: 95.73% | Avg Loss: 0.1132
Task: Reconstruction | Avg Loss: 3.3558 
MoE Balancing Loss: 12273.4898
Mutual Information | Avg Loss: -0.00395
Total Loss: 4.1097
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277557.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277414.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276417.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277173.66
Time elapsed: 5:45:32.072138

 --- Epoch 43
Task: Classification | Acc: 95.78% | Avg Loss: 0.1139
Task: Reconstruction | Avg Loss: 3.3298 
MoE Balancing Loss: 12273.9949
Mutual Information | Avg Loss: -0.00400
Total Loss: 4.0725
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276052.91
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274967.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275048.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275317.06
Time elapsed: 5:56:08.638432

 --- Epoch 44
Task: Classification | Acc: 95.96% | Avg Loss: 0.1073
Task: Reconstruction | Avg Loss: 3.3215 
MoE Balancing Loss: 12271.2113
Mutual Information | Avg Loss: -0.00387
Total Loss: 4.0049
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276915.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276225.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276343.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276136.88
Time elapsed: 6:06:55.783917

 --- Epoch 45
Task: Classification | Acc: 95.87% | Avg Loss: 0.1091
Task: Reconstruction | Avg Loss: 3.2858 
MoE Balancing Loss: 12273.1160
Mutual Information | Avg Loss: -0.00392
Total Loss: 4.1400
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275440.66
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276067.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276448.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276409.50
Time elapsed: 6:17:43.206000

 --- Epoch 46
Task: Classification | Acc: 95.96% | Avg Loss: 0.1103
Task: Reconstruction | Avg Loss: 3.2592 
MoE Balancing Loss: 12274.4471
Mutual Information | Avg Loss: -0.00384
Total Loss: 4.0385
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279151.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276898.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276714.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276467.00
Time elapsed: 6:28:31.148930

 --- Epoch 47
Task: Classification | Acc: 96.07% | Avg Loss: 0.1047
Task: Reconstruction | Avg Loss: 3.2460 
MoE Balancing Loss: 12274.3703
Mutual Information | Avg Loss: -0.00389
Total Loss: 4.1360
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276325.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276327.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276403.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276605.06
Time elapsed: 6:39:05.468645

 --- Epoch 48
Task: Classification | Acc: 96.14% | Avg Loss: 0.1031
Task: Reconstruction | Avg Loss: 3.2267 
MoE Balancing Loss: 12270.9526
Mutual Information | Avg Loss: -0.00372
Total Loss: 3.9959
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274858.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275051.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275446.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275698.47
Time elapsed: 6:49:52.107071

 --- Epoch 49
Task: Classification | Acc: 96.08% | Avg Loss: 0.1042
Task: Reconstruction | Avg Loss: 3.2045 
MoE Balancing Loss: 12276.2276
Mutual Information | Avg Loss: -0.00372
Total Loss: 4.0342
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278840.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277675.75
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276769.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277334.28
Time elapsed: 7:00:40.098338

 --- Epoch 50
Task: Classification | Acc: 96.23% | Avg Loss: 0.1056
Task: Reconstruction | Avg Loss: 3.1841 
MoE Balancing Loss: 12274.5479
Mutual Information | Avg Loss: -0.00380
Total Loss: 3.9712
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277885.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276483.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276356.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276742.59
Time elapsed: 7:11:27.617404
Example 1 ---
Original text: if you dig on david mamet's mind tricks... rent this movie and enjoy!
Reconstructed text: is s film with like johnat's cl standard... is a story interesting
Original IDs: [101, 2065, 2017, 10667, 2006, 2585, 5003, 11368, 1005, 1055, 2568, 12225, 1012, 1012, 1012, 9278, 2023, 3185, 1998, 5959, 999, 102]
Predicted IDs: [101, 2003, 1055, 2143, 2007, 2066, 2198, 4017, 1005, 1055, 18856, 3115, 1012, 1012, 1012, 2003, 1037, 2466, 102, 5875, 102, 102]
BLEU Score: 0.1238
Example 2 ---
Original text: a rewarding work of art for only the most patient and challenge - hungry moviegoers.
Reconstructed text: the is is is the art of a,, funny, well - age thevoking is.
Original IDs: [101, 1037, 10377, 2075, 2147, 1997, 2396, 2005, 2069, 1996, 2087, 5776, 1998, 4119, 1011, 7501, 3185, 3995, 2545, 1012, 102]
Predicted IDs: [101, 1996, 2003, 2003, 2003, 1996, 2396, 1997, 1037, 1010, 1010, 6057, 1010, 2092, 1011, 2287, 1996, 22776, 2003, 1012, 102]
BLEU Score: 0.3333
Example 3 ---
Original text: pacino is brilliant as the sleep - deprived dormer, his increasing weariness as much existential as it is physical.
Reconstructed text: thefolds a like a well - pro indian film with the pu who, a unaeniaing view of a engaging,
Original IDs: [101, 14397, 5740, 2003, 8235, 2004, 1996, 3637, 1011, 17676, 19568, 2121, 1010, 2010, 4852, 4929, 9961, 2004, 2172, 25953, 4818, 2004, 2009, 2003, 3558, 1012, 102]
Predicted IDs: [101, 1996, 10371, 2015, 1037, 2066, 1037, 2092, 1011, 4013, 2796, 2143, 2007, 1996, 16405, 2040, 1010, 1037, 14477, 19825, 2075, 3193, 1997, 1037, 11973, 1010, 102]
BLEU Score: 0.1429

 --- Epoch 51
Task: Classification | Acc: 96.38% | Avg Loss: 0.1014
Task: Reconstruction | Avg Loss: 3.1678 
MoE Balancing Loss: 12275.7979
Mutual Information | Avg Loss: -0.00381
Total Loss: 4.0196
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276752.44
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275672.38
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276325.91
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275648.84
Time elapsed: 7:22:16.178381

 --- Epoch 52
Task: Classification | Acc: 96.14% | Avg Loss: 0.1002
Task: Reconstruction | Avg Loss: 3.1385 
MoE Balancing Loss: 12272.5291
Mutual Information | Avg Loss: -0.00387
Total Loss: 4.0438
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277723.16
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276431.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276642.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276873.84
Time elapsed: 7:32:52.426906

 --- Epoch 53
Task: Classification | Acc: 96.31% | Avg Loss: 0.0975
Task: Reconstruction | Avg Loss: 3.1138 
MoE Balancing Loss: 12273.6294
Mutual Information | Avg Loss: -0.00386
Total Loss: 4.0304
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274642.69
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274691.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274638.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274425.56
Time elapsed: 7:43:39.618850

 --- Epoch 54
Task: Classification | Acc: 96.46% | Avg Loss: 0.0978
Task: Reconstruction | Avg Loss: 3.0963 
MoE Balancing Loss: 12271.7464
Mutual Information | Avg Loss: -0.00377
Total Loss: 4.0620
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276675.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277281.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276883.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277016.84
Time elapsed: 7:54:27.375881

 --- Epoch 55
Task: Classification | Acc: 96.45% | Avg Loss: 0.0942
Task: Reconstruction | Avg Loss: 3.0874 
MoE Balancing Loss: 12273.4934
Mutual Information | Avg Loss: -0.00375
Total Loss: 3.9597
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278416.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277422.72
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277058.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277610.12
Time elapsed: 8:05:14.512942

 --- Epoch 56
Task: Classification | Acc: 96.55% | Avg Loss: 0.0954
Task: Reconstruction | Avg Loss: 3.0682 
MoE Balancing Loss: 12275.6477
Mutual Information | Avg Loss: -0.00381
Total Loss: 4.0637
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277746.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276528.19
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277041.94
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277217.78
Time elapsed: 8:11:23.380488

 --- Epoch 57
Task: Classification | Acc: 96.51% | Avg Loss: 0.1004
Task: Reconstruction | Avg Loss: 3.0461 
MoE Balancing Loss: 12272.3714
Mutual Information | Avg Loss: -0.00390
Total Loss: 4.0810
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276604.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277073.72
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277491.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276922.84
Time elapsed: 8:12:59.205792

 --- Epoch 58
Task: Classification | Acc: 96.67% | Avg Loss: 0.0919
Task: Reconstruction | Avg Loss: 3.0223 
MoE Balancing Loss: 12274.9294
Mutual Information | Avg Loss: -0.00383
Total Loss: 4.0322
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277790.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276248.38
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276974.62
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276351.59
Time elapsed: 8:14:34.936673

 --- Epoch 59
Task: Classification | Acc: 96.69% | Avg Loss: 0.0907
Task: Reconstruction | Avg Loss: 3.0096 
MoE Balancing Loss: 12273.7717
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.9866
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276018.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275653.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275900.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276392.31
Time elapsed: 8:16:10.222153

 --- Epoch 60
Task: Classification | Acc: 96.65% | Avg Loss: 0.0904
Task: Reconstruction | Avg Loss: 3.0039 
MoE Balancing Loss: 12274.8004
Mutual Information | Avg Loss: -0.00367
Total Loss: 3.9074
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278446.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276755.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276736.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277260.75
Time elapsed: 8:17:45.440664
Example 1 ---
Original text: the magic of the film lies not in the mysterious spring but in the richness of its performances.
Reconstructed text: the addition of the man thats a detailed, humor, and the fascinating beauty of the times.
Original IDs: [101, 1996, 3894, 1997, 1996, 2143, 3658, 2025, 1999, 1996, 8075, 3500, 2021, 1999, 1996, 4138, 2791, 1997, 2049, 4616, 1012, 102]
Predicted IDs: [101, 1996, 2804, 1997, 1996, 2158, 2008, 2015, 1037, 6851, 1010, 8562, 1010, 1998, 1996, 17160, 5053, 1997, 1996, 2335, 1012, 102]
BLEU Score: 0.3684
Example 2 ---
Original text: harris commands the screen, using his frailty to suggest the ravages of a life of corruption and ruthlessness.
Reconstructed text: it manages the film, using the perfectquent to in the wonderhering of of remarkable sacrifice of vietnamese and beauty
Original IDs: [101, 5671, 10954, 1996, 3898, 1010, 2478, 2010, 25737, 3723, 2000, 6592, 1996, 10958, 3567, 8449, 1997, 1037, 2166, 1997, 7897, 1998, 18101, 2791, 1012, 102]
Predicted IDs: [101, 2009, 9020, 1996, 2143, 1010, 2478, 1996, 3819, 15417, 2000, 1999, 1996, 4687, 22658, 1997, 1997, 9487, 8688, 1997, 9101, 1998, 102, 5053, 102, 102]
BLEU Score: 0.4000
Example 3 ---
Original text: ` ` the time machine'' is a movie that has no interest in itself.
Reconstructed text: ` ` ` ` `'' is the'' is a ` for the.
Original IDs: [101, 1036, 1036, 1996, 2051, 3698, 1005, 1005, 2003, 1037, 3185, 2008, 2038, 2053, 3037, 1999, 2993, 1012, 102]
Predicted IDs: [101, 1036, 1036, 1036, 1036, 1036, 1005, 1005, 2003, 1996, 1005, 1005, 2003, 1037, 1036, 2005, 1996, 1012, 102]
BLEU Score: 0.4366

 --- Epoch 61
Task: Classification | Acc: 96.53% | Avg Loss: 0.0930
Task: Reconstruction | Avg Loss: 2.9920 
MoE Balancing Loss: 12273.0273
Mutual Information | Avg Loss: -0.00371
Total Loss: 3.9517
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276795.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276367.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276722.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276618.59
Time elapsed: 8:19:20.849783

 --- Epoch 62
Task: Classification | Acc: 96.56% | Avg Loss: 0.0920
Task: Reconstruction | Avg Loss: 2.9559 
MoE Balancing Loss: 12273.4477
Mutual Information | Avg Loss: -0.00373
Total Loss: 3.9332
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276253.62
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276057.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275745.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275685.19
Time elapsed: 8:20:55.958838

 --- Epoch 63
Task: Classification | Acc: 96.73% | Avg Loss: 0.0884
Task: Reconstruction | Avg Loss: 2.9581 
MoE Balancing Loss: 12274.1748
Mutual Information | Avg Loss: -0.00378
Total Loss: 3.9430
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278324.00
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277000.81
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277637.03
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277414.50
Time elapsed: 8:22:31.261764

 --- Epoch 64
Task: Classification | Acc: 96.78% | Avg Loss: 0.0886
Task: Reconstruction | Avg Loss: 2.9280 
MoE Balancing Loss: 12272.4008
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.9332
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275516.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275039.91
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275074.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275173.28
Time elapsed: 8:24:06.713048

 --- Epoch 65
Task: Classification | Acc: 96.85% | Avg Loss: 0.0846
Task: Reconstruction | Avg Loss: 2.9219 
MoE Balancing Loss: 12272.2958
Mutual Information | Avg Loss: -0.00371
Total Loss: 3.9341
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276062.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274511.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275231.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275339.78
Time elapsed: 8:25:41.861401

 --- Epoch 66
Task: Classification | Acc: 96.83% | Avg Loss: 0.0872
Task: Reconstruction | Avg Loss: 2.9030 
MoE Balancing Loss: 12272.8930
Mutual Information | Avg Loss: -0.00374
Total Loss: 3.8134
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276917.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276725.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276998.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277029.88
Time elapsed: 8:27:16.686136

 --- Epoch 67
Task: Classification | Acc: 96.79% | Avg Loss: 0.0883
Task: Reconstruction | Avg Loss: 2.9051 
MoE Balancing Loss: 12271.5667
Mutual Information | Avg Loss: -0.00379
Total Loss: 3.9372
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277876.44
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276746.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277171.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277046.41
Time elapsed: 8:28:52.444319

 --- Epoch 68
Task: Classification | Acc: 96.95% | Avg Loss: 0.0851
Task: Reconstruction | Avg Loss: 2.8777 
MoE Balancing Loss: 12273.8374
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.8906
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277509.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277387.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277666.34
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277438.25
Time elapsed: 8:30:27.740120

 --- Epoch 69
Task: Classification | Acc: 96.79% | Avg Loss: 0.0858
Task: Reconstruction | Avg Loss: 2.8453 
MoE Balancing Loss: 12275.7332
Mutual Information | Avg Loss: -0.00370
Total Loss: 3.7763
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278387.06
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277967.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277371.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277355.62
Time elapsed: 8:32:03.345377

 --- Epoch 70
Task: Classification | Acc: 97.01% | Avg Loss: 0.0825
Task: Reconstruction | Avg Loss: 2.8551 
MoE Balancing Loss: 12271.2693
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.8370
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277293.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276571.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277294.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277286.47
Time elapsed: 8:33:38.829959
Example 1 ---
Original text: few films capture so perfectly the hopes and dreams of little boys on baseball fields as well as the grown men who sit in the stands.
Reconstructed text: director that has runs into the intelligence and strokes the film in taking people people could ever through the own, working to the own theater
Original IDs: [101, 2261, 3152, 5425, 2061, 6669, 1996, 8069, 1998, 5544, 1997, 2210, 3337, 2006, 3598, 4249, 2004, 2092, 2004, 1996, 4961, 2273, 2040, 4133, 1999, 1996, 4832, 1012, 102]
Predicted IDs: [101, 2472, 2008, 2038, 3216, 2046, 1996, 4454, 1998, 13692, 1996, 2143, 1999, 2635, 2111, 2111, 2071, 2412, 2083, 1996, 2219, 1010, 2551, 2000, 1996, 2219, 4258, 102, 102]
BLEU Score: 0.1851
Example 2 ---
Original text: with the exception of some fleetingly amusing improvisations by cedric the entertainer as perry's boss, there isn't a redeeming moment here.
Reconstructed text: with a doubt of profile of seeing the humanist, in the of part of child's paintings, but the film's patiencely back psychologicalic..
Original IDs: [101, 2007, 1996, 6453, 1997, 2070, 25085, 2135, 19142, 24584, 2015, 2011, 26170, 1996, 21751, 2004, 6890, 1005, 1055, 5795, 1010, 2045, 2003, 1050, 1005, 1056, 1037, 2417, 21564, 2075, 2617, 2182, 1012, 102]
Predicted IDs: [101, 2007, 1037, 4797, 1997, 6337, 1997, 3773, 1996, 24464, 1010, 1999, 1996, 1997, 2112, 1997, 2775, 1005, 1055, 5265, 1010, 2021, 1996, 2143, 1005, 1055, 11752, 2135, 2067, 8317, 2594, 1012, 1012, 102]
BLEU Score: 0.2593
Example 3 ---
Original text: the movie's accumulated force still feels like an ugly knot tightening in your stomach.
Reconstructed text: doesn't have much to together up a own spirits in the characters,
Original IDs: [101, 1996, 3185, 1005, 1055, 14830, 2486, 2145, 5683, 2066, 2019, 9200, 12226, 18711, 1999, 2115, 4308, 1012, 102]
Predicted IDs: [101, 2515, 1050, 1005, 1056, 2031, 2172, 2000, 2362, 2039, 1037, 2219, 4382, 2015, 1999, 1996, 3494, 1010, 102]
BLEU Score: 0.1238

 --- Epoch 71
Task: Classification | Acc: 96.98% | Avg Loss: 0.0826
Task: Reconstruction | Avg Loss: 2.8364 
MoE Balancing Loss: 12273.7663
Mutual Information | Avg Loss: -0.00371
Total Loss: 3.8954
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278520.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277332.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277332.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277622.47
Time elapsed: 8:35:14.613741

 --- Epoch 72
Task: Classification | Acc: 97.01% | Avg Loss: 0.0819
Task: Reconstruction | Avg Loss: 2.8405 
MoE Balancing Loss: 12268.8663
Mutual Information | Avg Loss: -0.00369
Total Loss: 3.8388
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278009.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277691.16
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278331.59
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277866.91
Time elapsed: 8:36:50.227251

 --- Epoch 73
Task: Classification | Acc: 96.95% | Avg Loss: 0.0813
Task: Reconstruction | Avg Loss: 2.8267 
MoE Balancing Loss: 12275.0612
Mutual Information | Avg Loss: -0.00367
Total Loss: 3.8489
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276681.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275928.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275915.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276250.66
Time elapsed: 8:38:25.611503

 --- Epoch 74
Task: Classification | Acc: 97.05% | Avg Loss: 0.0820
Task: Reconstruction | Avg Loss: 2.8166 
MoE Balancing Loss: 12273.1720
Mutual Information | Avg Loss: -0.00366
Total Loss: 3.7973
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278052.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276030.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276760.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276942.19
Time elapsed: 8:40:00.741337

 --- Epoch 75
Task: Classification | Acc: 97.06% | Avg Loss: 0.0810
Task: Reconstruction | Avg Loss: 2.7857 
MoE Balancing Loss: 12274.3916
Mutual Information | Avg Loss: -0.00366
Total Loss: 3.9158
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278875.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277499.91
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278190.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277355.22
Time elapsed: 8:41:36.689507

 --- Epoch 76
Task: Classification | Acc: 97.15% | Avg Loss: 0.0787
Task: Reconstruction | Avg Loss: 2.7690 
MoE Balancing Loss: 12272.1738
Mutual Information | Avg Loss: -0.00357
Total Loss: 3.8860
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277725.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276910.16
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277214.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277686.00
Time elapsed: 8:43:12.550686

 --- Epoch 77
Task: Classification | Acc: 97.03% | Avg Loss: 0.0805
Task: Reconstruction | Avg Loss: 2.7731 
MoE Balancing Loss: 12276.5641
Mutual Information | Avg Loss: -0.00368
Total Loss: 3.8376
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277568.38
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277752.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277902.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277384.59
Time elapsed: 8:44:48.252901

 --- Epoch 78
Task: Classification | Acc: 97.07% | Avg Loss: 0.0799
Task: Reconstruction | Avg Loss: 2.7637 
MoE Balancing Loss: 12274.6862
Mutual Information | Avg Loss: -0.00365
Total Loss: 3.7613
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279741.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278067.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278409.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278640.06
Time elapsed: 8:46:23.725354

 --- Epoch 79
Task: Classification | Acc: 97.07% | Avg Loss: 0.0819
Task: Reconstruction | Avg Loss: 2.7388 
MoE Balancing Loss: 12274.6607
Mutual Information | Avg Loss: -0.00378
Total Loss: 3.8552
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277216.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276299.19
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276055.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276385.03
Time elapsed: 8:47:59.561239

 --- Epoch 80
Task: Classification | Acc: 97.15% | Avg Loss: 0.0780
Task: Reconstruction | Avg Loss: 2.7464 
MoE Balancing Loss: 12274.4435
Mutual Information | Avg Loss: -0.00369
Total Loss: 3.7922
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278539.06
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277689.47
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277592.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277499.25
Time elapsed: 8:49:35.389916
Example 1 ---
Original text: if steven soderbergh's ` solaris'is a failure it is a glorious failure.
Reconstructed text: is steven korbergh's work a vulgar, in a new of's the form.
Original IDs: [101, 2065, 7112, 2061, 4063, 4059, 2232, 1005, 1055, 1036, 5943, 2483, 1005, 2003, 1037, 4945, 2009, 2003, 1037, 14013, 4945, 1012, 102]
Predicted IDs: [101, 2003, 7112, 12849, 2099, 4059, 2232, 1005, 1055, 2147, 1037, 29364, 1010, 1999, 1037, 2047, 1997, 1005, 1055, 1996, 2433, 1012, 102]
BLEU Score: 0.3750
Example 2 ---
Original text: it seems like i have been waiting my whole life for this movie and now i can't wait for the sequel.
Reconstructed text: it the s to be made, a special movie of the door that serving they didn't necessarily, it stupid.
Original IDs: [101, 2009, 3849, 2066, 1045, 2031, 2042, 3403, 2026, 2878, 2166, 2005, 2023, 3185, 1998, 2085, 1045, 6187, 1050, 1005, 1056, 3524, 2005, 1996, 8297, 1012, 102]
Predicted IDs: [101, 2009, 1996, 1055, 2000, 2022, 2081, 1010, 1037, 2569, 3185, 1997, 1996, 2341, 2008, 3529, 2027, 2106, 1050, 1005, 1056, 9352, 1010, 2009, 5236, 1012, 102]
BLEU Score: 0.2174
Example 3 ---
Original text: it's hard to imagine alan arkin being better than he is in this performance.
Reconstructed text: it's still a months and 295 movie was, but your is in the movie.
Original IDs: [101, 2009, 1005, 1055, 2524, 2000, 5674, 5070, 15745, 2378, 2108, 2488, 2084, 2002, 2003, 1999, 2023, 2836, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 2145, 1037, 2706, 1998, 21679, 3185, 2001, 1010, 2021, 2115, 2003, 1999, 1996, 3185, 1012, 102]
BLEU Score: 0.2941

 --- Epoch 81
Task: Classification | Acc: 97.10% | Avg Loss: 0.0765
Task: Reconstruction | Avg Loss: 2.7303 
MoE Balancing Loss: 12275.5604
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.7730
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277576.69
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276686.81
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276422.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277054.22
Time elapsed: 8:51:10.819929

 --- Epoch 82
Task: Classification | Acc: 97.21% | Avg Loss: 0.0782
Task: Reconstruction | Avg Loss: 2.7036 
MoE Balancing Loss: 12274.1950
Mutual Information | Avg Loss: -0.00386
Total Loss: 3.8495
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276077.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276804.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276851.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277115.66
Time elapsed: 8:52:46.722289

 --- Epoch 83
Task: Classification | Acc: 97.17% | Avg Loss: 0.0776
Task: Reconstruction | Avg Loss: 2.7025 
MoE Balancing Loss: 12270.4586
Mutual Information | Avg Loss: -0.00374
Total Loss: 3.7793
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276398.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277058.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277076.34
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277267.75
Time elapsed: 8:54:22.341521

 --- Epoch 84
Task: Classification | Acc: 97.17% | Avg Loss: 0.0773
Task: Reconstruction | Avg Loss: 2.6979 
MoE Balancing Loss: 12272.8644
Mutual Information | Avg Loss: -0.00382
Total Loss: 3.8065
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279741.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277584.72
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278274.69
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277786.06
Time elapsed: 8:55:58.179201

 --- Epoch 85
Task: Classification | Acc: 97.31% | Avg Loss: 0.0739
Task: Reconstruction | Avg Loss: 2.6787 
MoE Balancing Loss: 12275.9875
Mutual Information | Avg Loss: -0.00372
Total Loss: 3.7078
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275916.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276250.88
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276645.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276390.69
Time elapsed: 8:57:33.553089

 --- Epoch 86
Task: Classification | Acc: 97.25% | Avg Loss: 0.0760
Task: Reconstruction | Avg Loss: 2.6629 
MoE Balancing Loss: 12275.7108
Mutual Information | Avg Loss: -0.00387
Total Loss: 3.8178
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277916.69
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276722.47
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277436.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276892.62
Time elapsed: 8:59:09.407777

 --- Epoch 87
Task: Classification | Acc: 97.13% | Avg Loss: 0.0799
Task: Reconstruction | Avg Loss: 2.6592 
MoE Balancing Loss: 12270.0969
Mutual Information | Avg Loss: -0.00395
Total Loss: 3.8844
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274721.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275311.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274932.69
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275244.66
Time elapsed: 9:00:45.445261

 --- Epoch 88
Task: Classification | Acc: 97.39% | Avg Loss: 0.0726
Task: Reconstruction | Avg Loss: 2.6473 
MoE Balancing Loss: 12272.0437
Mutual Information | Avg Loss: -0.00385
Total Loss: 3.7831
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275091.44
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274355.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274678.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274677.91
Time elapsed: 9:02:20.984149

 --- Epoch 89
Task: Classification | Acc: 97.23% | Avg Loss: 0.0778
Task: Reconstruction | Avg Loss: 2.6437 
MoE Balancing Loss: 12272.4151
Mutual Information | Avg Loss: -0.00384
Total Loss: 3.8182
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276102.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275775.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276207.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276266.09
Time elapsed: 9:03:56.870467

 --- Epoch 90
Task: Classification | Acc: 97.19% | Avg Loss: 0.0742
Task: Reconstruction | Avg Loss: 2.6297 
MoE Balancing Loss: 12268.7009
Mutual Information | Avg Loss: -0.00384
Total Loss: 3.8037
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277230.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276696.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277158.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276810.69
Time elapsed: 9:05:32.706910
Example 1 ---
Original text: lovely and poignant.
Reconstructed text: humorous and porky.
Original IDs: [101, 8403, 1998, 13433, 25593, 1012, 102]
Predicted IDs: [101, 14742, 1998, 13433, 15952, 1012, 102]
BLEU Score: 0.5000
Example 2 ---
Original text: a rewarding work of art for only the most patient and challenge - hungry moviegoers.
Reconstructed text: a a a charming of perfect in a of, intelligent, self - assured drama of suck. a
Original IDs: [101, 1037, 10377, 2075, 2147, 1997, 2396, 2005, 2069, 1996, 2087, 5776, 1998, 4119, 1011, 7501, 3185, 3995, 2545, 1012, 102]
Predicted IDs: [101, 1037, 1037, 1037, 11951, 1997, 3819, 1999, 1037, 1997, 1010, 9414, 1010, 2969, 1011, 8916, 3689, 1997, 11891, 1012, 1037]
BLEU Score: 0.2000
Example 3 ---
Original text: dragonfly has no atmosphere, no tension - - nothing but costner, flailing away.
Reconstructed text: aos - wit comedy and a comedy - - a taless,linely a mor
Original IDs: [101, 5202, 14151, 2038, 2053, 7224, 1010, 2053, 6980, 1011, 1011, 2498, 2021, 3465, 3678, 1010, 13109, 29544, 2185, 1012, 102]
Predicted IDs: [101, 1037, 2891, 1011, 15966, 4038, 1998, 1037, 4038, 1011, 1011, 102, 1037, 11937, 3238, 1010, 102, 4179, 2135, 1037, 22822]
BLEU Score: 0.1871

 --- Epoch 91
Task: Classification | Acc: 97.28% | Avg Loss: 0.0728
Task: Reconstruction | Avg Loss: 2.6228 
MoE Balancing Loss: 12271.6574
Mutual Information | Avg Loss: -0.00373
Total Loss: 3.7334
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278215.66
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277276.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277341.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277092.16
Time elapsed: 9:07:08.488155

 --- Epoch 92
Task: Classification | Acc: 97.26% | Avg Loss: 0.0754
Task: Reconstruction | Avg Loss: 2.6011 
MoE Balancing Loss: 12273.6351
Mutual Information | Avg Loss: -0.00375
Total Loss: 3.7291
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277032.44
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276210.91
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277141.34
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276823.16
Time elapsed: 9:08:44.058461

 --- Epoch 93
Task: Classification | Acc: 97.20% | Avg Loss: 0.0739
Task: Reconstruction | Avg Loss: 2.6063 
MoE Balancing Loss: 12277.8111
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.7075
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277852.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276476.56
Layer 2:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276227.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277000.78
Time elapsed: 9:10:19.535206

 --- Epoch 94
Task: Classification | Acc: 97.44% | Avg Loss: 0.0712
Task: Reconstruction | Avg Loss: 2.6025 
MoE Balancing Loss: 12271.5270
Mutual Information | Avg Loss: -0.00392
Total Loss: 3.8216
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276378.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276502.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276559.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276628.97
Time elapsed: 9:11:56.058812

 --- Epoch 95
Task: Classification | Acc: 97.37% | Avg Loss: 0.0713
Task: Reconstruction | Avg Loss: 2.5863 
MoE Balancing Loss: 12272.6299
Mutual Information | Avg Loss: -0.00383
Total Loss: 3.7522
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277078.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275864.00
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276957.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276772.66
Time elapsed: 9:13:31.916245

 --- Epoch 96
Task: Classification | Acc: 97.46% | Avg Loss: 0.0699
Task: Reconstruction | Avg Loss: 2.5903 
MoE Balancing Loss: 12276.1522
Mutual Information | Avg Loss: -0.00387
Total Loss: 3.7346
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276952.38
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276107.16
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275673.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275405.41
Time elapsed: 9:15:07.455580

 --- Epoch 97
Task: Classification | Acc: 97.39% | Avg Loss: 0.0691
Task: Reconstruction | Avg Loss: 2.5682 
MoE Balancing Loss: 12273.9820
Mutual Information | Avg Loss: -0.00385
Total Loss: 3.7041
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277991.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276620.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277854.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277054.22
Time elapsed: 9:16:42.965129

 --- Epoch 98
Task: Classification | Acc: 97.36% | Avg Loss: 0.0723
Task: Reconstruction | Avg Loss: 2.5620 
MoE Balancing Loss: 12272.8484
Mutual Information | Avg Loss: -0.00388
Total Loss: 3.7353
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276887.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276788.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276299.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276609.47
Time elapsed: 9:18:18.594200

 --- Epoch 99
Task: Classification | Acc: 97.45% | Avg Loss: 0.0715
Task: Reconstruction | Avg Loss: 2.5539 
MoE Balancing Loss: 12274.3911
Mutual Information | Avg Loss: -0.00378
Total Loss: 3.7133
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276948.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275845.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276075.91
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276118.25
Time elapsed: 9:19:54.134160

 --- Epoch 100
Task: Classification | Acc: 97.47% | Avg Loss: 0.0688
Task: Reconstruction | Avg Loss: 2.5655 
MoE Balancing Loss: 12272.5809
Mutual Information | Avg Loss: -0.00372
Total Loss: 3.6563
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277759.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277328.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277471.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277491.84
Time elapsed: 9:21:29.436410
Example 1 ---
Original text: the volatile dynamics of female friendship is the subject of this unhurried, low - key film that is so off - hollywood that it seems positively french in its rhythms and resonance.
Reconstructed text: the poke, the the lawrence, sly, g apprunt, self - numb direction, average surprises pitch - - that's as entertaining..
Original IDs: [101, 1996, 20606, 10949, 1997, 2931, 6860, 2003, 1996, 3395, 1997, 2023, 4895, 24572, 11998, 1010, 2659, 1011, 3145, 2143, 2008, 2003, 2061, 2125, 1011, 5365, 2008, 2009, 3849, 13567, 2413, 1999, 2049, 17900, 1998, 17011, 1012, 102]
Predicted IDs: [101, 1996, 26202, 1010, 1996, 1996, 5623, 1010, 1055, 2135, 1010, 1043, 10439, 6820, 3372, 1010, 2969, 1011, 15903, 3257, 1010, 2779, 20096, 6510, 1011, 1011, 2008, 1005, 1055, 2004, 14036, 102, 102, 1012, 102, 102, 1012, 102]
BLEU Score: 0.1715
Example 2 ---
Original text: without non - stop techno or the existential overtones of a kieslowski morality tale, maelstrom is just another winter sleepers.
Reconstructed text: a self - depth homage with a iconteristic rep and a self -dic innerations and incitiesng that the antonia'sensation in.
Original IDs: [101, 2302, 2512, 1011, 2644, 21416, 2030, 1996, 25953, 4818, 2058, 11115, 1997, 1037, 11382, 2229, 8261, 5488, 16561, 6925, 1010, 11530, 4877, 13887, 2003, 2074, 2178, 3467, 24372, 2015, 1012, 102]
Predicted IDs: [101, 1037, 2969, 1011, 5995, 14822, 2007, 1037, 12696, 3334, 6553, 16360, 1998, 1037, 2969, 1011, 14808, 5110, 3370, 2015, 1998, 4297, 6447, 3070, 2008, 1996, 24272, 1005, 8742, 1999, 1012, 102]
BLEU Score: 0.1816
Example 3 ---
Original text: a marvel like none you've seen.
Reconstructed text: you know a that it's happening,
Original IDs: [101, 1037, 8348, 2066, 3904, 2017, 1005, 2310, 2464, 1012, 102]
Predicted IDs: [101, 2017, 2113, 1037, 2008, 2009, 1005, 1055, 6230, 1010, 102]
BLEU Score: 0.2500

 --- Epoch 101
Task: Classification | Acc: 97.56% | Avg Loss: 0.0684
Task: Reconstruction | Avg Loss: 2.5390 
MoE Balancing Loss: 12272.1609
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.7180
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276973.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276226.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275902.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277169.75
Time elapsed: 9:23:05.176383

 --- Epoch 102
Task: Classification | Acc: 97.41% | Avg Loss: 0.0701
Task: Reconstruction | Avg Loss: 2.5394 
MoE Balancing Loss: 12282.0219
Mutual Information | Avg Loss: -0.00383
Total Loss: 3.6642
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278543.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277186.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278362.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276732.88
Time elapsed: 9:24:40.485155

 --- Epoch 103
Task: Classification | Acc: 97.44% | Avg Loss: 0.0700
Task: Reconstruction | Avg Loss: 2.5304 
MoE Balancing Loss: 12271.8387
Mutual Information | Avg Loss: -0.00390
Total Loss: 3.7272
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275942.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276473.16
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276696.12
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276648.72
Time elapsed: 9:26:16.277771

 --- Epoch 104
Task: Classification | Acc: 97.47% | Avg Loss: 0.0686
Task: Reconstruction | Avg Loss: 2.5277 
MoE Balancing Loss: 12272.4977
Mutual Information | Avg Loss: -0.00389
Total Loss: 3.7441
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276707.25
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276011.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275363.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275629.00
Time elapsed: 9:27:52.031534

 --- Epoch 105
Task: Classification | Acc: 97.72% | Avg Loss: 0.0642
Task: Reconstruction | Avg Loss: 2.5226 
MoE Balancing Loss: 12273.7610
Mutual Information | Avg Loss: -0.00376
Total Loss: 3.7362
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279013.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277266.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277443.06
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277810.53
Time elapsed: 9:29:28.170968

 --- Epoch 106
Task: Classification | Acc: 97.45% | Avg Loss: 0.0700
Task: Reconstruction | Avg Loss: 2.5015 
MoE Balancing Loss: 12275.6943
Mutual Information | Avg Loss: -0.00387
Total Loss: 3.6906
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277886.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276157.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276972.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276577.09
Time elapsed: 9:31:04.005417

 --- Epoch 107
Task: Classification | Acc: 97.49% | Avg Loss: 0.0709
Task: Reconstruction | Avg Loss: 2.5068 
MoE Balancing Loss: 12272.9517
Mutual Information | Avg Loss: -0.00398
Total Loss: 3.7660
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276173.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276547.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277491.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277038.62
Time elapsed: 9:32:41.604602

 --- Epoch 108
Task: Classification | Acc: 97.56% | Avg Loss: 0.0675
Task: Reconstruction | Avg Loss: 2.4970 
MoE Balancing Loss: 12273.9566
Mutual Information | Avg Loss: -0.00388
Total Loss: 3.6867
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276418.16
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275707.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276398.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276714.91
Time elapsed: 9:36:14.178128

 --- Epoch 109
Task: Classification | Acc: 97.57% | Avg Loss: 0.0680
Task: Reconstruction | Avg Loss: 2.4836 
MoE Balancing Loss: 12272.7076
Mutual Information | Avg Loss: -0.00398
Total Loss: 3.6928
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276699.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275807.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276388.69
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275958.00
Time elapsed: 9:47:23.881001

 --- Epoch 110
Task: Classification | Acc: 97.66% | Avg Loss: 0.0648
Task: Reconstruction | Avg Loss: 2.4847 
MoE Balancing Loss: 12272.8602
Mutual Information | Avg Loss: -0.00390
Total Loss: 3.6237
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278600.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276876.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277804.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277333.53
Time elapsed: 9:58:33.742581
Example 1 ---
Original text: as a first - time director, paxton has tapped something in himself as an actor that provides frailty with its dark soul.
Reconstructed text: an a large - eyed director and suited of the that is a pub of view that the peoples into a noir trademark and an
Original IDs: [101, 2004, 1037, 2034, 1011, 2051, 2472, 1010, 27765, 2038, 10410, 2242, 1999, 2370, 2004, 2019, 3364, 2008, 3640, 25737, 3723, 2007, 2049, 2601, 3969, 1012, 102]
Predicted IDs: [101, 2019, 1037, 2312, 1011, 7168, 2472, 1998, 10897, 1997, 1996, 2008, 2003, 1037, 9047, 1997, 3193, 2008, 1996, 2111, 2015, 2046, 1037, 15587, 11749, 1998, 2019]
BLEU Score: 0.2000
Example 2 ---
Original text: director uwe boll and the actors provide scant reason to care in this crude'70s throwback.
Reconstructed text: the clciy,, the filmly pats things to reel up the ownrky debut shanghaisy
Original IDs: [101, 2472, 1057, 8545, 8945, 3363, 1998, 1996, 5889, 3073, 13594, 2102, 3114, 2000, 2729, 1999, 2023, 13587, 1005, 17549, 5466, 5963, 1012, 102]
Predicted IDs: [101, 1996, 18856, 6895, 2100, 1010, 1010, 1996, 2143, 2135, 6986, 2015, 2477, 2000, 15934, 2039, 1996, 2219, 15952, 2834, 8344, 6508, 102, 102]
BLEU Score: 0.1247
Example 3 ---
Original text: the film is quiet, threatening and unforgettable.
Reconstructed text: the results is possible, desirable di strangelyeneectble and una
Original IDs: [101, 1996, 2143, 2003, 4251, 1010, 8701, 1998, 4895, 29278, 18150, 10880, 1012, 102]
Predicted IDs: [101, 1996, 3463, 2003, 2825, 1010, 16166, 4487, 13939, 8625, 22471, 3468, 1998, 14477]
BLEU Score: 0.4000

 --- Epoch 111
Task: Classification | Acc: 97.64% | Avg Loss: 0.0664
Task: Reconstruction | Avg Loss: 2.4772 
MoE Balancing Loss: 12270.4684
Mutual Information | Avg Loss: -0.00381
Total Loss: 3.5620
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275769.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277032.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277084.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276944.88
Time elapsed: 10:09:43.344346

 --- Epoch 112
Task: Classification | Acc: 97.61% | Avg Loss: 0.0662
Task: Reconstruction | Avg Loss: 2.4772 
MoE Balancing Loss: 12273.3698
Mutual Information | Avg Loss: -0.00395
Total Loss: 3.7257
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279475.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277799.75
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278788.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278082.22
Time elapsed: 10:20:53.596759

 --- Epoch 113
Task: Classification | Acc: 97.58% | Avg Loss: 0.0682
Task: Reconstruction | Avg Loss: 2.4706 
MoE Balancing Loss: 12271.6529
Mutual Information | Avg Loss: -0.00396
Total Loss: 3.6363
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275847.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275517.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276409.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276178.50
Time elapsed: 10:23:27.981621

 --- Epoch 114
Task: Classification | Acc: 97.60% | Avg Loss: 0.0640
Task: Reconstruction | Avg Loss: 2.4477 
MoE Balancing Loss: 12270.0087
Mutual Information | Avg Loss: -0.00394
Total Loss: 3.6320
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276415.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276582.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276753.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276728.06
Time elapsed: 10:25:03.788628

 --- Epoch 115
Task: Classification | Acc: 97.65% | Avg Loss: 0.0631
Task: Reconstruction | Avg Loss: 2.4541 
MoE Balancing Loss: 12271.5832
Mutual Information | Avg Loss: -0.00387
Total Loss: 3.5948
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275577.38
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275771.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275884.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275342.47
Time elapsed: 10:26:39.068672

 --- Epoch 116
Task: Classification | Acc: 97.61% | Avg Loss: 0.0658
Task: Reconstruction | Avg Loss: 2.4450 
MoE Balancing Loss: 12275.5931
Mutual Information | Avg Loss: -0.00392
Total Loss: 3.6735
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279986.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276865.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277381.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277802.69
Time elapsed: 10:28:15.061652

 --- Epoch 117
Task: Classification | Acc: 97.68% | Avg Loss: 0.0649
Task: Reconstruction | Avg Loss: 2.4262 
MoE Balancing Loss: 12273.0120
Mutual Information | Avg Loss: -0.00400
Total Loss: 3.6982
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277385.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277312.84
Layer 2:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276747.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277183.94
Time elapsed: 10:29:51.275665

 --- Epoch 118
Task: Classification | Acc: 97.75% | Avg Loss: 0.0633
Task: Reconstruction | Avg Loss: 2.4229 
MoE Balancing Loss: 12272.2575
Mutual Information | Avg Loss: -0.00386
Total Loss: 3.6209
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276255.25
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277285.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277110.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276868.56
Time elapsed: 10:31:27.171925

 --- Epoch 119
Task: Classification | Acc: 97.66% | Avg Loss: 0.0650
Task: Reconstruction | Avg Loss: 2.4229 
MoE Balancing Loss: 12275.2025
Mutual Information | Avg Loss: -0.00413
Total Loss: 3.8165
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277460.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276123.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276324.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276590.00
Time elapsed: 10:33:03.793439

 --- Epoch 120
Task: Classification | Acc: 97.68% | Avg Loss: 0.0625
Task: Reconstruction | Avg Loss: 2.4089 
MoE Balancing Loss: 12274.8293
Mutual Information | Avg Loss: -0.00399
Total Loss: 3.6575
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277873.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276321.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277106.06
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277119.53
Time elapsed: 10:34:39.845171
Example 1 ---
Original text: a deep and meaningful film.
Reconstructed text: a visually and work.
Original IDs: [101, 1037, 2784, 1998, 15902, 2143, 1012, 102]
Predicted IDs: [101, 1037, 17453, 1998, 102, 2147, 1012, 102]
BLEU Score: 0.4912
Example 2 ---
Original text: a valueless kiddie paean to pro basketball underwritten by the nba.
Reconstructed text: the rev extended who iran to the own american ang video in the edge and but
Original IDs: [101, 1037, 3643, 3238, 25358, 2666, 6643, 11219, 2000, 4013, 3455, 2104, 15773, 2011, 1996, 6452, 1012, 102]
Predicted IDs: [101, 1996, 2128, 2615, 3668, 2040, 4238, 2000, 1996, 2219, 2137, 17076, 2678, 1999, 1996, 3341, 1998, 2021]
BLEU Score: 0.1250
Example 3 ---
Original text: harris commands the screen, using his frailty to suggest the ravages of a life of corruption and ruthlessness.
Reconstructed text: , ',,, film with lend, to and the ether spiritualcy of the characters landscape vietnamese. beauty the
Original IDs: [101, 5671, 10954, 1996, 3898, 1010, 2478, 2010, 25737, 3723, 2000, 6592, 1996, 10958, 3567, 8449, 1997, 1037, 2166, 1997, 7897, 1998, 18101, 2791, 1012, 102]
Predicted IDs: [101, 1010, 1005, 1010, 1010, 1010, 2143, 2007, 18496, 1010, 2000, 1998, 1996, 28855, 6259, 5666, 1997, 1996, 3494, 5957, 9101, 1012, 102, 5053, 102, 1996]
BLEU Score: 0.3182

 --- Epoch 121
Task: Classification | Acc: 97.82% | Avg Loss: 0.0615
Task: Reconstruction | Avg Loss: 2.3991 
MoE Balancing Loss: 12270.5402
Mutual Information | Avg Loss: -0.00398
Total Loss: 3.6912
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276644.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278016.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278308.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278786.72
Time elapsed: 10:36:16.321577

 --- Epoch 122
Task: Classification | Acc: 97.72% | Avg Loss: 0.0629
Task: Reconstruction | Avg Loss: 2.3863 
MoE Balancing Loss: 12269.0707
Mutual Information | Avg Loss: -0.00397
Total Loss: 3.5881
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276367.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276408.88
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277005.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276876.09
Time elapsed: 10:37:51.833470

 --- Epoch 123
Task: Classification | Acc: 97.58% | Avg Loss: 0.0678
Task: Reconstruction | Avg Loss: 2.3992 
MoE Balancing Loss: 12275.4186
Mutual Information | Avg Loss: -0.00404
Total Loss: 3.6327
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278272.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276949.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276818.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277198.09
Time elapsed: 10:39:27.832432

 --- Epoch 124
Task: Classification | Acc: 97.65% | Avg Loss: 0.0629
Task: Reconstruction | Avg Loss: 2.3930 
MoE Balancing Loss: 12275.1715
Mutual Information | Avg Loss: -0.00406
Total Loss: 3.6888
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277327.38
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276171.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277329.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276959.72
Time elapsed: 10:41:04.141809

 --- Epoch 125
Task: Classification | Acc: 97.62% | Avg Loss: 0.0668
Task: Reconstruction | Avg Loss: 2.3833 
MoE Balancing Loss: 12276.6206
Mutual Information | Avg Loss: -0.00417
Total Loss: 3.7332
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276449.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274825.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275169.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274934.12
Time elapsed: 10:42:40.491105

 --- Epoch 126
Task: Classification | Acc: 97.64% | Avg Loss: 0.0645
Task: Reconstruction | Avg Loss: 2.3858 
MoE Balancing Loss: 12274.5768
Mutual Information | Avg Loss: -0.00401
Total Loss: 3.6025
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278175.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277297.38
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276531.62
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277045.09
Time elapsed: 10:44:16.272431

 --- Epoch 127
Task: Classification | Acc: 97.73% | Avg Loss: 0.0621
Task: Reconstruction | Avg Loss: 2.3811 
MoE Balancing Loss: 12273.1566
Mutual Information | Avg Loss: -0.00400
Total Loss: 3.6252
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276275.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275978.72
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276593.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276128.16
Time elapsed: 10:45:52.134017

 --- Epoch 128
Task: Classification | Acc: 97.72% | Avg Loss: 0.0624
Task: Reconstruction | Avg Loss: 2.3696 
MoE Balancing Loss: 12275.1126
Mutual Information | Avg Loss: -0.00396
Total Loss: 3.6293
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277544.00
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276196.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277612.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277133.31
Time elapsed: 10:47:28.127317

 --- Epoch 129
Task: Classification | Acc: 97.64% | Avg Loss: 0.0654
Task: Reconstruction | Avg Loss: 2.3721 
MoE Balancing Loss: 12271.5110
Mutual Information | Avg Loss: -0.00393
Total Loss: 3.5834
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276776.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277017.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277517.62
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276973.59
Time elapsed: 10:49:03.879999

 --- Epoch 130
Task: Classification | Acc: 97.72% | Avg Loss: 0.0608
Task: Reconstruction | Avg Loss: 2.3570 
MoE Balancing Loss: 12268.0927
Mutual Information | Avg Loss: -0.00400
Total Loss: 3.6421
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275520.44
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276724.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276826.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277246.81
Time elapsed: 10:50:40.055925
Example 1 ---
Original text: kinnear doesn't aim for our sympathy, but rather delivers a performance of striking skill and depth.
Reconstructed text: the bourne just isn't afraid to theoint. but but also a work of spontaneous compromise and the performances that
Original IDs: [101, 12631, 22084, 2099, 2515, 1050, 1005, 1056, 6614, 2005, 2256, 11883, 1010, 2021, 2738, 18058, 1037, 2836, 1997, 8478, 8066, 1998, 5995, 1012, 102]
Predicted IDs: [101, 1996, 15803, 2074, 2003, 1050, 1005, 1056, 4452, 2000, 1996, 25785, 1012, 2021, 2021, 2036, 1037, 2147, 1997, 17630, 12014, 1998, 1996, 4616, 2008]
BLEU Score: 0.2857
Example 2 ---
Original text: it's slow - - very, very slow.
Reconstructed text: it's not - - or, too bad
Original IDs: [101, 2009, 1005, 1055, 4030, 1011, 1011, 2200, 1010, 2200, 4030, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 2025, 1011, 1011, 2030, 1010, 2205, 2919, 102, 102]
BLEU Score: 0.4971
Example 3 ---
Original text: a grimly competent and stolid and earnest military courtroom drama.
Reconstructed text: the crities and devols sick - time
Original IDs: [101, 1037, 22561, 17824, 1998, 2358, 10893, 2094, 1998, 17300, 2510, 20747, 3689, 1012, 102]
Predicted IDs: [101, 1996, 13675, 6447, 1998, 16475, 4747, 2015, 102, 5305, 1011, 2051, 102, 102, 102]
BLEU Score: 0.0807

 --- Epoch 131
Task: Classification | Acc: 97.73% | Avg Loss: 0.0650
Task: Reconstruction | Avg Loss: 2.3475 
MoE Balancing Loss: 12272.4833
Mutual Information | Avg Loss: -0.00401
Total Loss: 3.6704
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277797.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276794.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277174.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277539.69
Time elapsed: 10:52:16.672970

 --- Epoch 132
Task: Classification | Acc: 97.76% | Avg Loss: 0.0618
Task: Reconstruction | Avg Loss: 2.3419 
MoE Balancing Loss: 12275.7814
Mutual Information | Avg Loss: -0.00397
Total Loss: 3.5545
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277955.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278285.31
Layer 2:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276624.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277273.09
Time elapsed: 10:53:52.297410

 --- Epoch 133
Task: Classification | Acc: 97.72% | Avg Loss: 0.0643
Task: Reconstruction | Avg Loss: 2.3435 
MoE Balancing Loss: 12270.8071
Mutual Information | Avg Loss: -0.00395
Total Loss: 3.5904
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275405.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276093.00
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277176.62
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276211.41
Time elapsed: 10:55:28.095265

 --- Epoch 134
Task: Classification | Acc: 97.81% | Avg Loss: 0.0620
Task: Reconstruction | Avg Loss: 2.3296 
MoE Balancing Loss: 12272.3888
Mutual Information | Avg Loss: -0.00404
Total Loss: 3.6120
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276618.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275598.72
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275965.38
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276210.28
Time elapsed: 10:57:03.916818

 --- Epoch 135
Task: Classification | Acc: 97.80% | Avg Loss: 0.0617
Task: Reconstruction | Avg Loss: 2.3266 
MoE Balancing Loss: 12272.7258
Mutual Information | Avg Loss: -0.00388
Total Loss: 3.5218
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278053.84
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278253.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278019.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277703.41
Time elapsed: 10:58:39.746508

 --- Epoch 136
Task: Classification | Acc: 97.82% | Avg Loss: 0.0605
Task: Reconstruction | Avg Loss: 2.3346 
MoE Balancing Loss: 12270.6459
Mutual Information | Avg Loss: -0.00400
Total Loss: 3.5664
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275255.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275169.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275832.59
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275602.34
Time elapsed: 11:08:42.023490

 --- Epoch 137
Task: Classification | Acc: 97.76% | Avg Loss: 0.0630
Task: Reconstruction | Avg Loss: 2.3244 
MoE Balancing Loss: 12271.8225
Mutual Information | Avg Loss: -0.00405
Total Loss: 3.6183
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278205.78
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275521.72
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276614.38
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276822.88
Time elapsed: 11:19:51.636860

 --- Epoch 138
Task: Classification | Acc: 97.80% | Avg Loss: 0.0620
Task: Reconstruction | Avg Loss: 2.3070 
MoE Balancing Loss: 12272.4499
Mutual Information | Avg Loss: -0.00399
Total Loss: 3.6140
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278305.50
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278517.75
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278242.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278223.09
Time elapsed: 11:31:02.457729

 --- Epoch 139
Task: Classification | Acc: 97.79% | Avg Loss: 0.0589
Task: Reconstruction | Avg Loss: 2.3176 
MoE Balancing Loss: 12272.2683
Mutual Information | Avg Loss: -0.00403
Total Loss: 3.6131
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276625.84
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276205.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276266.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276761.25
Time elapsed: 11:42:11.395786

 --- Epoch 140
Task: Classification | Acc: 97.83% | Avg Loss: 0.0585
Task: Reconstruction | Avg Loss: 2.3061 
MoE Balancing Loss: 12270.8329
Mutual Information | Avg Loss: -0.00402
Total Loss: 3.5472
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275466.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275711.91
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276107.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276088.22
Time elapsed: 11:53:20.280014
Example 1 ---
Original text: it's a remarkably solid and subtly satirical tour de force.
Reconstructed text: it'is anieser and numb strong french fare...
Original IDs: [101, 2009, 1005, 1055, 1037, 17431, 5024, 1998, 28797, 17251, 2778, 2139, 2486, 1012, 102]
Predicted IDs: [101, 2009, 1005, 2003, 1037, 15580, 2121, 1998, 15903, 2844, 2413, 13258, 1012, 1012, 1012]
BLEU Score: 0.0758
Example 2 ---
Original text: while its careful pace and seemingly opaque story may not satisfy every moviegoer's appetite, the film's final scene is soaringly, transparently moving.
Reconstructed text: the the the film is is a running is the as the as the ritter's touched days the film's patience. colds is anagag..
Original IDs: [101, 2096, 2049, 6176, 6393, 1998, 9428, 28670, 2466, 2089, 2025, 13225, 2296, 3185, 3995, 2121, 1005, 1055, 18923, 1010, 1996, 2143, 1005, 1055, 2345, 3496, 2003, 23990, 2135, 1010, 13338, 2135, 3048, 1012, 102]
Predicted IDs: [101, 1996, 1996, 1996, 2143, 2003, 2003, 1037, 2770, 2003, 1996, 2004, 1996, 2004, 1996, 23168, 1005, 1055, 5028, 2420, 1996, 2143, 1005, 1055, 11752, 1012, 3147, 2015, 2003, 2019, 8490, 8490, 1012, 1012, 102]
BLEU Score: 0.2222
Example 3 ---
Original text: this is human comedy at its most amusing, interesting and confirming.
Reconstructed text: unfortunately is a movie with the girl as a cold, repetitive,
Original IDs: [101, 2023, 2003, 2529, 4038, 2012, 2049, 2087, 19142, 1010, 5875, 1998, 19195, 1012, 102]
Predicted IDs: [101, 6854, 2003, 1037, 3185, 2007, 1996, 2611, 2004, 1037, 3147, 1010, 23563, 1010, 102]
BLEU Score: 0.1538

 --- Epoch 141
Task: Classification | Acc: 97.80% | Avg Loss: 0.0602
Task: Reconstruction | Avg Loss: 2.3025 
MoE Balancing Loss: 12272.7469
Mutual Information | Avg Loss: -0.00419
Total Loss: 3.7067
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277164.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275856.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275956.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275740.97
Time elapsed: 12:04:32.063577

 --- Epoch 142
Task: Classification | Acc: 97.84% | Avg Loss: 0.0608
Task: Reconstruction | Avg Loss: 2.3004 
MoE Balancing Loss: 12272.7373
Mutual Information | Avg Loss: -0.00405
Total Loss: 3.5840
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278137.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276766.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277188.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277565.69
Time elapsed: 12:15:41.094242

 --- Epoch 143
Task: Classification | Acc: 97.91% | Avg Loss: 0.0578
Task: Reconstruction | Avg Loss: 2.2989 
MoE Balancing Loss: 12270.0912
Mutual Information | Avg Loss: -0.00413
Total Loss: 3.5847
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278140.78
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278021.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278194.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277759.72
Time elapsed: 12:26:50.874091

 --- Epoch 144
Task: Classification | Acc: 97.87% | Avg Loss: 0.0583
Task: Reconstruction | Avg Loss: 2.2874 
MoE Balancing Loss: 12273.7463
Mutual Information | Avg Loss: -0.00419
Total Loss: 3.5878
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276668.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275383.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275297.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275737.56
Time elapsed: 12:34:33.384996

 --- Epoch 145
Task: Classification | Acc: 97.88% | Avg Loss: 0.0574
Task: Reconstruction | Avg Loss: 2.2806 
MoE Balancing Loss: 12273.1924
Mutual Information | Avg Loss: -0.00418
Total Loss: 3.6050
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278255.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277102.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278214.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277873.78
Time elapsed: 12:45:42.713756

 --- Epoch 146
Task: Classification | Acc: 97.95% | Avg Loss: 0.0553
Task: Reconstruction | Avg Loss: 2.2893 
MoE Balancing Loss: 12271.8066
Mutual Information | Avg Loss: -0.00409
Total Loss: 3.5582
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277719.78
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277840.75
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277021.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277262.66
Time elapsed: 12:56:51.968397

 --- Epoch 147
Task: Classification | Acc: 97.91% | Avg Loss: 0.0563
Task: Reconstruction | Avg Loss: 2.2781 
MoE Balancing Loss: 12275.6199
Mutual Information | Avg Loss: -0.00405
Total Loss: 3.6049
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277106.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276862.47
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276572.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276786.69
Time elapsed: 13:08:01.407421

 --- Epoch 148
Task: Classification | Acc: 97.92% | Avg Loss: 0.0599
Task: Reconstruction | Avg Loss: 2.2891 
MoE Balancing Loss: 12272.8717
Mutual Information | Avg Loss: -0.00406
Total Loss: 3.5991
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276246.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275118.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275316.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275794.16
Time elapsed: 13:19:11.343354

 --- Epoch 149
Task: Classification | Acc: 97.93% | Avg Loss: 0.0577
Task: Reconstruction | Avg Loss: 2.2709 
MoE Balancing Loss: 12273.9106
Mutual Information | Avg Loss: -0.00409
Total Loss: 3.5719
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276977.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276485.56
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276797.12
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276295.03
Time elapsed: 13:30:21.233036

 --- Epoch 150
Task: Classification | Acc: 97.96% | Avg Loss: 0.0558
Task: Reconstruction | Avg Loss: 2.2552 
MoE Balancing Loss: 12273.3386
Mutual Information | Avg Loss: -0.00412
Total Loss: 3.5878
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276494.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276990.06
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276262.69
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276181.50
Time elapsed: 13:41:30.360357
Example 1 ---
Original text: pumpkin takes an admirable look at the hypocrisy of political correctness, but it does so with such an uneven tone that you never know when humor ends and tragedy begins.
Reconstructed text: be more as a deby of in the dufiesten aspects of the movie battles can happen pulls even if they is anything ` time. time.
Original IDs: [101, 25730, 3138, 2019, 4748, 14503, 3085, 2298, 2012, 1996, 1044, 22571, 10085, 6935, 2100, 1997, 2576, 6149, 2791, 1010, 2021, 2009, 2515, 2061, 2007, 2107, 2019, 17837, 4309, 2008, 2017, 2196, 2113, 2043, 8562, 4515, 1998, 10576, 4269, 1012, 102]
Predicted IDs: [101, 2022, 2062, 2004, 1037, 2139, 3762, 1997, 1999, 1996, 4241, 14213, 6528, 5919, 1997, 1996, 3185, 2645, 2015, 102, 102, 102, 2064, 4148, 102, 102, 102, 8005, 2130, 2065, 2027, 2003, 2505, 1036, 2051, 1012, 102, 102, 2051, 1012, 102]
BLEU Score: 0.0923
Example 2 ---
Original text: it's a work by an artist so in control of both his medium and his message that he can improvise like a jazzman.
Reconstructed text: it's not to for the same of the lives of in conde scenario, that that as the never catceraions in the parental villain.
Original IDs: [101, 2009, 1005, 1055, 1037, 2147, 2011, 2019, 3063, 2061, 1999, 2491, 1997, 2119, 2010, 5396, 1998, 2010, 4471, 2008, 2002, 2064, 17727, 12298, 5562, 2066, 1037, 4166, 2386, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 2025, 2000, 2005, 1996, 2168, 1997, 1996, 3268, 1997, 1999, 24707, 11967, 1010, 2008, 2008, 2004, 1996, 2196, 4937, 19357, 8496, 1999, 1996, 18643, 12700, 1012, 102]
BLEU Score: 0.2308
Example 3 ---
Original text: it's a lovely film with lovely performances by buy and accorsi.
Reconstructed text: it's a disaster material with a summer is find and desabing.
Original IDs: [101, 2009, 1005, 1055, 1037, 8403, 2143, 2007, 8403, 4616, 2011, 4965, 1998, 16222, 5668, 2072, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 1037, 7071, 3430, 2007, 1037, 2621, 2003, 2424, 1998, 4078, 7875, 2075, 1012, 102]
BLEU Score: 0.4615

 --- Epoch 151
Task: Classification | Acc: 97.89% | Avg Loss: 0.0570
Task: Reconstruction | Avg Loss: 2.2525 
MoE Balancing Loss: 12273.3686
Mutual Information | Avg Loss: -0.00406
Total Loss: 3.5918
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276652.16
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276554.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276808.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277351.88
Time elapsed: 13:52:40.703295

 --- Epoch 152
Task: Classification | Acc: 97.92% | Avg Loss: 0.0590
Task: Reconstruction | Avg Loss: 2.2401 
MoE Balancing Loss: 12272.0422
Mutual Information | Avg Loss: -0.00412
Total Loss: 3.5524
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276784.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276415.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276512.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275846.25
Time elapsed: 14:03:50.083624

 --- Epoch 153
Task: Classification | Acc: 97.88% | Avg Loss: 0.0582
Task: Reconstruction | Avg Loss: 2.2398 
MoE Balancing Loss: 12270.1562
Mutual Information | Avg Loss: -0.00416
Total Loss: 3.6462
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276582.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276016.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277741.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277703.28
Time elapsed: 14:05:38.252185

 --- Epoch 154
Task: Classification | Acc: 97.87% | Avg Loss: 0.0583
Task: Reconstruction | Avg Loss: 2.2422 
MoE Balancing Loss: 12272.5705
Mutual Information | Avg Loss: -0.00421
Total Loss: 3.5150
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277618.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277457.22
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276889.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277288.06
Time elapsed: 14:07:14.075820

 --- Epoch 155
Task: Classification | Acc: 97.91% | Avg Loss: 0.0579
Task: Reconstruction | Avg Loss: 2.2310 
MoE Balancing Loss: 12275.4727
Mutual Information | Avg Loss: -0.00416
Total Loss: 3.5600
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276155.62
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277372.88
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277115.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277133.25
Time elapsed: 14:08:50.167855

 --- Epoch 156
Task: Classification | Acc: 97.94% | Avg Loss: 0.0552
Task: Reconstruction | Avg Loss: 2.2379 
MoE Balancing Loss: 12273.0438
Mutual Information | Avg Loss: -0.00421
Total Loss: 3.5653
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278407.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277210.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277103.34
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277102.78
Time elapsed: 14:10:26.116272

 --- Epoch 157
Task: Classification | Acc: 97.95% | Avg Loss: 0.0586
Task: Reconstruction | Avg Loss: 2.2361 
MoE Balancing Loss: 12271.1150
Mutual Information | Avg Loss: -0.00417
Total Loss: 3.5620
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276670.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275727.44
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276038.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276182.47
Time elapsed: 14:12:02.054827

 --- Epoch 158
Task: Classification | Acc: 97.95% | Avg Loss: 0.0591
Task: Reconstruction | Avg Loss: 2.2162 
MoE Balancing Loss: 12280.1349
Mutual Information | Avg Loss: -0.00422
Total Loss: 3.6027
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277190.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276278.25
Layer 2:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277522.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276679.41
Time elapsed: 14:13:38.852799

 --- Epoch 159
Task: Classification | Acc: 98.13% | Avg Loss: 0.0556
Task: Reconstruction | Avg Loss: 2.2223 
MoE Balancing Loss: 12273.9618
Mutual Information | Avg Loss: -0.00421
Total Loss: 3.5619
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277365.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277427.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277423.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276748.84
Time elapsed: 14:15:15.012640

 --- Epoch 160
Task: Classification | Acc: 97.94% | Avg Loss: 0.0571
Task: Reconstruction | Avg Loss: 2.2285 
MoE Balancing Loss: 12271.4243
Mutual Information | Avg Loss: -0.00407
Total Loss: 3.4884
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276818.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276567.56
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277583.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277264.69
Time elapsed: 14:16:50.551748
Example 1 ---
Original text: the vivid lead performances sustain interest and empathy, but the journey is far more interesting than the final destination.
Reconstructed text: is it that s than to get well - up to has as a more alien on the own row
Original IDs: [101, 1996, 14954, 2599, 4616, 15770, 3037, 1998, 26452, 1010, 2021, 1996, 4990, 2003, 2521, 2062, 5875, 2084, 1996, 2345, 7688, 1012, 102]
Predicted IDs: [101, 2003, 2009, 2008, 1055, 2084, 2000, 2131, 2092, 1011, 2039, 2000, 2038, 2004, 1037, 2062, 7344, 2006, 1996, 2219, 5216, 102, 102]
BLEU Score: 0.1902
Example 2 ---
Original text: the director knows how to apply textural gloss, but his portrait of sex - as - war is strictly sitcom.
Reconstructed text: the think really likely to downtown a the distance,, a bunch of paint - year - something -
Original IDs: [101, 1996, 2472, 4282, 2129, 2000, 6611, 3793, 11137, 27068, 1010, 2021, 2010, 6533, 1997, 3348, 1011, 2004, 1011, 2162, 2003, 9975, 13130, 1012, 102]
Predicted IDs: [101, 1996, 2228, 2428, 3497, 2000, 5116, 1037, 1996, 3292, 1010, 1010, 1037, 9129, 1997, 6773, 1011, 2095, 1011, 2242, 1011, 102, 102, 102, 102]
BLEU Score: 0.2715
Example 3 ---
Original text: some of their jokes work, but most fail miserably and in the end, pumpkin is far more offensive than it is funny.
Reconstructed text: one of the most plotting and most un untiveved., the the good, that that seem is clever too long to home.
Original IDs: [101, 2070, 1997, 2037, 13198, 2147, 1010, 2021, 2087, 8246, 28616, 6906, 6321, 1998, 1999, 1996, 2203, 1010, 25730, 2003, 2521, 2062, 5805, 2084, 2009, 2003, 6057, 1012, 102]
Predicted IDs: [101, 2028, 1997, 1996, 2087, 20699, 1998, 2087, 4895, 4895, 6024, 7178, 1012, 1010, 1996, 1996, 2204, 1010, 2008, 2008, 4025, 2003, 12266, 2205, 2146, 2000, 2188, 1012, 102]
BLEU Score: 0.3197

 --- Epoch 161
Task: Classification | Acc: 98.05% | Avg Loss: 0.0524
Task: Reconstruction | Avg Loss: 2.2059 
MoE Balancing Loss: 12274.4686
Mutual Information | Avg Loss: -0.00426
Total Loss: 3.4862
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276973.16
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277440.81
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276948.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277417.66
Time elapsed: 14:18:26.366913

 --- Epoch 162
Task: Classification | Acc: 98.07% | Avg Loss: 0.0546
Task: Reconstruction | Avg Loss: 2.2151 
MoE Balancing Loss: 12272.2358
Mutual Information | Avg Loss: -0.00428
Total Loss: 3.5772
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278215.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277391.06
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277424.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276701.06
Time elapsed: 14:20:02.634140

 --- Epoch 163
Task: Classification | Acc: 97.92% | Avg Loss: 0.0582
Task: Reconstruction | Avg Loss: 2.2067 
MoE Balancing Loss: 12272.2156
Mutual Information | Avg Loss: -0.00416
Total Loss: 3.4658
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275056.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275699.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276420.59
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276327.66
Time elapsed: 14:21:38.204927

 --- Epoch 164
Task: Classification | Acc: 98.08% | Avg Loss: 0.0527
Task: Reconstruction | Avg Loss: 2.1895 
MoE Balancing Loss: 12279.3305
Mutual Information | Avg Loss: -0.00418
Total Loss: 3.5575
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278329.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277373.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276839.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276843.59
Time elapsed: 14:23:14.727559

 --- Epoch 165
Task: Classification | Acc: 98.04% | Avg Loss: 0.0545
Task: Reconstruction | Avg Loss: 2.2072 
MoE Balancing Loss: 12275.1775
Mutual Information | Avg Loss: -0.00422
Total Loss: 3.5008
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277993.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275231.66
Layer 2:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277051.38
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276498.56
Time elapsed: 14:24:50.612323

 --- Epoch 166
Task: Classification | Acc: 98.01% | Avg Loss: 0.0574
Task: Reconstruction | Avg Loss: 2.1934 
MoE Balancing Loss: 12270.8108
Mutual Information | Avg Loss: -0.00422
Total Loss: 3.5475
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275420.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277418.75
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277563.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277109.50
Time elapsed: 14:26:26.896568

 --- Epoch 167
Task: Classification | Acc: 98.12% | Avg Loss: 0.0533
Task: Reconstruction | Avg Loss: 2.2012 
MoE Balancing Loss: 12271.8591
Mutual Information | Avg Loss: -0.00432
Total Loss: 3.5567
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276832.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276724.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277415.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277089.56
Time elapsed: 14:28:03.348905

 --- Epoch 168
Task: Classification | Acc: 98.04% | Avg Loss: 0.0534
Task: Reconstruction | Avg Loss: 2.1827 
MoE Balancing Loss: 12276.4082
Mutual Information | Avg Loss: -0.00440
Total Loss: 3.5233
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277122.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274748.19
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275367.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275437.09
Time elapsed: 14:29:39.291649

 --- Epoch 169
Task: Classification | Acc: 98.11% | Avg Loss: 0.0522
Task: Reconstruction | Avg Loss: 2.1870 
MoE Balancing Loss: 12274.9423
Mutual Information | Avg Loss: -0.00436
Total Loss: 3.5169
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276986.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277644.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276568.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276340.84
Time elapsed: 14:31:15.269207

 --- Epoch 170
Task: Classification | Acc: 98.05% | Avg Loss: 0.0547
Task: Reconstruction | Avg Loss: 2.1616 
MoE Balancing Loss: 12275.7662
Mutual Information | Avg Loss: -0.00431
Total Loss: 3.6022
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277811.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277231.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277743.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277562.50
Time elapsed: 14:32:52.121531
Example 1 ---
Original text: without ever becoming didactic, director carlos carrera expertly weaves this novelistic story of entangled interrelationships and complex morality.
Reconstructed text: is a a a storyline, a a the film,ss with theobful tract of virtuallyucty meanderings and desire.
Original IDs: [101, 2302, 2412, 3352, 2106, 28804, 1010, 2472, 5828, 12385, 6906, 6739, 2135, 25308, 2015, 2023, 9974, 2594, 2466, 1997, 4372, 27898, 6970, 16570, 10708, 19801, 1998, 3375, 16561, 1012, 102]
Predicted IDs: [101, 2003, 1037, 1037, 1037, 9994, 1010, 1037, 1037, 1996, 2143, 1010, 2015, 2015, 2007, 1996, 16429, 3993, 12859, 1997, 8990, 14194, 3723, 2812, 4063, 8613, 1998, 4792, 1012, 102, 102]
BLEU Score: 0.1905
Example 2 ---
Original text: the film's performances are thrilling.
Reconstructed text: the movie's not is narration.
Original IDs: [101, 1996, 2143, 1005, 1055, 4616, 2024, 26162, 1012, 102]
Predicted IDs: [101, 1996, 3185, 1005, 1055, 2025, 2003, 21283, 1012, 102]
BLEU Score: 0.4286
Example 3 ---
Original text: more whiny downer than corruscating commentary.
Reconstructed text: becomesly coldful band than as an intostaious present.
Original IDs: [101, 2062, 1059, 10606, 2100, 2091, 2121, 2084, 2522, 12171, 2271, 18252, 8570, 1012, 102]
Predicted IDs: [101, 4150, 2135, 3147, 3993, 2316, 2084, 2004, 2019, 2046, 9153, 6313, 2556, 1012, 102]
BLEU Score: 0.2222

 --- Epoch 171
Task: Classification | Acc: 98.08% | Avg Loss: 0.0540
Task: Reconstruction | Avg Loss: 2.1756 
MoE Balancing Loss: 12274.5066
Mutual Information | Avg Loss: -0.00424
Total Loss: 3.4569
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277735.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276771.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277091.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277300.78
Time elapsed: 14:34:28.156136

 --- Epoch 172
Task: Classification | Acc: 98.16% | Avg Loss: 0.0529
Task: Reconstruction | Avg Loss: 2.1756 
MoE Balancing Loss: 12279.0735
Mutual Information | Avg Loss: -0.00437
Total Loss: 3.5485
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277757.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275666.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276619.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276357.12
Time elapsed: 14:36:04.487064

 --- Epoch 173
Task: Classification | Acc: 98.20% | Avg Loss: 0.0516
Task: Reconstruction | Avg Loss: 2.1581 
MoE Balancing Loss: 12273.5209
Mutual Information | Avg Loss: -0.00434
Total Loss: 3.4742
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275922.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277149.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276418.41
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276618.94
Time elapsed: 14:37:40.530500

 --- Epoch 174
Task: Classification | Acc: 97.98% | Avg Loss: 0.0538
Task: Reconstruction | Avg Loss: 2.1589 
MoE Balancing Loss: 12272.4011
Mutual Information | Avg Loss: -0.00439
Total Loss: 3.4909
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277682.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277500.75
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277687.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276807.38
Time elapsed: 14:39:16.738705

 --- Epoch 175
Task: Classification | Acc: 98.04% | Avg Loss: 0.0546
Task: Reconstruction | Avg Loss: 2.1614 
MoE Balancing Loss: 12275.3043
Mutual Information | Avg Loss: -0.00440
Total Loss: 3.4691
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277132.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277118.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277629.50
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277650.16
Time elapsed: 14:40:52.763437

 --- Epoch 176
Task: Classification | Acc: 98.05% | Avg Loss: 0.0517
Task: Reconstruction | Avg Loss: 2.1555 
MoE Balancing Loss: 12273.7708
Mutual Information | Avg Loss: -0.00447
Total Loss: 3.5556
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276773.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277188.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276738.12
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277296.53
Time elapsed: 14:42:29.395545

 --- Epoch 177
Task: Classification | Acc: 97.86% | Avg Loss: 0.0596
Task: Reconstruction | Avg Loss: 2.1469 
MoE Balancing Loss: 12269.9867
Mutual Information | Avg Loss: -0.00443
Total Loss: 3.5426
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276118.00
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276372.56
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277053.12
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277169.16
Time elapsed: 14:44:05.999783

 --- Epoch 178
Task: Classification | Acc: 97.95% | Avg Loss: 0.0571
Task: Reconstruction | Avg Loss: 2.1413 
MoE Balancing Loss: 12270.1722
Mutual Information | Avg Loss: -0.00437
Total Loss: 3.5668
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278381.69
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278391.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277690.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278273.03
Time elapsed: 14:45:42.804549

 --- Epoch 179
Task: Classification | Acc: 98.06% | Avg Loss: 0.0538
Task: Reconstruction | Avg Loss: 2.1320 
MoE Balancing Loss: 12276.1430
Mutual Information | Avg Loss: -0.00441
Total Loss: 3.5099
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277239.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276842.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277198.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276766.16
Time elapsed: 14:47:19.109471

 --- Epoch 180
Task: Classification | Acc: 98.02% | Avg Loss: 0.0557
Task: Reconstruction | Avg Loss: 2.1486 
MoE Balancing Loss: 12272.4197
Mutual Information | Avg Loss: -0.00431
Total Loss: 3.4679
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276601.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276193.19
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277280.12
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277327.72
Time elapsed: 14:48:54.991831
Example 1 ---
Original text: it's a much more emotional journey than what shyamalan has given us in his past two movies, and gibson, stepping in for bruce willis, is the perfect actor to take us on the trip.
Reconstructed text: it's probably scientific effort to have a movie, the,,, except in the hee star,,,, the, than a three around, dramatic and allows wants to be id the neutral.
Original IDs: [101, 2009, 1005, 1055, 1037, 2172, 2062, 6832, 4990, 2084, 2054, 11004, 8067, 5802, 2038, 2445, 2149, 1999, 2010, 2627, 2048, 5691, 1010, 1998, 9406, 1010, 9085, 1999, 2005, 5503, 12688, 1010, 2003, 1996, 3819, 3364, 2000, 2202, 2149, 2006, 1996, 4440, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 2763, 4045, 3947, 2000, 2031, 1037, 3185, 1010, 1996, 1010, 1010, 1010, 3272, 1999, 1996, 18235, 2732, 1010, 1010, 1010, 1010, 1996, 1010, 2084, 1037, 2093, 2105, 1010, 6918, 1998, 4473, 4122, 2000, 2022, 8909, 102, 1996, 8699, 1012, 102]
BLEU Score: 0.3250
Example 2 ---
Original text: ... mafia, rap stars and hood rats butt their ugly heads in a regurgitation of cinematic violence that gives brutal birth to an unlikely, but likable, hero. '
Reconstructed text: a a a performances, good of, presenting, to a genre to a into theish rhythm of the wonderful that so, between the modern telling, proves simultaneously harrowinglifting.
Original IDs: [101, 1012, 1012, 1012, 13897, 1010, 9680, 3340, 1998, 7415, 11432, 10007, 2037, 9200, 4641, 1999, 1037, 19723, 12514, 18557, 1997, 21014, 4808, 2008, 3957, 12077, 4182, 2000, 2019, 9832, 1010, 2021, 5622, 2912, 3468, 1010, 5394, 1012, 1005, 102]
Predicted IDs: [101, 1037, 1037, 1037, 4616, 1010, 2204, 1997, 1010, 10886, 1010, 2000, 1037, 6907, 2000, 1037, 2046, 1996, 4509, 6348, 1997, 1996, 6919, 2008, 2061, 1010, 2090, 1996, 2715, 4129, 1010, 16481, 7453, 24560, 2075, 102, 102, 26644, 1012, 102]
BLEU Score: 0.2424
Example 3 ---
Original text: add yet another hat to a talented head, clooney's a good director.
Reconstructed text: , is iningly with a weak ii and it doesn't a profound one,
Original IDs: [101, 5587, 2664, 2178, 6045, 2000, 1037, 10904, 2132, 1010, 18856, 7828, 3240, 1005, 1055, 1037, 2204, 2472, 1012, 102]
Predicted IDs: [101, 1010, 2003, 1999, 15787, 2007, 1037, 5410, 2462, 1998, 2009, 2515, 1050, 1005, 1056, 1037, 13769, 2028, 1010, 102]
BLEU Score: 0.2000

 --- Epoch 181
Task: Classification | Acc: 98.11% | Avg Loss: 0.0539
Task: Reconstruction | Avg Loss: 2.1387 
MoE Balancing Loss: 12274.3665
Mutual Information | Avg Loss: -0.00437
Total Loss: 3.4541
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276763.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277092.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277187.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276799.72
Time elapsed: 14:50:31.061657

 --- Epoch 182
Task: Classification | Acc: 98.12% | Avg Loss: 0.0520
Task: Reconstruction | Avg Loss: 2.1317 
MoE Balancing Loss: 12274.4559
Mutual Information | Avg Loss: -0.00434
Total Loss: 3.5092
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277344.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278071.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277327.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276969.25
Time elapsed: 14:52:07.619732

 --- Epoch 183
Task: Classification | Acc: 98.06% | Avg Loss: 0.0514
Task: Reconstruction | Avg Loss: 2.1319 
MoE Balancing Loss: 12270.7094
Mutual Information | Avg Loss: -0.00435
Total Loss: 3.5201
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276300.09
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275518.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275780.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276162.81
Time elapsed: 14:53:43.818152

 --- Epoch 184
Task: Classification | Acc: 98.11% | Avg Loss: 0.0538
Task: Reconstruction | Avg Loss: 2.1153 
MoE Balancing Loss: 12274.3831
Mutual Information | Avg Loss: -0.00437
Total Loss: 3.5250
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277255.69
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276232.56
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276779.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276825.19
Time elapsed: 14:55:20.068419

 --- Epoch 185
Task: Classification | Acc: 97.99% | Avg Loss: 0.0562
Task: Reconstruction | Avg Loss: 2.1260 
MoE Balancing Loss: 12271.4057
Mutual Information | Avg Loss: -0.00442
Total Loss: 3.3735
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278306.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277303.12
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277126.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277141.03
Time elapsed: 14:56:55.488728

 --- Epoch 186
Task: Classification | Acc: 98.27% | Avg Loss: 0.0499
Task: Reconstruction | Avg Loss: 2.1274 
MoE Balancing Loss: 12272.1176
Mutual Information | Avg Loss: -0.00440
Total Loss: 3.5050
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276699.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276274.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276971.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277223.66
Time elapsed: 14:58:31.892241

 --- Epoch 187
Task: Classification | Acc: 98.17% | Avg Loss: 0.0519
Task: Reconstruction | Avg Loss: 2.1152 
MoE Balancing Loss: 12276.2920
Mutual Information | Avg Loss: -0.00451
Total Loss: 3.5349
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277495.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276685.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277490.94
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276901.41
Time elapsed: 15:00:08.665330

 --- Epoch 188
Task: Classification | Acc: 98.08% | Avg Loss: 0.0539
Task: Reconstruction | Avg Loss: 2.1106 
MoE Balancing Loss: 12274.9665
Mutual Information | Avg Loss: -0.00452
Total Loss: 3.5135
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277393.91
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277395.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277118.56
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276736.12
Time elapsed: 15:01:45.213692

 --- Epoch 189
Task: Classification | Acc: 98.06% | Avg Loss: 0.0528
Task: Reconstruction | Avg Loss: 2.1110 
MoE Balancing Loss: 12273.2719
Mutual Information | Avg Loss: -0.00452
Total Loss: 3.4386
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278106.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278022.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279014.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278495.12
Time elapsed: 15:03:21.131593

 --- Epoch 190
Task: Classification | Acc: 98.12% | Avg Loss: 0.0542
Task: Reconstruction | Avg Loss: 2.1055 
MoE Balancing Loss: 12272.4816
Mutual Information | Avg Loss: -0.00450
Total Loss: 3.5303
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277433.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277046.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276840.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277292.12
Time elapsed: 15:04:57.906500
Example 1 ---
Original text: this is a shameless sham, calculated to cash in on the popularity of its stars.
Reconstructed text: may be a kn doingpromising that urge to be oute the all of its own
Original IDs: [101, 2023, 2003, 1037, 9467, 3238, 25850, 1010, 10174, 2000, 5356, 1999, 2006, 1996, 6217, 1997, 2049, 3340, 1012, 102]
Predicted IDs: [101, 2089, 2022, 1037, 14161, 2725, 25013, 2008, 9075, 2000, 2022, 2041, 2063, 1996, 2035, 1997, 2049, 2219, 102, 102]
BLEU Score: 0.2917
Example 2 ---
Original text: the film tunes into a grief that could lead a man across centuries.
Reconstructed text: shirkss about the film that might has a price of excitement, but
Original IDs: [101, 1996, 2143, 13281, 2046, 1037, 9940, 2008, 2071, 2599, 1037, 2158, 2408, 4693, 1012, 102]
Predicted IDs: [101, 11895, 19987, 2015, 2055, 1996, 2143, 2008, 2453, 2038, 1037, 3976, 1997, 8277, 1010, 2021]
BLEU Score: 0.2849
Example 3 ---
Original text: one of the smartest takes on singles culture i've seen in a long time.
Reconstructed text: is that the the previous observation in the ` you've completely sitting this `
Original IDs: [101, 2028, 1997, 1996, 6047, 4355, 3138, 2006, 3895, 3226, 1045, 1005, 2310, 2464, 1999, 1037, 2146, 2051, 1012, 102]
Predicted IDs: [101, 2003, 2008, 1996, 1996, 3025, 8089, 1999, 1996, 1036, 2017, 1005, 2310, 3294, 3564, 2023, 102, 1036, 102, 102]
BLEU Score: 0.1871

 --- Epoch 191
Task: Classification | Acc: 98.22% | Avg Loss: 0.0510
Task: Reconstruction | Avg Loss: 2.0917 
MoE Balancing Loss: 12270.5968
Mutual Information | Avg Loss: -0.00441
Total Loss: 3.4872
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277671.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278151.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278552.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278038.12
Time elapsed: 15:06:34.612163

 --- Epoch 192
Task: Classification | Acc: 98.07% | Avg Loss: 0.0524
Task: Reconstruction | Avg Loss: 2.0974 
MoE Balancing Loss: 12277.5411
Mutual Information | Avg Loss: -0.00454
Total Loss: 3.5179
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278700.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277682.56
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277416.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277390.25
Time elapsed: 15:08:11.048041

 --- Epoch 193
Task: Classification | Acc: 97.98% | Avg Loss: 0.0558
Task: Reconstruction | Avg Loss: 2.0947 
MoE Balancing Loss: 12274.1035
Mutual Information | Avg Loss: -0.00451
Total Loss: 3.4869
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277051.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275896.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276725.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277153.88
Time elapsed: 15:09:47.381682

 --- Epoch 194
Task: Classification | Acc: 98.19% | Avg Loss: 0.0507
Task: Reconstruction | Avg Loss: 2.0966 
MoE Balancing Loss: 12274.9665
Mutual Information | Avg Loss: -0.00452
Total Loss: 3.5243
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277299.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277645.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277506.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277403.06
Time elapsed: 15:11:24.090505

 --- Epoch 195
Task: Classification | Acc: 98.18% | Avg Loss: 0.0513
Task: Reconstruction | Avg Loss: 2.0907 
MoE Balancing Loss: 12271.7526
Mutual Information | Avg Loss: -0.00444
Total Loss: 3.5100
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278290.16
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276597.16
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276954.31
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277016.56
Time elapsed: 15:13:00.774181

 --- Epoch 196
Task: Classification | Acc: 98.07% | Avg Loss: 0.0515
Task: Reconstruction | Avg Loss: 2.0895 
MoE Balancing Loss: 12273.3995
Mutual Information | Avg Loss: -0.00460
Total Loss: 3.5120
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279579.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279654.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278531.72
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277913.94
Time elapsed: 15:14:37.611643

 --- Epoch 197
Task: Classification | Acc: 98.03% | Avg Loss: 0.0530
Task: Reconstruction | Avg Loss: 2.0843 
MoE Balancing Loss: 12270.4681
Mutual Information | Avg Loss: -0.00447
Total Loss: 3.4993
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275412.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276164.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276773.12
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277266.75
Time elapsed: 15:16:14.181148

 --- Epoch 198
Task: Classification | Acc: 98.03% | Avg Loss: 0.0547
Task: Reconstruction | Avg Loss: 2.0758 
MoE Balancing Loss: 12271.3058
Mutual Information | Avg Loss: -0.00448
Total Loss: 3.4613
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278066.00
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277257.00
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276817.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276797.56
Time elapsed: 15:17:50.271632

 --- Epoch 199
Task: Classification | Acc: 98.02% | Avg Loss: 0.0541
Task: Reconstruction | Avg Loss: 2.0643 
MoE Balancing Loss: 12271.1238
Mutual Information | Avg Loss: -0.00450
Total Loss: 3.4627
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277635.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276783.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278373.69
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277628.47
Time elapsed: 15:19:26.487605

 --- Epoch 200
Task: Classification | Acc: 98.09% | Avg Loss: 0.0529
Task: Reconstruction | Avg Loss: 2.0874 
MoE Balancing Loss: 12275.1653
Mutual Information | Avg Loss: -0.00450
Total Loss: 3.5014
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275710.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276275.31
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276812.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276476.81
Time elapsed: 15:21:02.870642
Example 1 ---
Original text: every dance becomes about seduction, where backstabbing and betrayals are celebrated, and sex is currency.
Reconstructed text: the film has visually evelyn, more romanticie staged, litd as passion, in new relationships
Original IDs: [101, 2296, 3153, 4150, 2055, 26962, 1010, 2073, 10457, 2696, 23200, 1998, 14583, 2015, 2024, 6334, 1010, 1998, 3348, 2003, 9598, 1012, 102]
Predicted IDs: [101, 1996, 2143, 2038, 17453, 12903, 1010, 2062, 6298, 2666, 9813, 1010, 5507, 2094, 2004, 6896, 1010, 102, 102, 1999, 2047, 6550, 102]
BLEU Score: 0.1109
Example 2 ---
Original text: turns potentially forgettable formula into something strangely diverting.
Reconstructed text: , sac figurescre picture passed thisttered annoyinge
Original IDs: [101, 4332, 9280, 5293, 10880, 5675, 2046, 2242, 13939, 27345, 2075, 1012, 102]
Predicted IDs: [101, 1010, 17266, 4481, 16748, 3861, 2979, 2023, 14795, 15703, 2063, 102, 102]
BLEU Score: 0.0000
Example 3 ---
Original text: villeneuve spends too much time wallowing in bibi's generic angst ( there are a lot of shots of her gazing out windows ).
Reconstructed text: a pump life as approaches agent fatal anthony using gary summer's new ecological humor list themaxmax
Original IDs: [101, 20184, 28104, 15970, 2205, 2172, 2051, 2813, 14138, 1999, 12170, 5638, 1005, 1055, 12391, 17076, 3367, 1006, 2045, 2024, 1037, 2843, 1997, 7171, 1997, 2014, 16448, 2041, 3645, 1007, 1012, 102]
Predicted IDs: [101, 1037, 10216, 2166, 2004, 8107, 4005, 10611, 4938, 2478, 5639, 2621, 1005, 1055, 2047, 12231, 8562, 102, 102, 102, 102, 2862, 1996, 102, 102, 102, 17848, 17848, 102, 102, 102, 102]
BLEU Score: 0.0735

 --- Epoch 201
Task: Classification | Acc: 98.24% | Avg Loss: 0.0505
Task: Reconstruction | Avg Loss: 2.0724 
MoE Balancing Loss: 12272.4809
Mutual Information | Avg Loss: -0.00443
Total Loss: 3.5120
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275543.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276337.94
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276145.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276497.56
Time elapsed: 15:22:39.601067

 --- Epoch 202
Task: Classification | Acc: 98.21% | Avg Loss: 0.0514
Task: Reconstruction | Avg Loss: 2.0673 
MoE Balancing Loss: 12275.7062
Mutual Information | Avg Loss: -0.00438
Total Loss: 3.4343
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278114.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277340.41
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278271.34
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278200.59
Time elapsed: 15:24:15.665100

 --- Epoch 203
Task: Classification | Acc: 98.10% | Avg Loss: 0.0558
Task: Reconstruction | Avg Loss: 2.0661 
MoE Balancing Loss: 12271.1129
Mutual Information | Avg Loss: -0.00458
Total Loss: 3.4598
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275152.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276340.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276862.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276772.28
Time elapsed: 15:25:51.949528

 --- Epoch 204
Task: Classification | Acc: 98.04% | Avg Loss: 0.0545
Task: Reconstruction | Avg Loss: 2.0766 
MoE Balancing Loss: 12276.7241
Mutual Information | Avg Loss: -0.00459
Total Loss: 3.5268
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276348.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276488.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277299.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276548.81
Time elapsed: 15:27:28.688153

 --- Epoch 205
Task: Classification | Acc: 98.16% | Avg Loss: 0.0519
Task: Reconstruction | Avg Loss: 2.0601 
MoE Balancing Loss: 12274.0046
Mutual Information | Avg Loss: -0.00458
Total Loss: 3.4555
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278690.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277323.91
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278072.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277685.91
Time elapsed: 15:29:05.095936

 --- Epoch 206
Task: Classification | Acc: 98.18% | Avg Loss: 0.0514
Task: Reconstruction | Avg Loss: 2.0523 
MoE Balancing Loss: 12270.3363
Mutual Information | Avg Loss: -0.00460
Total Loss: 3.4314
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277739.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277496.00
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277898.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277935.19
Time elapsed: 15:30:41.008738

 --- Epoch 207
Task: Classification | Acc: 98.14% | Avg Loss: 0.0527
Task: Reconstruction | Avg Loss: 2.0455 
MoE Balancing Loss: 12273.2271
Mutual Information | Avg Loss: -0.00472
Total Loss: 3.4887
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275865.19
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277137.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275830.03
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275860.31
Time elapsed: 15:32:17.313349

 --- Epoch 208
Task: Classification | Acc: 98.18% | Avg Loss: 0.0509
Task: Reconstruction | Avg Loss: 2.0532 
MoE Balancing Loss: 12273.1415
Mutual Information | Avg Loss: -0.00463
Total Loss: 3.4775
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276884.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276890.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276845.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276640.50
Time elapsed: 15:33:53.657240

 --- Epoch 209
Task: Classification | Acc: 98.15% | Avg Loss: 0.0519
Task: Reconstruction | Avg Loss: 2.0382 
MoE Balancing Loss: 12273.2762
Mutual Information | Avg Loss: -0.00452
Total Loss: 3.4753
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275380.00
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275771.41
Layer 2:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275138.06
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276035.69
Time elapsed: 15:35:30.087892

 --- Epoch 210
Task: Classification | Acc: 98.13% | Avg Loss: 0.0547
Task: Reconstruction | Avg Loss: 2.0354 
MoE Balancing Loss: 12274.8619
Mutual Information | Avg Loss: -0.00463
Total Loss: 3.4894
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276139.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276680.28
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276432.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277492.78
Time elapsed: 15:37:06.759357
Example 1 ---
Original text: far more imaginative and ambitious than the trivial, cash - in features nickelodeon has made from its other animated tv series.
Reconstructed text: is to attempts, despite achieving a slightly,iche - interest kid that it it in a few yorkrk diversion and
Original IDs: [101, 2521, 2062, 28575, 1998, 12479, 2084, 1996, 20610, 1010, 5356, 1011, 1999, 2838, 20814, 2038, 2081, 2013, 2049, 2060, 6579, 2694, 2186, 1012, 102]
Predicted IDs: [101, 2003, 2000, 4740, 1010, 2750, 10910, 1037, 3621, 1010, 17322, 1011, 3037, 4845, 2008, 2009, 2009, 1999, 1037, 2261, 2259, 8024, 20150, 1998, 102]
BLEU Score: 0.1737
Example 2 ---
Original text: it gets onto the screen just about as much of the novella as one could reasonably expect, and is engrossing and moving in its own right.
Reconstructed text: is a has therates of few who in that who earth such more with brand on and it is a strongrocity and siity of fresh in the entire
Original IDs: [101, 2009, 4152, 3031, 1996, 3898, 2074, 2055, 2004, 2172, 1997, 1996, 20674, 2004, 2028, 2071, 16286, 5987, 1010, 1998, 2003, 25540, 25725, 2075, 1998, 3048, 1999, 2049, 2219, 2157, 1012, 102]
Predicted IDs: [101, 2003, 1037, 2038, 1996, 20370, 1997, 2261, 2040, 1999, 2008, 2040, 3011, 2107, 2062, 2007, 4435, 2006, 1998, 2009, 2003, 1037, 2844, 21735, 1998, 9033, 3012, 1997, 4840, 1999, 1996, 2972]
BLEU Score: 0.2500
Example 3 ---
Original text: the film's tone and pacing are off almost from the get - go.
Reconstructed text: the film's hero enough that about can in with a movie - jerking, nerve
Original IDs: [101, 1996, 2143, 1005, 1055, 4309, 1998, 15732, 2024, 2125, 2471, 2013, 1996, 2131, 1011, 2175, 1012, 102]
Predicted IDs: [101, 1996, 2143, 1005, 1055, 5394, 2438, 2008, 2055, 2064, 1999, 2007, 1037, 3185, 1011, 22387, 1010, 9113]
BLEU Score: 0.2500

 --- Epoch 211
Task: Classification | Acc: 98.17% | Avg Loss: 0.0507
Task: Reconstruction | Avg Loss: 2.0415 
MoE Balancing Loss: 12276.6279
Mutual Information | Avg Loss: -0.00456
Total Loss: 3.4615
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 280439.94
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277893.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277796.00
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277864.72
Time elapsed: 15:38:43.522239

 --- Epoch 212
Task: Classification | Acc: 98.23% | Avg Loss: 0.0515
Task: Reconstruction | Avg Loss: 2.0280 
MoE Balancing Loss: 12269.6334
Mutual Information | Avg Loss: -0.00454
Total Loss: 3.4576
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276506.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274857.03
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276082.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275949.00
Time elapsed: 15:40:19.882056

 --- Epoch 213
Task: Classification | Acc: 98.23% | Avg Loss: 0.0496
Task: Reconstruction | Avg Loss: 2.0467 
MoE Balancing Loss: 12271.4092
Mutual Information | Avg Loss: -0.00460
Total Loss: 3.4583
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276610.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276672.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277470.38
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276507.66
Time elapsed: 15:41:56.077626

 --- Epoch 214
Task: Classification | Acc: 98.33% | Avg Loss: 0.0497
Task: Reconstruction | Avg Loss: 2.0456 
MoE Balancing Loss: 12271.1958
Mutual Information | Avg Loss: -0.00458
Total Loss: 3.4239
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274733.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276785.03
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275981.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276699.41
Time elapsed: 15:43:32.057001

 --- Epoch 215
Task: Classification | Acc: 98.17% | Avg Loss: 0.0510
Task: Reconstruction | Avg Loss: 2.0275 
MoE Balancing Loss: 12272.4235
Mutual Information | Avg Loss: -0.00448
Total Loss: 3.4546
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276359.66
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277087.16
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277554.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276982.06
Time elapsed: 15:45:08.609033

 --- Epoch 216
Task: Classification | Acc: 98.24% | Avg Loss: 0.0494
Task: Reconstruction | Avg Loss: 2.0247 
MoE Balancing Loss: 12271.5439
Mutual Information | Avg Loss: -0.00455
Total Loss: 3.4402
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276241.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275790.62
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276510.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276615.19
Time elapsed: 15:46:44.802599

 --- Epoch 217
Task: Classification | Acc: 98.28% | Avg Loss: 0.0484
Task: Reconstruction | Avg Loss: 2.0115 
MoE Balancing Loss: 12273.5660
Mutual Information | Avg Loss: -0.00457
Total Loss: 3.4259
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276261.84
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275123.97
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275048.91
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275940.94
Time elapsed: 15:48:20.835866

 --- Epoch 218
Task: Classification | Acc: 98.26% | Avg Loss: 0.0500
Task: Reconstruction | Avg Loss: 2.0226 
MoE Balancing Loss: 12272.0040
Mutual Information | Avg Loss: -0.00465
Total Loss: 3.4835
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275341.03
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275636.25
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275576.94
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275749.38
Time elapsed: 15:49:57.192726

 --- Epoch 219
Task: Classification | Acc: 98.26% | Avg Loss: 0.0492
Task: Reconstruction | Avg Loss: 2.0203 
MoE Balancing Loss: 12272.6626
Mutual Information | Avg Loss: -0.00460
Total Loss: 3.4676
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277227.00
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276606.75
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276645.06
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276237.06
Time elapsed: 15:51:33.707783

 --- Epoch 220
Task: Classification | Acc: 98.05% | Avg Loss: 0.0523
Task: Reconstruction | Avg Loss: 2.0126 
MoE Balancing Loss: 12272.4346
Mutual Information | Avg Loss: -0.00463
Total Loss: 3.4388
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277001.12
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277455.00
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277214.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277332.16
Time elapsed: 15:53:09.992391
Example 1 ---
Original text: a misogynistic piece of filth that attempts to pass itself off as hip, young adult entertainment.
Reconstructed text: is a belookt out of editing that that want to be his with ferrer black and a disasterros
Original IDs: [101, 1037, 28616, 15707, 26942, 2594, 3538, 1997, 10882, 24658, 2008, 4740, 2000, 3413, 2993, 2125, 2004, 5099, 1010, 2402, 4639, 4024, 1012, 102]
Predicted IDs: [101, 2003, 1037, 19337, 14659, 2102, 2041, 1997, 9260, 2008, 2008, 2215, 2000, 2022, 2010, 2007, 28390, 2304, 1998, 1037, 7071, 7352, 102, 102]
BLEU Score: 0.2222
Example 2 ---
Original text: liotta put on 30 pounds for the role, and has completely transformed himself from his smooth, goodfellas image.
Reconstructed text: detriny and unprooughked the film that are make shake affection about their charactersdquentnce
Original IDs: [101, 5622, 14517, 2050, 2404, 2006, 2382, 7038, 2005, 1996, 2535, 1010, 1998, 2038, 3294, 8590, 2370, 2013, 2010, 5744, 1010, 2204, 23510, 3022, 3746, 1012, 102]
Predicted IDs: [101, 2139, 18886, 4890, 1998, 4895, 21572, 10593, 8126, 102, 1996, 2143, 2008, 2024, 2191, 6073, 12242, 2055, 2037, 3494, 102, 102, 2094, 15417, 5897, 102, 102]
BLEU Score: 0.0831
Example 3 ---
Original text: the weight of the piece, the unerring professionalism of the chilly production, and the fascination embedded in the lurid topic prove recommendation enough.
Reconstructed text: the remarkably the of film and and jarringing art of the modern emotional, and the sincere performance in an usual twistssm se se..
Original IDs: [101, 1996, 3635, 1997, 1996, 3538, 1010, 1996, 16655, 18807, 2658, 2964, 1997, 1996, 24222, 2537, 1010, 1998, 1996, 18987, 11157, 1999, 1996, 11320, 14615, 8476, 6011, 12832, 2438, 1012, 102]
Predicted IDs: [101, 1996, 17431, 1996, 1997, 2143, 1998, 1998, 15723, 4892, 2075, 2396, 1997, 1996, 2715, 6832, 1010, 1998, 1996, 18006, 2836, 1999, 2019, 5156, 21438, 6491, 7367, 7367, 1012, 1012, 102]
BLEU Score: 0.3459

 --- Epoch 221
Task: Classification | Acc: 98.12% | Avg Loss: 0.0525
Task: Reconstruction | Avg Loss: 2.0104 
MoE Balancing Loss: 12272.2248
Mutual Information | Avg Loss: -0.00471
Total Loss: 3.5224
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277658.78
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276730.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277375.19
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277409.03
Time elapsed: 15:59:51.638101

 --- Epoch 222
Task: Classification | Acc: 98.24% | Avg Loss: 0.0496
Task: Reconstruction | Avg Loss: 2.0119 
MoE Balancing Loss: 12269.1811
Mutual Information | Avg Loss: -0.00466
Total Loss: 3.3803
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278076.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276930.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277566.34
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277466.84
Time elapsed: 16:11:00.978184

 --- Epoch 223
Task: Classification | Acc: 98.32% | Avg Loss: 0.0501
Task: Reconstruction | Avg Loss: 2.0050 
MoE Balancing Loss: 12271.2565
Mutual Information | Avg Loss: -0.00457
Total Loss: 3.5195
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277189.88
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277222.59
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277071.91
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277099.50
Time elapsed: 16:22:12.001120

 --- Epoch 224
Task: Classification | Acc: 98.28% | Avg Loss: 0.0488
Task: Reconstruction | Avg Loss: 2.0110 
MoE Balancing Loss: 12270.8569
Mutual Information | Avg Loss: -0.00473
Total Loss: 3.4796
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277830.69
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278141.06
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277580.59
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277903.06
Time elapsed: 16:33:21.894703

 --- Epoch 225
Task: Classification | Acc: 98.23% | Avg Loss: 0.0490
Task: Reconstruction | Avg Loss: 1.9945 
MoE Balancing Loss: 12277.6263
Mutual Information | Avg Loss: -0.00468
Total Loss: 3.4581
Layer 0:
-- Expert usage: [0.2, 0.16, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277150.31
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275512.16
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275591.53
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275941.97
Time elapsed: 16:44:31.456372

 --- Epoch 226
Task: Classification | Acc: 98.35% | Avg Loss: 0.0475
Task: Reconstruction | Avg Loss: 1.9977 
MoE Balancing Loss: 12272.4741
Mutual Information | Avg Loss: -0.00466
Total Loss: 3.4139
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279054.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277503.81
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278023.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277379.34
Time elapsed: 16:55:41.093367

 --- Epoch 227
Task: Classification | Acc: 98.17% | Avg Loss: 0.0510
Task: Reconstruction | Avg Loss: 1.9945 
MoE Balancing Loss: 12270.4394
Mutual Information | Avg Loss: -0.00469
Total Loss: 3.4318
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276802.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276553.47
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276650.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277479.31
Time elapsed: 17:06:50.011617

 --- Epoch 228
Task: Classification | Acc: 98.29% | Avg Loss: 0.0481
Task: Reconstruction | Avg Loss: 1.9984 
MoE Balancing Loss: 12273.0088
Mutual Information | Avg Loss: -0.00468
Total Loss: 3.3922
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275882.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277247.47
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276362.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276011.81
Time elapsed: 17:17:07.276154

 --- Epoch 229
Task: Classification | Acc: 98.12% | Avg Loss: 0.0518
Task: Reconstruction | Avg Loss: 2.0142 
MoE Balancing Loss: 12273.9976
Mutual Information | Avg Loss: -0.00470
Total Loss: 3.3607
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276826.25
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275357.03
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276926.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276334.28
Time elapsed: 17:27:57.295142

 --- Epoch 230
Task: Classification | Acc: 98.14% | Avg Loss: 0.0494
Task: Reconstruction | Avg Loss: 1.9872 
MoE Balancing Loss: 12272.7987
Mutual Information | Avg Loss: -0.00469
Total Loss: 3.4939
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277840.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277656.53
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277365.78
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277514.94
Time elapsed: 17:38:47.894569
Example 1 ---
Original text: paid in full is so stale, in fact, that its most vibrant scene is one that uses clips from brian de palma's scarface.
Reconstructed text: though the heartbreak developmentndingtou traditional solemn explode obvious, academy shot pay testimony and whether like ass from among capden's shelf.
Original IDs: [101, 3825, 1999, 2440, 2003, 2061, 26729, 1010, 1999, 2755, 1010, 2008, 2049, 2087, 17026, 3496, 2003, 2028, 2008, 3594, 15281, 2013, 4422, 2139, 23985, 1005, 1055, 18982, 10732, 1012, 102]
Predicted IDs: [101, 2295, 1996, 27724, 2458, 15683, 24826, 3151, 19487, 15044, 5793, 1010, 2914, 2915, 3477, 10896, 1998, 3251, 2066, 2004, 2015, 2013, 2426, 6178, 4181, 1005, 1055, 11142, 1012, 102, 102]
BLEU Score: 0.1462
Example 2 ---
Original text: the best film about baseball to hit theaters since field of dreams.
Reconstructed text: a different ideas is slight to be called a kind of masterpiece.
Original IDs: [101, 1996, 2190, 2143, 2055, 3598, 2000, 2718, 12370, 2144, 2492, 1997, 5544, 1012, 102]
Predicted IDs: [101, 1037, 2367, 4784, 2003, 7263, 2000, 2022, 2170, 1037, 2785, 1997, 17743, 1012, 102]
BLEU Score: 0.2308
Example 3 ---
Original text: the experience of going to a film festival is a rewarding one ; the experiencing of sampling one through this movie is not.
Reconstructed text: the refusal of lacks of a film that that can be red dares the world that have, why the first exit can free
Original IDs: [101, 1996, 3325, 1997, 2183, 2000, 1037, 2143, 2782, 2003, 1037, 10377, 2075, 2028, 1025, 1996, 13417, 1997, 16227, 2028, 2083, 2023, 3185, 2003, 2025, 1012, 102]
Predicted IDs: [101, 1996, 13948, 1997, 14087, 1997, 1037, 2143, 2008, 2008, 2064, 2022, 2417, 8108, 2015, 1996, 2088, 2008, 2031, 1010, 2339, 1996, 2034, 6164, 2064, 2489, 102]
BLEU Score: 0.2500

 --- Epoch 231
Task: Classification | Acc: 98.22% | Avg Loss: 0.0521
Task: Reconstruction | Avg Loss: 1.9805 
MoE Balancing Loss: 12273.0358
Mutual Information | Avg Loss: -0.00471
Total Loss: 3.4329
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277212.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276660.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277736.16
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277588.22
Time elapsed: 17:48:51.501772

 --- Epoch 232
Task: Classification | Acc: 98.21% | Avg Loss: 0.0493
Task: Reconstruction | Avg Loss: 1.9856 
MoE Balancing Loss: 12278.5386
Mutual Information | Avg Loss: -0.00463
Total Loss: 3.4250
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279836.66
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278330.44
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277820.84
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277205.94
Time elapsed: 17:50:28.231778

 --- Epoch 233
Task: Classification | Acc: 98.17% | Avg Loss: 0.0475
Task: Reconstruction | Avg Loss: 1.9861 
MoE Balancing Loss: 12272.4750
Mutual Information | Avg Loss: -0.00467
Total Loss: 3.4595
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275504.84
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277249.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276354.91
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277371.75
Time elapsed: 17:52:04.959903

 --- Epoch 234
Task: Classification | Acc: 98.23% | Avg Loss: 0.0500
Task: Reconstruction | Avg Loss: 1.9853 
MoE Balancing Loss: 12279.9463
Mutual Information | Avg Loss: -0.00469
Total Loss: 3.4287
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279632.97
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277542.88
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276589.25
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277381.97
Time elapsed: 17:53:41.496378

 --- Epoch 235
Task: Classification | Acc: 98.27% | Avg Loss: 0.0508
Task: Reconstruction | Avg Loss: 1.9636 
MoE Balancing Loss: 12270.1535
Mutual Information | Avg Loss: -0.00473
Total Loss: 3.4411
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 274350.81
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276879.19
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277076.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276809.28
Time elapsed: 17:55:18.254505

 --- Epoch 236
Task: Classification | Acc: 98.30% | Avg Loss: 0.0482
Task: Reconstruction | Avg Loss: 1.9734 
MoE Balancing Loss: 12268.9143
Mutual Information | Avg Loss: -0.00473
Total Loss: 3.4154
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275345.72
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275349.91
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276229.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275992.09
Time elapsed: 17:56:54.541573

 --- Epoch 237
Task: Classification | Acc: 98.27% | Avg Loss: 0.0490
Task: Reconstruction | Avg Loss: 1.9774 
MoE Balancing Loss: 12267.9701
Mutual Information | Avg Loss: -0.00477
Total Loss: 3.5161
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276016.34
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276689.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276824.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276461.38
Time elapsed: 17:58:31.697137

 --- Epoch 238
Task: Classification | Acc: 98.38% | Avg Loss: 0.0481
Task: Reconstruction | Avg Loss: 1.9692 
MoE Balancing Loss: 12269.2483
Mutual Information | Avg Loss: -0.00481
Total Loss: 3.4162
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276469.28
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277216.72
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276175.81
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276285.59
Time elapsed: 18:00:08.114308

 --- Epoch 239
Task: Classification | Acc: 98.39% | Avg Loss: 0.0456
Task: Reconstruction | Avg Loss: 1.9626 
MoE Balancing Loss: 12269.0324
Mutual Information | Avg Loss: -0.00470
Total Loss: 3.3763
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276895.56
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275640.44
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276305.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276342.41
Time elapsed: 18:01:44.529337

 --- Epoch 240
Task: Classification | Acc: 98.41% | Avg Loss: 0.0458
Task: Reconstruction | Avg Loss: 1.9613 
MoE Balancing Loss: 12273.4863
Mutual Information | Avg Loss: -0.00475
Total Loss: 3.4489
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278562.53
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276849.56
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276963.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277423.53
Time elapsed: 18:03:21.552172
Example 1 ---
Original text: looks and feels like a project better suited for the small screen.
Reconstructed text: is,, in a non - industry way of pointless creates.
Original IDs: [101, 3504, 1998, 5683, 2066, 1037, 2622, 2488, 10897, 2005, 1996, 2235, 3898, 1012, 102]
Predicted IDs: [101, 2003, 1010, 1010, 1999, 1037, 2512, 1011, 3068, 2126, 1997, 23100, 9005, 1012, 102]
BLEU Score: 0.1538
Example 2 ---
Original text: the film's hackneyed message is not helped by the thin characterizations, nonexistent plot and pretentious visual style.
Reconstructed text: a tires's smart, of of proportions top in - core with story, cheat intovent into arts and faylistic sentimentality and
Original IDs: [101, 1996, 2143, 1005, 1055, 28425, 2098, 4471, 2003, 2025, 3271, 2011, 1996, 4857, 23191, 2015, 1010, 3904, 9048, 16173, 2102, 5436, 1998, 3653, 6528, 20771, 5107, 2806, 1012, 102]
Predicted IDs: [101, 1037, 13310, 1005, 1055, 6047, 1010, 1997, 1997, 19173, 2327, 1999, 1011, 4563, 2007, 2466, 1010, 21910, 2046, 15338, 2046, 2840, 1998, 6904, 8516, 6553, 23069, 3012, 102, 1998]
BLEU Score: 0.1304
Example 3 ---
Original text: a solid film... but more conscientious than it is truly stirring.
Reconstructed text: the major done.... instantly disorientable a of
Original IDs: [101, 1037, 5024, 2143, 1012, 1012, 1012, 2021, 2062, 9530, 11020, 11638, 6313, 2084, 2009, 2003, 5621, 18385, 1012, 102]
Predicted IDs: [101, 1996, 2350, 2589, 1012, 1012, 1012, 1012, 6880, 4487, 21748, 11638, 3085, 102, 102, 102, 1037, 102, 1997, 102]
BLEU Score: 0.0669

 --- Epoch 241
Task: Classification | Acc: 98.25% | Avg Loss: 0.0462
Task: Reconstruction | Avg Loss: 1.9613 
MoE Balancing Loss: 12264.8389
Mutual Information | Avg Loss: -0.00476
Total Loss: 3.4146
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 273575.41
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 275635.38
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276213.47
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276077.12
Time elapsed: 18:04:58.163389

 --- Epoch 242
Task: Classification | Acc: 98.27% | Avg Loss: 0.0474
Task: Reconstruction | Avg Loss: 1.9539 
MoE Balancing Loss: 12277.7758
Mutual Information | Avg Loss: -0.00473
Total Loss: 3.4722
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 281742.47
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278372.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278804.34
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278572.91
Time elapsed: 18:06:35.313351

 --- Epoch 243
Task: Classification | Acc: 98.26% | Avg Loss: 0.0496
Task: Reconstruction | Avg Loss: 1.9422 
MoE Balancing Loss: 12270.2008
Mutual Information | Avg Loss: -0.00479
Total Loss: 3.4218
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277149.75
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277469.66
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277150.09
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277544.81
Time elapsed: 18:08:12.040898

 --- Epoch 244
Task: Classification | Acc: 98.09% | Avg Loss: 0.0535
Task: Reconstruction | Avg Loss: 1.9589 
MoE Balancing Loss: 12274.2146
Mutual Information | Avg Loss: -0.00473
Total Loss: 3.3071
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277746.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276935.09
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277251.28
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277595.97
Time elapsed: 18:09:48.128728

 --- Epoch 245
Task: Classification | Acc: 98.21% | Avg Loss: 0.0494
Task: Reconstruction | Avg Loss: 1.9523 
MoE Balancing Loss: 12275.5036
Mutual Information | Avg Loss: -0.00468
Total Loss: 3.5193
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279720.38
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278743.81
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278049.44
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278489.97
Time elapsed: 18:11:25.941115

 --- Epoch 246
Task: Classification | Acc: 98.25% | Avg Loss: 0.0512
Task: Reconstruction | Avg Loss: 1.9520 
MoE Balancing Loss: 12274.1311
Mutual Information | Avg Loss: -0.00471
Total Loss: 3.4219
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277449.06
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277134.50
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277178.75
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276706.91
Time elapsed: 18:13:02.927887

 --- Epoch 247
Task: Classification | Acc: 98.25% | Avg Loss: 0.0512
Task: Reconstruction | Avg Loss: 1.9469 
MoE Balancing Loss: 12274.0943
Mutual Information | Avg Loss: -0.00465
Total Loss: 3.4524
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276885.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277832.84
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276825.22
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277711.00
Time elapsed: 18:14:40.072520

 --- Epoch 248
Task: Classification | Acc: 98.15% | Avg Loss: 0.0522
Task: Reconstruction | Avg Loss: 1.9509 
MoE Balancing Loss: 12272.9296
Mutual Information | Avg Loss: -0.00472
Total Loss: 3.4540
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276697.84
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277184.78
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278019.62
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277450.00
Time elapsed: 18:16:17.135263

 --- Epoch 249
Task: Classification | Acc: 98.14% | Avg Loss: 0.0513
Task: Reconstruction | Avg Loss: 1.9286 
MoE Balancing Loss: 12274.4018
Mutual Information | Avg Loss: -0.00486
Total Loss: 3.5120
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 279040.22
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278328.69
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 278138.88
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277936.84
Time elapsed: 18:17:54.897470

 --- Epoch 250
Task: Classification | Acc: 98.18% | Avg Loss: 0.0504
Task: Reconstruction | Avg Loss: 1.9462 
MoE Balancing Loss: 12276.4045
Mutual Information | Avg Loss: -0.00474
Total Loss: 3.4151
Layer 0:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 276960.59
Layer 1:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277382.34
Layer 2:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277454.97
Layer 3:
-- Expert usage: [0.2, 0.17, 0.14, 0.12, 0.11, 0.1, 0.09, 0.08], std: 277003.06
Time elapsed: 18:19:31.606958
Example 1 ---
Original text: writer / director joe carnahan's grimy crime drama is a manual of precinct cliches, but it moves fast enough to cover its clunky dialogue and lapses in logic.
Reconstructed text: seemingly in with served ofssenl's )ook of to accused childhooded of saturday undylusus sequences and with future in, comes fl ob resiveble and supermarket tabloid..
Original IDs: [101, 3213, 1013, 2472, 3533, 2482, 15272, 2319, 1005, 1055, 11844, 2100, 4126, 3689, 2003, 1037, 6410, 1997, 18761, 18856, 17322, 2015, 1010, 2021, 2009, 5829, 3435, 2438, 2000, 3104, 2049, 18856, 16814, 2100, 7982, 1998, 10876, 2229, 1999, 7961, 1012, 102]
Predicted IDs: [101, 9428, 1999, 2007, 2366, 1997, 14416, 2140, 1005, 1055, 1007, 14659, 1997, 2000, 5496, 5593, 2098, 1997, 5095, 6151, 8516, 2271, 2271, 10071, 1998, 2007, 2925, 1999, 1010, 3310, 13109, 27885, 2128, 12742, 3468, 1998, 17006, 24173, 1012, 102, 1012, 102]
BLEU Score: 0.1931
Example 2 ---
Original text: like you couldn't smell this turkey rotting from miles away.
Reconstructed text: i just didn't understand the single name responsible day
Original IDs: [101, 2066, 2017, 2071, 1050, 1005, 1056, 5437, 2023, 4977, 22005, 2013, 2661, 2185, 1012, 102]
Predicted IDs: [101, 1045, 2074, 2106, 1050, 1005, 1056, 3305, 1996, 2309, 2171, 3625, 2154, 102, 102, 102]
BLEU Score: 0.0819
Example 3 ---
Original text: a gorgeous, high - spirited musical from india that exquisitely blends music, dance, song, and high drama.
Reconstructed text: a rare, self - down a and but is end funny is darkly drama, suspense, revenge, and surprisingly. wide
Original IDs: [101, 1037, 9882, 1010, 2152, 1011, 24462, 3315, 2013, 2634, 2008, 19401, 2135, 12586, 2015, 2189, 1010, 3153, 1010, 2299, 1010, 1998, 2152, 3689, 1012, 102]
Predicted IDs: [101, 1037, 4678, 1010, 2969, 1011, 2091, 1037, 1998, 2021, 2003, 2203, 6057, 2003, 27148, 3689, 1010, 23873, 1010, 7195, 1010, 1998, 10889, 1012, 102, 2898]
BLEU Score: 0.3750

--- Final Test BLEU Score ---
Avg BLEU Score: 0.1541
