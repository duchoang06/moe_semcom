nohup: ignoring input
Training started at 2025-06-20 23:30:47
Training detail: learning rate: 1e-4, weight decay: 5e-3, total epochs: 250, batch size: 128, lambda_moe_lb: 0.0002, seed: 2006
Starting training...

 --- Epoch 1
Task: Classification | Acc: 61.17% | Avg Loss: 0.6667
Task: Reconstruction | Avg Loss: 6.4138 
MoE Balancing Loss: 10442.3499
Mutual Information | Avg Loss: -0.00000
Total Loss: 5.6778
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331370.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330965.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330975.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330550.66
Time elapsed: 0:01:46.667781

 --- Epoch 2
Task: Classification | Acc: 83.57% | Avg Loss: 0.3824
Task: Reconstruction | Avg Loss: 5.6377 
MoE Balancing Loss: 10426.8057
Mutual Information | Avg Loss: -0.00009
Total Loss: 5.2689
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332284.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332044.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331954.91
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332027.44
Time elapsed: 0:03:34.843945

 --- Epoch 3
Task: Classification | Acc: 85.29% | Avg Loss: 0.3428
Task: Reconstruction | Avg Loss: 5.3409 
MoE Balancing Loss: 10429.3575
Mutual Information | Avg Loss: -0.00044
Total Loss: 4.8616
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333357.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333187.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333076.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333043.88
Time elapsed: 0:05:22.990890

 --- Epoch 4
Task: Classification | Acc: 86.70% | Avg Loss: 0.3163
Task: Reconstruction | Avg Loss: 5.1518 
MoE Balancing Loss: 10429.1549
Mutual Information | Avg Loss: -0.00120
Total Loss: 4.6473
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331657.59
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331145.97
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331145.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331520.19
Time elapsed: 0:07:11.149548

 --- Epoch 5
Task: Classification | Acc: 86.78% | Avg Loss: 0.3114
Task: Reconstruction | Avg Loss: 4.9487 
MoE Balancing Loss: 10427.5398
Mutual Information | Avg Loss: -0.00222
Total Loss: 4.7066
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 328677.16
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329818.59
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329535.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329766.00
Time elapsed: 0:09:00.013584

 --- Epoch 6
Task: Classification | Acc: 87.70% | Avg Loss: 0.2952
Task: Reconstruction | Avg Loss: 4.8097 
MoE Balancing Loss: 10428.4727
Mutual Information | Avg Loss: -0.00310
Total Loss: 4.7399
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330853.34
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331432.88
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331670.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331744.00
Time elapsed: 0:10:49.363777

 --- Epoch 7
Task: Classification | Acc: 88.31% | Avg Loss: 0.2851
Task: Reconstruction | Avg Loss: 4.6913 
MoE Balancing Loss: 10429.8201
Mutual Information | Avg Loss: -0.00378
Total Loss: 4.6910
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329833.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330525.91
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330783.91
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330982.38
Time elapsed: 0:12:38.806535

 --- Epoch 8
Task: Classification | Acc: 89.22% | Avg Loss: 0.2658
Task: Reconstruction | Avg Loss: 4.6022 
MoE Balancing Loss: 10430.7611
Mutual Information | Avg Loss: -0.00422
Total Loss: 4.3915
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329894.59
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328871.12
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329317.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329444.97
Time elapsed: 0:14:27.140241

 --- Epoch 9
Task: Classification | Acc: 89.80% | Avg Loss: 0.2545
Task: Reconstruction | Avg Loss: 4.5279 
MoE Balancing Loss: 10427.5306
Mutual Information | Avg Loss: -0.00435
Total Loss: 4.4535
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331390.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331802.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331837.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332123.12
Time elapsed: 0:16:16.481459

 --- Epoch 10
Task: Classification | Acc: 90.05% | Avg Loss: 0.2478
Task: Reconstruction | Avg Loss: 4.4719 
MoE Balancing Loss: 10429.6635
Mutual Information | Avg Loss: -0.00446
Total Loss: 4.3891
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331842.03
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331553.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332162.56
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332509.44
Time elapsed: 0:18:05.810760
Example 1 ---
Original text: one from the heart.
Reconstructed text: ` is is a movie..
Original IDs: [101, 2028, 2013, 1996, 2540, 1012, 102]
Predicted IDs: [1036, 2003, 2003, 1037, 3185, 1012, 1012]
BLEU Score: 0.0000
Example 2 ---
Original text: there is nothing outstanding about this film, but it is good enough and will likely be appreciated most by sailors and folks who know their way around a submarine.
Reconstructed text: ' ` ` ` ` ` movie, but, you one of're hard to be in the time'to have be one of the movie.
Original IDs: [101, 2045, 2003, 2498, 5151, 2055, 2023, 2143, 1010, 2021, 2009, 2003, 2204, 2438, 1998, 2097, 3497, 2022, 12315, 2087, 2011, 11279, 1998, 12455, 2040, 2113, 2037, 2126, 2105, 1037, 6982, 1012, 102]
Predicted IDs: [101, 1005, 1036, 1036, 1036, 1036, 1036, 3185, 1010, 2021, 1010, 2017, 2028, 1997, 1005, 2128, 2524, 2000, 2022, 1999, 1996, 2051, 102, 1005, 2000, 2031, 2022, 2028, 1997, 1996, 3185, 1012, 102]
BLEU Score: 0.1277
Example 3 ---
Original text: birthday girl is an amusing joy ride, with some surprisingly violent moments.
Reconstructed text: littles of a funny,ness, a a andvently. -
Original IDs: [101, 5798, 2611, 2003, 2019, 19142, 6569, 4536, 1010, 2007, 2070, 10889, 6355, 5312, 1012, 102]
Predicted IDs: [101, 2210, 2015, 1997, 1037, 6057, 1010, 2791, 1010, 1037, 1037, 1998, 15338, 2135, 1012, 1011]
BLEU Score: 0.1411

 --- Epoch 11
Task: Classification | Acc: 90.63% | Avg Loss: 0.2361
Task: Reconstruction | Avg Loss: 4.4119 
MoE Balancing Loss: 10429.9235
Mutual Information | Avg Loss: -0.00459
Total Loss: 4.3443
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330009.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329729.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329572.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329598.38
Time elapsed: 0:19:54.496748

 --- Epoch 12
Task: Classification | Acc: 90.81% | Avg Loss: 0.2317
Task: Reconstruction | Avg Loss: 4.3498 
MoE Balancing Loss: 10430.1401
Mutual Information | Avg Loss: -0.00454
Total Loss: 4.5228
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329971.47
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329745.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330116.22
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330254.41
Time elapsed: 0:21:43.874813

 --- Epoch 13
Task: Classification | Acc: 91.18% | Avg Loss: 0.2238
Task: Reconstruction | Avg Loss: 4.3107 
MoE Balancing Loss: 10430.7952
Mutual Information | Avg Loss: -0.00448
Total Loss: 4.1962
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330179.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330018.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330123.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330196.00
Time elapsed: 0:23:32.159504

 --- Epoch 14
Task: Classification | Acc: 91.81% | Avg Loss: 0.2090
Task: Reconstruction | Avg Loss: 4.2659 
MoE Balancing Loss: 10429.0172
Mutual Information | Avg Loss: -0.00445
Total Loss: 4.2518
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330815.19
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330805.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331076.44
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330927.12
Time elapsed: 0:25:21.066652

 --- Epoch 15
Task: Classification | Acc: 91.86% | Avg Loss: 0.2091
Task: Reconstruction | Avg Loss: 4.2251 
MoE Balancing Loss: 10429.6659
Mutual Information | Avg Loss: -0.00454
Total Loss: 4.2614
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332122.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330955.88
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331451.72
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331192.06
Time elapsed: 0:27:09.979943

 --- Epoch 16
Task: Classification | Acc: 92.01% | Avg Loss: 0.2032
Task: Reconstruction | Avg Loss: 4.1701 
MoE Balancing Loss: 10426.7246
Mutual Information | Avg Loss: -0.00453
Total Loss: 4.1250
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329999.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331217.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331005.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330942.38
Time elapsed: 0:28:58.587392

 --- Epoch 17
Task: Classification | Acc: 92.44% | Avg Loss: 0.1966
Task: Reconstruction | Avg Loss: 4.1058 
MoE Balancing Loss: 10430.4934
Mutual Information | Avg Loss: -0.00450
Total Loss: 4.1367
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331000.19
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330489.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330703.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331044.03
Time elapsed: 0:30:47.560624

 --- Epoch 18
Task: Classification | Acc: 92.78% | Avg Loss: 0.1848
Task: Reconstruction | Avg Loss: 4.0643 
MoE Balancing Loss: 10429.9196
Mutual Information | Avg Loss: -0.00437
Total Loss: 4.1705
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330533.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329918.16
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330402.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330506.41
Time elapsed: 0:32:36.720594

 --- Epoch 19
Task: Classification | Acc: 93.00% | Avg Loss: 0.1818
Task: Reconstruction | Avg Loss: 4.0208 
MoE Balancing Loss: 10429.8774
Mutual Information | Avg Loss: -0.00438
Total Loss: 3.9796
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329697.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330354.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330006.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330030.12
Time elapsed: 0:34:25.434658

 --- Epoch 20
Task: Classification | Acc: 93.08% | Avg Loss: 0.1807
Task: Reconstruction | Avg Loss: 3.9746 
MoE Balancing Loss: 10428.4575
Mutual Information | Avg Loss: -0.00431
Total Loss: 4.0806
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331127.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330629.62
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330722.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330869.00
Time elapsed: 0:36:14.571983
Example 1 ---
Original text: passable entertainment, but it's the kind of motion picture that won't make much of a splash when it's released, and will not be remembered long afterwards.
Reconstructed text: the - movie ', the'is the kind of the stuff that can't really funny to be not the it's, that it'have been funny enough
Original IDs: [101, 3413, 3085, 4024, 1010, 2021, 2009, 1005, 1055, 1996, 2785, 1997, 4367, 3861, 2008, 24185, 1050, 1005, 1056, 2191, 2172, 1997, 1037, 17624, 2043, 2009, 1005, 1055, 2207, 1010, 1998, 2097, 2025, 2022, 4622, 2146, 5728, 1012, 102]
Predicted IDs: [101, 1996, 1011, 3185, 1005, 1010, 1996, 1005, 2003, 1996, 2785, 1997, 1996, 4933, 2008, 6187, 1050, 1005, 1056, 2428, 6057, 2000, 2022, 2025, 1996, 2009, 1005, 1055, 1010, 2008, 2009, 1005, 2031, 2042, 6057, 2438, 102, 102, 102]
BLEU Score: 0.3406
Example 2 ---
Original text: the band's courage in the face of official repression is inspiring, especially for aging hippies ( this one included ).
Reconstructed text: the film's - adventure, of adass'' s a great -er, to surprisingly funny.
Original IDs: [101, 1996, 2316, 1005, 1055, 8424, 1999, 1996, 2227, 1997, 2880, 22422, 2003, 18988, 1010, 2926, 2005, 12520, 5099, 13046, 1006, 2023, 2028, 2443, 1007, 1012, 102]
Predicted IDs: [101, 1996, 2143, 1005, 1055, 1011, 6172, 102, 1010, 1997, 1037, 8883, 2015, 1005, 1005, 1055, 1037, 2307, 1011, 2121, 1010, 2000, 10889, 6057, 102, 102, 1012]
BLEU Score: 0.2104
Example 3 ---
Original text: it's another video movie photographed like a film, with the bad lighting that's often written off as indie film naturalism.
Reconstructed text: it's funny, and the the movie,, as a is, the's enly,, ` ` `s''
Original IDs: [101, 2009, 1005, 1055, 2178, 2678, 3185, 16164, 2066, 1037, 2143, 1010, 2007, 1996, 2919, 7497, 2008, 1005, 1055, 2411, 2517, 2125, 2004, 10271, 2143, 3019, 2964, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 6057, 1010, 1998, 1996, 1996, 3185, 1010, 1010, 2004, 1037, 2003, 1010, 1996, 1005, 1055, 4372, 2135, 1010, 1010, 1036, 1036, 1036, 2015, 1005, 1005]
BLEU Score: 0.3333

 --- Epoch 21
Task: Classification | Acc: 93.33% | Avg Loss: 0.1749
Task: Reconstruction | Avg Loss: 3.9422 
MoE Balancing Loss: 10431.8289
Mutual Information | Avg Loss: -0.00433
Total Loss: 4.0551
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331827.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331553.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331867.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331885.81
Time elapsed: 0:38:03.900868

 --- Epoch 22
Task: Classification | Acc: 93.54% | Avg Loss: 0.1681
Task: Reconstruction | Avg Loss: 3.9152 
MoE Balancing Loss: 10428.5192
Mutual Information | Avg Loss: -0.00431
Total Loss: 4.0665
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330770.09
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331303.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331221.84
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331574.59
Time elapsed: 0:39:53.251747

 --- Epoch 23
Task: Classification | Acc: 93.83% | Avg Loss: 0.1655
Task: Reconstruction | Avg Loss: 3.8760 
MoE Balancing Loss: 10432.1342
Mutual Information | Avg Loss: -0.00430
Total Loss: 4.2084
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332005.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330640.31
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330494.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330405.44
Time elapsed: 0:41:43.406639

 --- Epoch 24
Task: Classification | Acc: 93.74% | Avg Loss: 0.1661
Task: Reconstruction | Avg Loss: 3.8316 
MoE Balancing Loss: 10429.7956
Mutual Information | Avg Loss: -0.00428
Total Loss: 4.0385
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 328094.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328778.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328864.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328865.09
Time elapsed: 0:43:32.959106

 --- Epoch 25
Task: Classification | Acc: 93.74% | Avg Loss: 0.1647
Task: Reconstruction | Avg Loss: 3.8029 
MoE Balancing Loss: 10429.2521
Mutual Information | Avg Loss: -0.00429
Total Loss: 4.1130
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330900.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330616.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330609.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330724.19
Time elapsed: 0:45:22.777753

 --- Epoch 26
Task: Classification | Acc: 94.21% | Avg Loss: 0.1560
Task: Reconstruction | Avg Loss: 3.7664 
MoE Balancing Loss: 10429.8012
Mutual Information | Avg Loss: -0.00423
Total Loss: 3.9877
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332720.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331850.06
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331563.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331494.12
Time elapsed: 0:47:12.355018

 --- Epoch 27
Task: Classification | Acc: 94.23% | Avg Loss: 0.1507
Task: Reconstruction | Avg Loss: 3.7366 
MoE Balancing Loss: 10430.6132
Mutual Information | Avg Loss: -0.00417
Total Loss: 3.9575
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331379.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330814.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330660.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331058.53
Time elapsed: 0:49:01.963677

 --- Epoch 28
Task: Classification | Acc: 94.53% | Avg Loss: 0.1471
Task: Reconstruction | Avg Loss: 3.6990 
MoE Balancing Loss: 10427.3041
Mutual Information | Avg Loss: -0.00416
Total Loss: 3.9636
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330039.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330903.06
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330662.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330699.78
Time elapsed: 0:50:51.500656

 --- Epoch 29
Task: Classification | Acc: 94.48% | Avg Loss: 0.1459
Task: Reconstruction | Avg Loss: 3.6634 
MoE Balancing Loss: 10428.6665
Mutual Information | Avg Loss: -0.00405
Total Loss: 4.0200
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332062.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332104.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332600.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332470.97
Time elapsed: 0:52:41.297606

 --- Epoch 30
Task: Classification | Acc: 94.46% | Avg Loss: 0.1475
Task: Reconstruction | Avg Loss: 3.6457 
MoE Balancing Loss: 10431.3325
Mutual Information | Avg Loss: -0.00393
Total Loss: 3.7411
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332302.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331912.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331804.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331777.25
Time elapsed: 0:54:29.926205
Example 1 ---
Original text: ultimately feels empty and unsatisfying, like swallowing a communion wafer without the wine.
Reconstructed text: to endure as the diintcosible, thens the vital essence scooped in the film, but
Original IDs: [101, 4821, 5683, 4064, 1998, 4895, 16846, 2483, 14116, 1010, 2066, 18468, 1037, 15661, 11333, 7512, 2302, 1996, 4511, 1012, 102]
Predicted IDs: [101, 2000, 18094, 2004, 1996, 4487, 18447, 3597, 19307, 1010, 2059, 2015, 1996, 8995, 11305, 20804, 1999, 1996, 2143, 1010, 2021]
BLEU Score: 0.1250
Example 2 ---
Original text: macdowell, whose wifty southern charm has anchored lighter affairs... brings an absolutely riveting conviction to her role.
Reconstructed text: ( s n's unders of - of ` the america, - it one of the moreeniay most of of the.
Original IDs: [101, 6097, 3527, 4381, 1010, 3005, 15536, 6199, 2100, 2670, 11084, 2038, 14453, 9442, 3821, 1012, 1012, 1012, 7545, 2019, 7078, 15544, 19510, 2075, 10652, 2000, 2014, 2535, 1012, 102]
Predicted IDs: [101, 1006, 1055, 1050, 1005, 1055, 4895, 4063, 2015, 1997, 1011, 1997, 1036, 1996, 2637, 1010, 1011, 2009, 2028, 1997, 1996, 2062, 19825, 2100, 2087, 1997, 1997, 1996, 1012, 102]
BLEU Score: 0.0870
Example 3 ---
Original text: i can take infantile humor... but this is the sort of infantile that makes you wonder about changing the director and writer's diapers.
Reconstructed text: i is the ` of compassion and'' and the of the people that is need to the to terms in a the screen, you's ultimate lovers.
Original IDs: [101, 1045, 2064, 2202, 10527, 9463, 8562, 1012, 1012, 1012, 2021, 2023, 2003, 1996, 4066, 1997, 10527, 9463, 2008, 3084, 2017, 4687, 2055, 5278, 1996, 2472, 1998, 3213, 1005, 1055, 22939, 7347, 1012, 102]
Predicted IDs: [101, 1045, 2003, 1996, 1036, 1997, 15398, 1998, 1005, 1005, 1998, 1996, 1997, 1996, 2111, 2008, 2003, 2342, 2000, 1996, 2000, 3408, 1999, 1037, 1996, 3898, 1010, 2017, 1005, 1055, 7209, 10205, 1012, 102]
BLEU Score: 0.3333

 --- Epoch 31
Task: Classification | Acc: 94.49% | Avg Loss: 0.1450
Task: Reconstruction | Avg Loss: 3.6095 
MoE Balancing Loss: 10429.3783
Mutual Information | Avg Loss: -0.00403
Total Loss: 3.9196
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329909.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329987.62
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330390.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330549.56
Time elapsed: 0:56:19.539555

 --- Epoch 32
Task: Classification | Acc: 94.78% | Avg Loss: 0.1403
Task: Reconstruction | Avg Loss: 3.5759 
MoE Balancing Loss: 10430.6184
Mutual Information | Avg Loss: -0.00409
Total Loss: 3.8741
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333186.16
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333117.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333155.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332656.31
Time elapsed: 0:58:09.136127

 --- Epoch 33
Task: Classification | Acc: 94.89% | Avg Loss: 0.1348
Task: Reconstruction | Avg Loss: 3.5517 
MoE Balancing Loss: 10429.4422
Mutual Information | Avg Loss: -0.00407
Total Loss: 3.7490
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330690.47
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330414.56
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331056.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330896.31
Time elapsed: 0:59:58.123129

 --- Epoch 34
Task: Classification | Acc: 94.95% | Avg Loss: 0.1346
Task: Reconstruction | Avg Loss: 3.5295 
MoE Balancing Loss: 10431.1809
Mutual Information | Avg Loss: -0.00413
Total Loss: 4.0542
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333248.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332135.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332600.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332577.00
Time elapsed: 1:01:48.669702

 --- Epoch 35
Task: Classification | Acc: 94.85% | Avg Loss: 0.1384
Task: Reconstruction | Avg Loss: 3.4942 
MoE Balancing Loss: 10431.2733
Mutual Information | Avg Loss: -0.00401
Total Loss: 3.8211
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333497.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332607.78
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331502.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331863.31
Time elapsed: 1:03:38.187650

 --- Epoch 36
Task: Classification | Acc: 95.07% | Avg Loss: 0.1337
Task: Reconstruction | Avg Loss: 3.4675 
MoE Balancing Loss: 10430.1305
Mutual Information | Avg Loss: -0.00409
Total Loss: 3.8363
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330015.28
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329701.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330802.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330769.25
Time elapsed: 1:05:27.824795

 --- Epoch 37
Task: Classification | Acc: 95.15% | Avg Loss: 0.1303
Task: Reconstruction | Avg Loss: 3.4412 
MoE Balancing Loss: 10430.3740
Mutual Information | Avg Loss: -0.00381
Total Loss: 3.7551
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331928.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331014.25
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330768.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331261.31
Time elapsed: 1:07:16.981621

 --- Epoch 38
Task: Classification | Acc: 95.47% | Avg Loss: 0.1241
Task: Reconstruction | Avg Loss: 3.4198 
MoE Balancing Loss: 10428.5046
Mutual Information | Avg Loss: -0.00383
Total Loss: 3.7099
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331196.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331260.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331102.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331548.59
Time elapsed: 1:09:06.424355

 --- Epoch 39
Task: Classification | Acc: 95.34% | Avg Loss: 0.1264
Task: Reconstruction | Avg Loss: 3.3892 
MoE Balancing Loss: 10429.3906
Mutual Information | Avg Loss: -0.00392
Total Loss: 3.6838
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331566.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330825.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330829.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330877.31
Time elapsed: 1:10:55.827909

 --- Epoch 40
Task: Classification | Acc: 95.42% | Avg Loss: 0.1246
Task: Reconstruction | Avg Loss: 3.3777 
MoE Balancing Loss: 10429.0128
Mutual Information | Avg Loss: -0.00385
Total Loss: 3.7337
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329335.59
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328685.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329339.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329498.31
Time elapsed: 1:12:45.140618
Example 1 ---
Original text: in execution, this clever idea is far less funny than the original, killers from space.
Reconstructed text: a a film, a man is be morely as a new, full of laughs.
Original IDs: [101, 1999, 7781, 1010, 2023, 12266, 2801, 2003, 2521, 2625, 6057, 2084, 1996, 2434, 1010, 15978, 2013, 2686, 1012, 102]
Predicted IDs: [101, 1037, 1037, 2143, 1010, 1037, 2158, 2003, 2022, 2062, 2135, 2004, 1037, 2047, 1010, 2440, 1997, 11680, 1012, 102]
BLEU Score: 0.2219
Example 2 ---
Original text: the film's hackneyed message is not helped by the thin characterizations, nonexistent plot and pretentious visual style.
Reconstructed text: , it's contri, live in the the - s gags and inatuits down by preeatious..
Original IDs: [101, 1996, 2143, 1005, 1055, 28425, 2098, 4471, 2003, 2025, 3271, 2011, 1996, 4857, 23191, 2015, 1010, 3904, 9048, 16173, 2102, 5436, 1998, 3653, 6528, 20771, 5107, 2806, 1012, 102]
Predicted IDs: [101, 1010, 2009, 1005, 1055, 9530, 18886, 1010, 2444, 1999, 1996, 1996, 1011, 1055, 18201, 2015, 1998, 1999, 4017, 14663, 2015, 2091, 2011, 3653, 5243, 20771, 102, 1012, 1012, 102]
BLEU Score: 0.2983
Example 3 ---
Original text: it feels like an after - school special gussied up with some fancy special effects, and watching its rote plot points connect is about as exciting as gazing at an egg timer for 93 minutes.
Reconstructed text: ' s a visual audience, director and poosnares in a typical running setting. in a five sen are up to be forgotten. to makegli film of the..
Original IDs: [101, 2009, 5683, 2066, 2019, 2044, 1011, 2082, 2569, 12670, 11741, 2094, 2039, 2007, 2070, 11281, 2569, 3896, 1010, 1998, 3666, 2049, 18672, 2063, 5436, 2685, 7532, 2003, 2055, 2004, 10990, 2004, 16448, 2012, 2019, 8288, 25309, 2005, 6109, 2781, 1012, 102]
Predicted IDs: [101, 1005, 1055, 1037, 5107, 4378, 1010, 2472, 1998, 13433, 2891, 26148, 2015, 1999, 1037, 5171, 2770, 4292, 1012, 102, 1999, 1037, 2274, 12411, 102, 102, 2024, 2039, 2000, 2022, 6404, 1012, 102, 2000, 2191, 25394, 2143, 1997, 1996, 1012, 1012, 102]
BLEU Score: 0.1063

 --- Epoch 41
Task: Classification | Acc: 95.46% | Avg Loss: 0.1229
Task: Reconstruction | Avg Loss: 3.3510 
MoE Balancing Loss: 10428.4280
Mutual Information | Avg Loss: -0.00395
Total Loss: 3.7984
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329282.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329884.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329213.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329111.47
Time elapsed: 1:14:35.081264

 --- Epoch 42
Task: Classification | Acc: 95.80% | Avg Loss: 0.1169
Task: Reconstruction | Avg Loss: 3.3299 
MoE Balancing Loss: 10430.8066
Mutual Information | Avg Loss: -0.00384
Total Loss: 3.7315
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331805.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329786.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329943.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330164.34
Time elapsed: 1:16:24.500847

 --- Epoch 43
Task: Classification | Acc: 95.69% | Avg Loss: 0.1162
Task: Reconstruction | Avg Loss: 3.3002 
MoE Balancing Loss: 10427.9940
Mutual Information | Avg Loss: -0.00380
Total Loss: 3.6924
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330444.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330083.91
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330491.84
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330644.03
Time elapsed: 1:18:13.885388

 --- Epoch 44
Task: Classification | Acc: 95.79% | Avg Loss: 0.1133
Task: Reconstruction | Avg Loss: 3.2881 
MoE Balancing Loss: 10428.8611
Mutual Information | Avg Loss: -0.00373
Total Loss: 3.6256
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332132.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331511.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331547.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331069.31
Time elapsed: 1:20:03.200661

 --- Epoch 45
Task: Classification | Acc: 95.87% | Avg Loss: 0.1113
Task: Reconstruction | Avg Loss: 3.2518 
MoE Balancing Loss: 10429.2985
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.7565
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331327.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330691.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331091.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331245.09
Time elapsed: 1:21:53.263341

 --- Epoch 46
Task: Classification | Acc: 95.69% | Avg Loss: 0.1160
Task: Reconstruction | Avg Loss: 3.2357 
MoE Balancing Loss: 10428.1276
Mutual Information | Avg Loss: -0.00374
Total Loss: 3.6619
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331599.47
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331418.56
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331801.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332120.38
Time elapsed: 1:23:42.833902

 --- Epoch 47
Task: Classification | Acc: 95.98% | Avg Loss: 0.1114
Task: Reconstruction | Avg Loss: 3.2221 
MoE Balancing Loss: 10429.7326
Mutual Information | Avg Loss: -0.00381
Total Loss: 3.7589
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330835.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330043.59
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329993.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330288.59
Time elapsed: 1:25:32.813368

 --- Epoch 48
Task: Classification | Acc: 95.93% | Avg Loss: 0.1101
Task: Reconstruction | Avg Loss: 3.2030 
MoE Balancing Loss: 10427.0583
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.6192
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330025.16
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330916.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330494.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330962.53
Time elapsed: 1:27:22.226620

 --- Epoch 49
Task: Classification | Acc: 95.80% | Avg Loss: 0.1107
Task: Reconstruction | Avg Loss: 3.1748 
MoE Balancing Loss: 10429.1262
Mutual Information | Avg Loss: -0.00378
Total Loss: 3.6530
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333107.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332874.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332909.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333163.94
Time elapsed: 1:29:12.053160

 --- Epoch 50
Task: Classification | Acc: 95.93% | Avg Loss: 0.1078
Task: Reconstruction | Avg Loss: 3.1571 
MoE Balancing Loss: 10426.8205
Mutual Information | Avg Loss: -0.00359
Total Loss: 3.5922
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332551.19
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332122.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332097.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332297.53
Time elapsed: 1:31:01.499415
Example 1 ---
Original text: if you dig on david mamet's mind tricks... rent this movie and enjoy!
Reconstructed text: it to watch of chapooo's best masterpiece..., the film, to edit the
Original IDs: [101, 2065, 2017, 10667, 2006, 2585, 5003, 11368, 1005, 1055, 2568, 12225, 1012, 1012, 1012, 9278, 2023, 3185, 1998, 5959, 999, 102]
Predicted IDs: [101, 2009, 2000, 3422, 1997, 15775, 24667, 2080, 1005, 1055, 2190, 17743, 1012, 1012, 1012, 1010, 1996, 2143, 1010, 2000, 10086, 1996]
BLEU Score: 0.1250
Example 2 ---
Original text: a rewarding work of art for only the most patient and challenge - hungry moviegoers.
Reconstructed text: the best a rank of life as a the shocking areas a first - rate film thats the whole
Original IDs: [101, 1037, 10377, 2075, 2147, 1997, 2396, 2005, 2069, 1996, 2087, 5776, 1998, 4119, 1011, 7501, 3185, 3995, 2545, 1012, 102]
Predicted IDs: [101, 1996, 2190, 1037, 4635, 1997, 2166, 2004, 1037, 1996, 16880, 2752, 1037, 2034, 1011, 3446, 2143, 2008, 2015, 1996, 2878]
BLEU Score: 0.2105
Example 3 ---
Original text: pacino is brilliant as the sleep - deprived dormer, his increasing weariness as much existential as it is physical.
Reconstructed text: the to is said of a hands - packed film that, s the ways, and calrdanies than quite both...
Original IDs: [101, 14397, 5740, 2003, 8235, 2004, 1996, 3637, 1011, 17676, 19568, 2121, 1010, 2010, 4852, 4929, 9961, 2004, 2172, 25953, 4818, 2004, 2009, 2003, 3558, 1012, 102]
Predicted IDs: [101, 1996, 2000, 2003, 2056, 1997, 1037, 2398, 1011, 8966, 2143, 2008, 1010, 1055, 1996, 3971, 1010, 1998, 10250, 26992, 3111, 2084, 3243, 2119, 1012, 1012, 1012]
BLEU Score: 0.1818

 --- Epoch 51
Task: Classification | Acc: 95.95% | Avg Loss: 0.1076
Task: Reconstruction | Avg Loss: 3.1296 
MoE Balancing Loss: 10428.8408
Mutual Information | Avg Loss: -0.00373
Total Loss: 3.6356
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331327.44
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330665.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331169.41
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331250.00
Time elapsed: 1:32:51.468238

 --- Epoch 52
Task: Classification | Acc: 96.23% | Avg Loss: 0.1004
Task: Reconstruction | Avg Loss: 3.1098 
MoE Balancing Loss: 10429.3441
Mutual Information | Avg Loss: -0.00388
Total Loss: 3.6607
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332374.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331450.25
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331616.31
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331368.25
Time elapsed: 1:34:41.536192

 --- Epoch 53
Task: Classification | Acc: 96.19% | Avg Loss: 0.1050
Task: Reconstruction | Avg Loss: 3.0904 
MoE Balancing Loss: 10427.7104
Mutual Information | Avg Loss: -0.00370
Total Loss: 3.6547
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331558.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332369.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331701.50
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332166.53
Time elapsed: 1:36:31.726824

 --- Epoch 54
Task: Classification | Acc: 96.27% | Avg Loss: 0.1026
Task: Reconstruction | Avg Loss: 3.0642 
MoE Balancing Loss: 10428.9127
Mutual Information | Avg Loss: -0.00367
Total Loss: 3.6802
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332409.16
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332536.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333173.34
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332808.81
Time elapsed: 1:38:22.044338

 --- Epoch 55
Task: Classification | Acc: 96.25% | Avg Loss: 0.1011
Task: Reconstruction | Avg Loss: 3.0565 
MoE Balancing Loss: 10427.8883
Mutual Information | Avg Loss: -0.00356
Total Loss: 3.5811
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331080.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330948.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330639.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331067.34
Time elapsed: 1:40:11.747412

 --- Epoch 56
Task: Classification | Acc: 96.37% | Avg Loss: 0.1007
Task: Reconstruction | Avg Loss: 3.0409 
MoE Balancing Loss: 10429.8411
Mutual Information | Avg Loss: -0.00372
Total Loss: 3.6837
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332648.09
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332283.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333029.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332819.06
Time elapsed: 1:42:02.096184

 --- Epoch 57
Task: Classification | Acc: 96.45% | Avg Loss: 0.0987
Task: Reconstruction | Avg Loss: 3.0204 
MoE Balancing Loss: 10427.7299
Mutual Information | Avg Loss: -0.00374
Total Loss: 3.6992
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330997.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330590.25
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330660.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330210.31
Time elapsed: 1:43:52.358325

 --- Epoch 58
Task: Classification | Acc: 96.26% | Avg Loss: 0.1012
Task: Reconstruction | Avg Loss: 3.0091 
MoE Balancing Loss: 10430.7252
Mutual Information | Avg Loss: -0.00375
Total Loss: 3.6618
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331393.00
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330731.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330980.91
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331481.78
Time elapsed: 1:45:42.843516

 --- Epoch 59
Task: Classification | Acc: 96.44% | Avg Loss: 0.0978
Task: Reconstruction | Avg Loss: 2.9896 
MoE Balancing Loss: 10431.0542
Mutual Information | Avg Loss: -0.00374
Total Loss: 3.6117
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332085.34
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330808.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331379.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331264.50
Time elapsed: 1:47:32.835344

 --- Epoch 60
Task: Classification | Acc: 96.41% | Avg Loss: 0.0976
Task: Reconstruction | Avg Loss: 2.9651 
MoE Balancing Loss: 10429.1304
Mutual Information | Avg Loss: -0.00359
Total Loss: 3.5241
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330985.44
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330043.16
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330613.84
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330635.34
Time elapsed: 1:49:22.545997
Example 1 ---
Original text: the magic of the film lies not in the mysterious spring but in the richness of its performances.
Reconstructed text: is more to you'ever wasted for the awkward episode and creating the extensive middle of the movies.
Original IDs: [101, 1996, 3894, 1997, 1996, 2143, 3658, 2025, 1999, 1996, 8075, 3500, 2021, 1999, 1996, 4138, 2791, 1997, 2049, 4616, 1012, 102]
Predicted IDs: [101, 2003, 2062, 2000, 2017, 1005, 2412, 13842, 2005, 1996, 9596, 2792, 1998, 4526, 1996, 4866, 2690, 1997, 1996, 5691, 1012, 102]
BLEU Score: 0.2628
Example 2 ---
Original text: harris commands the screen, using his frailty to suggest the ravages of a life of corruption and ruthlessness.
Reconstructed text: the film is the, with the engaging face to take the martial dimness of the essence of sanity and emotionalness
Original IDs: [101, 5671, 10954, 1996, 3898, 1010, 2478, 2010, 25737, 3723, 2000, 6592, 1996, 10958, 3567, 8449, 1997, 1037, 2166, 1997, 7897, 1998, 18101, 2791, 1012, 102]
Predicted IDs: [101, 1996, 2143, 2003, 1996, 1010, 2007, 1996, 11973, 2227, 2000, 2202, 1996, 7761, 11737, 2791, 1997, 1996, 11305, 1997, 20039, 1998, 6832, 2791, 102, 102]
BLEU Score: 0.3333
Example 3 ---
Original text: ` ` the time machine'' is a movie that has no interest in itself.
Reconstructed text: the ` `aya laughs - - is, - and more interestingx for..
Original IDs: [101, 1036, 1036, 1996, 2051, 3698, 1005, 1005, 2003, 1037, 3185, 2008, 2038, 2053, 3037, 1999, 2993, 1012, 102]
Predicted IDs: [101, 1996, 1036, 1036, 12186, 11680, 1011, 1011, 2003, 1010, 1011, 1998, 2062, 5875, 2595, 2005, 1012, 1012, 102]
BLEU Score: 0.2495

 --- Epoch 61
Task: Classification | Acc: 96.54% | Avg Loss: 0.0972
Task: Reconstruction | Avg Loss: 2.9628 
MoE Balancing Loss: 10427.7965
Mutual Information | Avg Loss: -0.00365
Total Loss: 3.5709
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331839.22
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331906.97
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331831.31
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331945.97
Time elapsed: 1:51:12.826049

 --- Epoch 62
Task: Classification | Acc: 96.38% | Avg Loss: 0.0967
Task: Reconstruction | Avg Loss: 2.9406 
MoE Balancing Loss: 10427.2642
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.5599
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331022.56
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330690.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331188.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331249.72
Time elapsed: 1:53:03.008396

 --- Epoch 63
Task: Classification | Acc: 96.53% | Avg Loss: 0.0937
Task: Reconstruction | Avg Loss: 2.9228 
MoE Balancing Loss: 10428.2411
Mutual Information | Avg Loss: -0.00360
Total Loss: 3.5606
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331362.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329352.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329998.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329769.44
Time elapsed: 1:54:52.903123

 --- Epoch 64
Task: Classification | Acc: 96.57% | Avg Loss: 0.0915
Task: Reconstruction | Avg Loss: 2.9166 
MoE Balancing Loss: 10428.1506
Mutual Information | Avg Loss: -0.00355
Total Loss: 3.5623
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332611.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331625.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332076.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331956.88
Time elapsed: 1:56:43.177948

 --- Epoch 65
Task: Classification | Acc: 96.58% | Avg Loss: 0.0928
Task: Reconstruction | Avg Loss: 2.8977 
MoE Balancing Loss: 10427.2843
Mutual Information | Avg Loss: -0.00359
Total Loss: 3.5581
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331342.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331402.56
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331441.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331240.75
Time elapsed: 1:58:33.383970

 --- Epoch 66
Task: Classification | Acc: 96.50% | Avg Loss: 0.0943
Task: Reconstruction | Avg Loss: 2.9011 
MoE Balancing Loss: 10429.0241
Mutual Information | Avg Loss: -0.00356
Total Loss: 3.4495
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330514.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329853.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329924.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330011.62
Time elapsed: 2:00:22.971085

 --- Epoch 67
Task: Classification | Acc: 96.61% | Avg Loss: 0.0920
Task: Reconstruction | Avg Loss: 2.8811 
MoE Balancing Loss: 10427.7414
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.5598
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331726.94
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330940.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330786.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331026.53
Time elapsed: 2:02:13.352202

 --- Epoch 68
Task: Classification | Acc: 96.82% | Avg Loss: 0.0877
Task: Reconstruction | Avg Loss: 2.8462 
MoE Balancing Loss: 10429.3348
Mutual Information | Avg Loss: -0.00366
Total Loss: 3.5084
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331116.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331144.78
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330980.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330917.72
Time elapsed: 2:04:03.283570

 --- Epoch 69
Task: Classification | Acc: 96.76% | Avg Loss: 0.0916
Task: Reconstruction | Avg Loss: 2.8441 
MoE Balancing Loss: 10426.0807
Mutual Information | Avg Loss: -0.00359
Total Loss: 3.4101
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330444.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331079.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331084.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331113.72
Time elapsed: 2:05:52.779624

 --- Epoch 70
Task: Classification | Acc: 96.88% | Avg Loss: 0.0861
Task: Reconstruction | Avg Loss: 2.8281 
MoE Balancing Loss: 10429.4032
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.4574
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332427.00
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331563.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331702.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331738.94
Time elapsed: 2:07:42.627385
Example 1 ---
Original text: few films capture so perfectly the hopes and dreams of little boys on baseball fields as well as the grown men who sit in the stands.
Reconstructed text: the addition to a same bridge at the brilliance of the videos, frank sneak as up as the living of for most of the..
Original IDs: [101, 2261, 3152, 5425, 2061, 6669, 1996, 8069, 1998, 5544, 1997, 2210, 3337, 2006, 3598, 4249, 2004, 2092, 2004, 1996, 4961, 2273, 2040, 4133, 1999, 1996, 4832, 1012, 102]
Predicted IDs: [101, 1996, 2804, 2000, 1037, 2168, 2958, 2012, 1996, 28850, 1997, 1996, 6876, 1010, 3581, 13583, 2004, 2039, 2004, 1996, 2542, 1997, 2005, 2087, 1997, 1996, 1012, 1012, 102]
BLEU Score: 0.2221
Example 2 ---
Original text: with the exception of some fleetingly amusing improvisations by cedric the entertainer as perry's boss, there isn't a redeeming moment here.
Reconstructed text: is a all of the transparent, to dragoning a the than of of it's sundance movie, won't necessarily substantial the movie.
Original IDs: [101, 2007, 1996, 6453, 1997, 2070, 25085, 2135, 19142, 24584, 2015, 2011, 26170, 1996, 21751, 2004, 6890, 1005, 1055, 5795, 1010, 2045, 2003, 1050, 1005, 1056, 1037, 2417, 21564, 2075, 2617, 2182, 1012, 102]
Predicted IDs: [101, 2003, 1037, 2035, 1997, 1996, 13338, 1010, 2000, 5202, 2075, 1037, 1996, 2084, 1997, 1997, 2009, 1005, 1055, 20140, 3185, 1010, 24185, 1050, 1005, 1056, 9352, 6937, 102, 102, 1996, 3185, 1012, 102]
BLEU Score: 0.3462
Example 3 ---
Original text: the movie's accumulated force still feels like an ugly knot tightening in your stomach.
Reconstructed text: the hoffman's convincing movie in a as of newsc actually is its style.
Original IDs: [101, 1996, 3185, 1005, 1055, 14830, 2486, 2145, 5683, 2066, 2019, 9200, 12226, 18711, 1999, 2115, 4308, 1012, 102]
Predicted IDs: [101, 1996, 15107, 1005, 1055, 13359, 3185, 1999, 1037, 2004, 1997, 2047, 11020, 2941, 2003, 2049, 2806, 1012, 102]
BLEU Score: 0.3118

 --- Epoch 71
Task: Classification | Acc: 96.84% | Avg Loss: 0.0880
Task: Reconstruction | Avg Loss: 2.8269 
MoE Balancing Loss: 10428.2269
Mutual Information | Avg Loss: -0.00353
Total Loss: 3.5260
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331622.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330921.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331501.59
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331615.75
Time elapsed: 2:09:32.620970

 --- Epoch 72
Task: Classification | Acc: 96.89% | Avg Loss: 0.0853
Task: Reconstruction | Avg Loss: 2.8276 
MoE Balancing Loss: 10428.1529
Mutual Information | Avg Loss: -0.00352
Total Loss: 3.4679
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330916.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331135.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330977.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331090.44
Time elapsed: 2:11:22.841899

 --- Epoch 73
Task: Classification | Acc: 96.90% | Avg Loss: 0.0861
Task: Reconstruction | Avg Loss: 2.8102 
MoE Balancing Loss: 10424.9721
Mutual Information | Avg Loss: -0.00355
Total Loss: 3.4744
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331962.09
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331924.62
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331654.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331884.41
Time elapsed: 2:13:13.096153

 --- Epoch 74
Task: Classification | Acc: 96.94% | Avg Loss: 0.0839
Task: Reconstruction | Avg Loss: 2.8021 
MoE Balancing Loss: 10426.4360
Mutual Information | Avg Loss: -0.00352
Total Loss: 3.4235
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333807.72
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332917.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333142.72
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333204.41
Time elapsed: 2:15:03.273684

 --- Epoch 75
Task: Classification | Acc: 96.92% | Avg Loss: 0.0824
Task: Reconstruction | Avg Loss: 2.7791 
MoE Balancing Loss: 10426.0526
Mutual Information | Avg Loss: -0.00373
Total Loss: 3.5425
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330459.94
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330777.59
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330804.69
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330975.31
Time elapsed: 2:16:53.961614

 --- Epoch 76
Task: Classification | Acc: 96.85% | Avg Loss: 0.0841
Task: Reconstruction | Avg Loss: 2.7455 
MoE Balancing Loss: 10428.3938
Mutual Information | Avg Loss: -0.00362
Total Loss: 3.5072
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331648.34
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330849.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330991.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331189.28
Time elapsed: 2:18:44.361672

 --- Epoch 77
Task: Classification | Acc: 96.72% | Avg Loss: 0.0849
Task: Reconstruction | Avg Loss: 2.7604 
MoE Balancing Loss: 10430.6159
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.4650
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330741.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331003.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331588.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331220.59
Time elapsed: 2:20:34.569699

 --- Epoch 78
Task: Classification | Acc: 96.89% | Avg Loss: 0.0848
Task: Reconstruction | Avg Loss: 2.7508 
MoE Balancing Loss: 10424.4807
Mutual Information | Avg Loss: -0.00351
Total Loss: 3.3891
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330048.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330508.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330207.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330333.28
Time elapsed: 2:22:24.281101

 --- Epoch 79
Task: Classification | Acc: 97.00% | Avg Loss: 0.0811
Task: Reconstruction | Avg Loss: 2.7189 
MoE Balancing Loss: 10427.8795
Mutual Information | Avg Loss: -0.00366
Total Loss: 3.4765
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331959.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331129.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331216.31
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331207.31
Time elapsed: 2:24:14.391607

 --- Epoch 80
Task: Classification | Acc: 97.13% | Avg Loss: 0.0797
Task: Reconstruction | Avg Loss: 2.7300 
MoE Balancing Loss: 10428.5841
Mutual Information | Avg Loss: -0.00352
Total Loss: 3.4176
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331745.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330944.06
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330949.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331278.53
Time elapsed: 2:26:04.377133
Example 1 ---
Original text: if steven soderbergh's ` solaris'is a failure it is a glorious failure.
Reconstructed text: as movie - triesberg n's cut - it's a emotional comedy, little standard.
Original IDs: [101, 2065, 7112, 2061, 4063, 4059, 2232, 1005, 1055, 1036, 5943, 2483, 1005, 2003, 1037, 4945, 2009, 2003, 1037, 14013, 4945, 1012, 102]
Predicted IDs: [101, 2004, 3185, 1011, 5363, 4059, 1050, 1005, 1055, 3013, 1011, 2009, 1005, 1055, 1037, 6832, 4038, 1010, 102, 2210, 3115, 1012, 102]
BLEU Score: 0.2353
Example 2 ---
Original text: it seems like i have been waiting my whole life for this movie and now i can't wait for the sequel.
Reconstructed text: i you is a sit a a stores of term,,, ` ` you don't feel like the party - hour
Original IDs: [101, 2009, 3849, 2066, 1045, 2031, 2042, 3403, 2026, 2878, 2166, 2005, 2023, 3185, 1998, 2085, 1045, 6187, 1050, 1005, 1056, 3524, 2005, 1996, 8297, 1012, 102]
Predicted IDs: [101, 1045, 2017, 2003, 1037, 4133, 1037, 1037, 5324, 1997, 2744, 1010, 1010, 1010, 1036, 1036, 2017, 2079, 1050, 1005, 1056, 2514, 2066, 1996, 2283, 1011, 3178]
BLEU Score: 0.1667
Example 3 ---
Original text: it's hard to imagine alan arkin being better than he is in this performance.
Reconstructed text: it's latest, and w saricksnt of it is all the story.
Original IDs: [101, 2009, 1005, 1055, 2524, 2000, 5674, 5070, 15745, 2378, 2108, 2488, 2084, 2002, 2003, 1999, 2023, 2836, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 6745, 1010, 1998, 1059, 18906, 6799, 2015, 3372, 1997, 2009, 2003, 2035, 1996, 2466, 1012, 102]
BLEU Score: 0.2477

 --- Epoch 81
Task: Classification | Acc: 96.89% | Avg Loss: 0.0848
Task: Reconstruction | Avg Loss: 2.7264 
MoE Balancing Loss: 10427.6644
Mutual Information | Avg Loss: -0.00356
Total Loss: 3.4079
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330369.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331381.06
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331677.50
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331465.34
Time elapsed: 2:27:54.356989

 --- Epoch 82
Task: Classification | Acc: 96.95% | Avg Loss: 0.0828
Task: Reconstruction | Avg Loss: 2.6971 
MoE Balancing Loss: 10427.4089
Mutual Information | Avg Loss: -0.00367
Total Loss: 3.4809
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331928.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331589.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331883.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331539.53
Time elapsed: 2:29:44.906482

 --- Epoch 83
Task: Classification | Acc: 96.78% | Avg Loss: 0.0844
Task: Reconstruction | Avg Loss: 2.6854 
MoE Balancing Loss: 10427.9305
Mutual Information | Avg Loss: -0.00362
Total Loss: 3.4072
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333581.34
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332690.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333274.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333488.78
Time elapsed: 2:31:35.015131

 --- Epoch 84
Task: Classification | Acc: 97.11% | Avg Loss: 0.0798
Task: Reconstruction | Avg Loss: 2.6845 
MoE Balancing Loss: 10431.9199
Mutual Information | Avg Loss: -0.00362
Total Loss: 3.4348
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330992.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330908.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330989.50
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330728.91
Time elapsed: 2:33:25.019406

 --- Epoch 85
Task: Classification | Acc: 96.99% | Avg Loss: 0.0827
Task: Reconstruction | Avg Loss: 2.6672 
MoE Balancing Loss: 10428.0600
Mutual Information | Avg Loss: -0.00337
Total Loss: 3.3410
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330634.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330540.19
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331410.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331239.28
Time elapsed: 2:35:14.695726

 --- Epoch 86
Task: Classification | Acc: 97.10% | Avg Loss: 0.0803
Task: Reconstruction | Avg Loss: 2.6538 
MoE Balancing Loss: 10426.4207
Mutual Information | Avg Loss: -0.00358
Total Loss: 3.4483
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331947.50
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331408.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331707.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331815.34
Time elapsed: 2:37:04.978101

 --- Epoch 87
Task: Classification | Acc: 97.03% | Avg Loss: 0.0802
Task: Reconstruction | Avg Loss: 2.6465 
MoE Balancing Loss: 10427.7330
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.5127
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330841.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330573.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330893.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330873.00
Time elapsed: 2:38:55.704800

 --- Epoch 88
Task: Classification | Acc: 97.16% | Avg Loss: 0.0768
Task: Reconstruction | Avg Loss: 2.6435 
MoE Balancing Loss: 10428.4547
Mutual Information | Avg Loss: -0.00351
Total Loss: 3.4180
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331640.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331435.12
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331392.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331506.31
Time elapsed: 2:40:45.831060

 --- Epoch 89
Task: Classification | Acc: 97.18% | Avg Loss: 0.0788
Task: Reconstruction | Avg Loss: 2.6140 
MoE Balancing Loss: 10424.9524
Mutual Information | Avg Loss: -0.00355
Total Loss: 3.4368
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331412.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330880.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331587.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331427.81
Time elapsed: 2:42:36.405003

 --- Epoch 90
Task: Classification | Acc: 97.09% | Avg Loss: 0.0798
Task: Reconstruction | Avg Loss: 2.6144 
MoE Balancing Loss: 10429.4558
Mutual Information | Avg Loss: -0.00362
Total Loss: 3.4329
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329729.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329409.59
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329582.56
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329696.28
Time elapsed: 2:44:26.716047
Example 1 ---
Original text: lovely and poignant.
Reconstructed text: finding the personalck,
Original IDs: [101, 8403, 1998, 13433, 25593, 1012, 102]
Predicted IDs: [101, 4531, 1996, 3167, 3600, 1010, 102]
BLEU Score: 0.0000
Example 2 ---
Original text: a rewarding work of art for only the most patient and challenge - hungry moviegoers.
Reconstructed text: a veryy tale of hollywoods as a very intriguing and one -pid psychological society
Original IDs: [101, 1037, 10377, 2075, 2147, 1997, 2396, 2005, 2069, 1996, 2087, 5776, 1998, 4119, 1011, 7501, 3185, 3995, 2545, 1012, 102]
Predicted IDs: [101, 1037, 2200, 2100, 6925, 1997, 5365, 2015, 2004, 1037, 2200, 23824, 1998, 2028, 1011, 23267, 8317, 2554, 102, 102, 102]
BLEU Score: 0.1858
Example 3 ---
Original text: dragonfly has no atmosphere, no tension - - nothing but costner, flailing away.
Reconstructed text: it, to is a, more surprising - - or, sappy and politicallyrky direction.
Original IDs: [101, 5202, 14151, 2038, 2053, 7224, 1010, 2053, 6980, 1011, 1011, 2498, 2021, 3465, 3678, 1010, 13109, 29544, 2185, 1012, 102]
Predicted IDs: [101, 2009, 1010, 2000, 2003, 1037, 1010, 2062, 11341, 1011, 1011, 2030, 1010, 20066, 7685, 1998, 10317, 15952, 3257, 1012, 102]
BLEU Score: 0.2941

 --- Epoch 91
Task: Classification | Acc: 97.20% | Avg Loss: 0.0770
Task: Reconstruction | Avg Loss: 2.6075 
MoE Balancing Loss: 10429.6150
Mutual Information | Avg Loss: -0.00356
Total Loss: 3.3614
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329526.50
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329625.88
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329395.72
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329495.25
Time elapsed: 2:46:16.873692

 --- Epoch 92
Task: Classification | Acc: 97.35% | Avg Loss: 0.0763
Task: Reconstruction | Avg Loss: 2.6000 
MoE Balancing Loss: 10424.5940
Mutual Information | Avg Loss: -0.00355
Total Loss: 3.3612
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330192.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330609.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330818.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330811.97
Time elapsed: 2:48:07.047461

 --- Epoch 93
Task: Classification | Acc: 97.29% | Avg Loss: 0.0747
Task: Reconstruction | Avg Loss: 2.6027 
MoE Balancing Loss: 10426.9157
Mutual Information | Avg Loss: -0.00360
Total Loss: 3.3378
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332147.47
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331019.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331039.72
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331251.44
Time elapsed: 2:49:56.864480

 --- Epoch 94
Task: Classification | Acc: 97.24% | Avg Loss: 0.0743
Task: Reconstruction | Avg Loss: 2.5879 
MoE Balancing Loss: 10426.8150
Mutual Information | Avg Loss: -0.00366
Total Loss: 3.4490
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329196.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328732.16
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329398.56
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328606.09
Time elapsed: 2:51:47.137432

 --- Epoch 95
Task: Classification | Acc: 97.33% | Avg Loss: 0.0744
Task: Reconstruction | Avg Loss: 2.5798 
MoE Balancing Loss: 10432.8690
Mutual Information | Avg Loss: -0.00356
Total Loss: 3.3852
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330523.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330181.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330303.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330609.66
Time elapsed: 2:53:37.344952

 --- Epoch 96
Task: Classification | Acc: 97.28% | Avg Loss: 0.0765
Task: Reconstruction | Avg Loss: 2.5766 
MoE Balancing Loss: 10426.8292
Mutual Information | Avg Loss: -0.00366
Total Loss: 3.3635
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332097.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331621.06
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331663.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331658.84
Time elapsed: 2:55:27.617248

 --- Epoch 97
Task: Classification | Acc: 97.37% | Avg Loss: 0.0753
Task: Reconstruction | Avg Loss: 2.5672 
MoE Balancing Loss: 10430.0418
Mutual Information | Avg Loss: -0.00347
Total Loss: 3.3418
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332031.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332745.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332760.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332246.88
Time elapsed: 2:57:18.062499

 --- Epoch 98
Task: Classification | Acc: 97.36% | Avg Loss: 0.0725
Task: Reconstruction | Avg Loss: 2.5624 
MoE Balancing Loss: 10427.9883
Mutual Information | Avg Loss: -0.00358
Total Loss: 3.3696
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329915.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329557.19
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330231.41
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330426.41
Time elapsed: 2:59:08.565252

 --- Epoch 99
Task: Classification | Acc: 97.38% | Avg Loss: 0.0741
Task: Reconstruction | Avg Loss: 2.5363 
MoE Balancing Loss: 10428.3399
Mutual Information | Avg Loss: -0.00350
Total Loss: 3.3395
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331128.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331934.91
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331351.44
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331790.53
Time elapsed: 3:00:59.428065

 --- Epoch 100
Task: Classification | Acc: 97.33% | Avg Loss: 0.0730
Task: Reconstruction | Avg Loss: 2.5356 
MoE Balancing Loss: 10432.1614
Mutual Information | Avg Loss: -0.00348
Total Loss: 3.2788
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332937.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331217.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332194.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331754.59
Time elapsed: 3:02:49.874088
Example 1 ---
Original text: the volatile dynamics of female friendship is the subject of this unhurried, low - key film that is so off - hollywood that it seems positively french in its rhythms and resonance.
Reconstructed text: a the series of quickly - of - the film, ago artl, crowd - conscious actress twice as old - and teenage boy in love for the belt and.
Original IDs: [101, 1996, 20606, 10949, 1997, 2931, 6860, 2003, 1996, 3395, 1997, 2023, 4895, 24572, 11998, 1010, 2659, 1011, 3145, 2143, 2008, 2003, 2061, 2125, 1011, 5365, 2008, 2009, 3849, 13567, 2413, 1999, 2049, 17900, 1998, 17011, 1012, 102]
Predicted IDs: [101, 1037, 1996, 2186, 1997, 2855, 1011, 1997, 1011, 1996, 2143, 1010, 3283, 2396, 2140, 1010, 4306, 1011, 9715, 3883, 102, 3807, 2004, 2214, 1011, 102, 1998, 9454, 2879, 1999, 2293, 2005, 1996, 5583, 1998, 102, 1012, 102]
BLEU Score: 0.3229
Example 2 ---
Original text: without non - stop techno or the existential overtones of a kieslowski morality tale, maelstrom is just another winter sleepers.
Reconstructed text: a pitch's pers the bra yorklesm of the po accountantn that remains poomos in their tired apparentfold promises.
Original IDs: [101, 2302, 2512, 1011, 2644, 21416, 2030, 1996, 25953, 4818, 2058, 11115, 1997, 1037, 11382, 2229, 8261, 5488, 16561, 6925, 1010, 11530, 4877, 13887, 2003, 2074, 2178, 3467, 24372, 2015, 1012, 102]
Predicted IDs: [101, 1037, 6510, 1005, 1055, 2566, 2015, 1996, 11655, 2259, 2571, 6491, 1997, 1996, 13433, 17907, 2078, 102, 102, 2008, 3464, 13433, 19506, 2015, 1999, 2037, 5458, 6835, 10371, 10659, 1012, 102]
BLEU Score: 0.1810
Example 3 ---
Original text: a marvel like none you've seen.
Reconstructed text: a credit with that you'll seen,
Original IDs: [101, 1037, 8348, 2066, 3904, 2017, 1005, 2310, 2464, 1012, 102]
Predicted IDs: [101, 1037, 4923, 2007, 2008, 2017, 1005, 2222, 2464, 1010, 102]
BLEU Score: 0.3750

 --- Epoch 101
Task: Classification | Acc: 97.34% | Avg Loss: 0.0742
Task: Reconstruction | Avg Loss: 2.5400 
MoE Balancing Loss: 10428.9978
Mutual Information | Avg Loss: -0.00351
Total Loss: 3.3555
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333366.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333199.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 333706.88
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 334257.00
Time elapsed: 3:04:40.722649

 --- Epoch 102
Task: Classification | Acc: 97.49% | Avg Loss: 0.0689
Task: Reconstruction | Avg Loss: 2.5273 
MoE Balancing Loss: 10430.5314
Mutual Information | Avg Loss: -0.00355
Total Loss: 3.2904
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333040.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331245.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331476.34
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331330.88
Time elapsed: 3:06:30.875786

 --- Epoch 103
Task: Classification | Acc: 97.42% | Avg Loss: 0.0695
Task: Reconstruction | Avg Loss: 2.5293 
MoE Balancing Loss: 10428.9741
Mutual Information | Avg Loss: -0.00362
Total Loss: 3.3607
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330612.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330308.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330184.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330161.53
Time elapsed: 3:08:21.418482

 --- Epoch 104
Task: Classification | Acc: 97.32% | Avg Loss: 0.0712
Task: Reconstruction | Avg Loss: 2.5073 
MoE Balancing Loss: 10429.4436
Mutual Information | Avg Loss: -0.00360
Total Loss: 3.3692
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332291.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331066.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332456.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331823.81
Time elapsed: 3:10:12.610595

 --- Epoch 105
Task: Classification | Acc: 97.47% | Avg Loss: 0.0705
Task: Reconstruction | Avg Loss: 2.5017 
MoE Balancing Loss: 10426.7363
Mutual Information | Avg Loss: -0.00365
Total Loss: 3.3604
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330819.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330914.88
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331140.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331007.41
Time elapsed: 3:12:03.407166

 --- Epoch 106
Task: Classification | Acc: 97.44% | Avg Loss: 0.0716
Task: Reconstruction | Avg Loss: 2.5047 
MoE Balancing Loss: 10426.8327
Mutual Information | Avg Loss: -0.00355
Total Loss: 3.3264
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331747.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331761.62
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331652.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331548.94
Time elapsed: 3:13:53.962618

 --- Epoch 107
Task: Classification | Acc: 97.39% | Avg Loss: 0.0712
Task: Reconstruction | Avg Loss: 2.4899 
MoE Balancing Loss: 10428.0881
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.3921
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332073.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331930.75
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332143.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332088.94
Time elapsed: 3:15:45.101561

 --- Epoch 108
Task: Classification | Acc: 97.49% | Avg Loss: 0.0701
Task: Reconstruction | Avg Loss: 2.4838 
MoE Balancing Loss: 10430.5202
Mutual Information | Avg Loss: -0.00360
Total Loss: 3.3156
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331782.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330930.19
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330986.72
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330946.75
Time elapsed: 3:17:36.142026

 --- Epoch 109
Task: Classification | Acc: 97.51% | Avg Loss: 0.0704
Task: Reconstruction | Avg Loss: 2.4854 
MoE Balancing Loss: 10427.6027
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.3296
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330969.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331333.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331816.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332045.59
Time elapsed: 3:19:27.149009

 --- Epoch 110
Task: Classification | Acc: 97.26% | Avg Loss: 0.0737
Task: Reconstruction | Avg Loss: 2.4691 
MoE Balancing Loss: 10430.5793
Mutual Information | Avg Loss: -0.00356
Total Loss: 3.2560
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329949.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329999.78
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329987.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330170.62
Time elapsed: 3:21:17.339271
Example 1 ---
Original text: as a first - time director, paxton has tapped something in himself as an actor that provides frailty with its dark soul.
Reconstructed text: for his large - fashioned casting people on the hero for his life and is and them display its originality and a
Original IDs: [101, 2004, 1037, 2034, 1011, 2051, 2472, 1010, 27765, 2038, 10410, 2242, 1999, 2370, 2004, 2019, 3364, 2008, 3640, 25737, 3723, 2007, 2049, 2601, 3969, 1012, 102]
Predicted IDs: [101, 2005, 2010, 2312, 1011, 13405, 3459, 102, 102, 2075, 2111, 2006, 1996, 5394, 2005, 2010, 2166, 1998, 2003, 1998, 2068, 4653, 2049, 2434, 3012, 1998, 1037]
BLEU Score: 0.1245
Example 2 ---
Original text: director uwe boll and the actors provide scant reason to care in this crude'70s throwback.
Reconstructed text: director overome fars and the idlessonicy failing to compensate for the paper - gr characterizationplex.
Original IDs: [101, 2472, 1057, 8545, 8945, 3363, 1998, 1996, 5889, 3073, 13594, 2102, 3114, 2000, 2729, 1999, 2023, 13587, 1005, 17549, 5466, 5963, 1012, 102]
Predicted IDs: [101, 2472, 2058, 8462, 2521, 2015, 1998, 1996, 8909, 3238, 12356, 2100, 7989, 2000, 19079, 2005, 1996, 3259, 1011, 24665, 23191, 19386, 1012, 102]
BLEU Score: 0.3125
Example 3 ---
Original text: the film is quiet, threatening and unforgettable.
Reconstructed text: collateralis is beautiful, desirable and un deltraus
Original IDs: [101, 1996, 2143, 2003, 4251, 1010, 8701, 1998, 4895, 29278, 18150, 10880, 1012, 102]
Predicted IDs: [101, 24172, 2483, 2003, 3376, 1010, 16166, 1998, 4895, 3972, 6494, 2271, 102, 102]
BLEU Score: 0.3309

 --- Epoch 111
Task: Classification | Acc: 97.57% | Avg Loss: 0.0668
Task: Reconstruction | Avg Loss: 2.4773 
MoE Balancing Loss: 10424.7425
Mutual Information | Avg Loss: -0.00342
Total Loss: 3.1971
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330444.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331355.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331289.91
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331642.19
Time elapsed: 3:23:07.722349

 --- Epoch 112
Task: Classification | Acc: 97.68% | Avg Loss: 0.0660
Task: Reconstruction | Avg Loss: 2.4554 
MoE Balancing Loss: 10427.8669
Mutual Information | Avg Loss: -0.00359
Total Loss: 3.3489
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331961.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331432.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331987.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331829.50
Time elapsed: 3:24:59.006483

 --- Epoch 113
Task: Classification | Acc: 97.57% | Avg Loss: 0.0679
Task: Reconstruction | Avg Loss: 2.4488 
MoE Balancing Loss: 10426.3422
Mutual Information | Avg Loss: -0.00369
Total Loss: 3.2592
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333038.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332290.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.06, 0.05, 0.04, 0.04, 0.04], std: 332496.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332763.19
Time elapsed: 3:26:50.205683

 --- Epoch 114
Task: Classification | Acc: 97.58% | Avg Loss: 0.0657
Task: Reconstruction | Avg Loss: 2.4551 
MoE Balancing Loss: 10425.9657
Mutual Information | Avg Loss: -0.00365
Total Loss: 3.2705
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330117.72
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330493.19
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.06, 0.05, 0.04, 0.04, 0.04], std: 329795.22
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330612.09
Time elapsed: 3:28:40.948992

 --- Epoch 115
Task: Classification | Acc: 97.53% | Avg Loss: 0.0680
Task: Reconstruction | Avg Loss: 2.4433 
MoE Balancing Loss: 10429.1008
Mutual Information | Avg Loss: -0.00363
Total Loss: 3.2263
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332931.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333204.06
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333578.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332810.88
Time elapsed: 3:30:31.918892

 --- Epoch 116
Task: Classification | Acc: 97.77% | Avg Loss: 0.0632
Task: Reconstruction | Avg Loss: 2.4395 
MoE Balancing Loss: 10425.7429
Mutual Information | Avg Loss: -0.00373
Total Loss: 3.3015
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330797.94
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330220.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330638.88
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330629.78
Time elapsed: 3:32:23.116145

 --- Epoch 117
Task: Classification | Acc: 97.63% | Avg Loss: 0.0652
Task: Reconstruction | Avg Loss: 2.4250 
MoE Balancing Loss: 10429.6555
Mutual Information | Avg Loss: -0.00372
Total Loss: 3.3319
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332306.34
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330523.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330981.47
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331078.38
Time elapsed: 3:34:14.517821

 --- Epoch 118
Task: Classification | Acc: 97.61% | Avg Loss: 0.0663
Task: Reconstruction | Avg Loss: 2.4213 
MoE Balancing Loss: 10427.5079
Mutual Information | Avg Loss: -0.00357
Total Loss: 3.2556
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331588.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332206.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332250.72
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332437.66
Time elapsed: 3:36:05.720749

 --- Epoch 119
Task: Classification | Acc: 97.60% | Avg Loss: 0.0655
Task: Reconstruction | Avg Loss: 2.4081 
MoE Balancing Loss: 10428.5987
Mutual Information | Avg Loss: -0.00383
Total Loss: 3.4420
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331403.34
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330034.06
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.06, 0.05, 0.04, 0.04, 0.04], std: 330012.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330208.78
Time elapsed: 3:37:57.718133

 --- Epoch 120
Task: Classification | Acc: 97.56% | Avg Loss: 0.0679
Task: Reconstruction | Avg Loss: 2.4045 
MoE Balancing Loss: 10431.3755
Mutual Information | Avg Loss: -0.00364
Total Loss: 3.2927
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333276.19
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331257.25
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331514.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331070.50
Time elapsed: 3:39:48.745179
Example 1 ---
Original text: a deep and meaningful film.
Reconstructed text: a sweet, moving film part
Original IDs: [101, 1037, 2784, 1998, 15902, 2143, 1012, 102]
Predicted IDs: [101, 1037, 4086, 1010, 3048, 2143, 102, 2112]
BLEU Score: 0.3333
Example 2 ---
Original text: a valueless kiddie paean to pro basketball underwritten by the nba.
Reconstructed text: the trench trenchflasborsible of engaging earlyodity in the title world
Original IDs: [101, 1037, 3643, 3238, 25358, 2666, 6643, 11219, 2000, 4013, 3455, 2104, 15773, 2011, 1996, 6452, 1012, 102]
Predicted IDs: [101, 1996, 14185, 14185, 10258, 3022, 12821, 19307, 1997, 11973, 2220, 7716, 3012, 1999, 1996, 2516, 2088, 102]
BLEU Score: 0.0819
Example 3 ---
Original text: harris commands the screen, using his frailty to suggest the ravages of a life of corruption and ruthlessness.
Reconstructed text: as, humor, and the film,p on the dem anniversary edition of its role of philadelphia and american beauty.
Original IDs: [101, 5671, 10954, 1996, 3898, 1010, 2478, 2010, 25737, 3723, 2000, 6592, 1996, 10958, 3567, 8449, 1997, 1037, 2166, 1997, 7897, 1998, 18101, 2791, 1012, 102]
Predicted IDs: [101, 1037, 2015, 1010, 8562, 1010, 1998, 1996, 2143, 1010, 2361, 2006, 1996, 17183, 5315, 3179, 1997, 2049, 2535, 1997, 4407, 1998, 2137, 5053, 1012, 102]
BLEU Score: 0.3043

 --- Epoch 121
Task: Classification | Acc: 97.61% | Avg Loss: 0.0666
Task: Reconstruction | Avg Loss: 2.4044 
MoE Balancing Loss: 10426.7103
Mutual Information | Avg Loss: -0.00371
Total Loss: 3.3303
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331918.19
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332276.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332426.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332475.12
Time elapsed: 3:41:39.968087

 --- Epoch 122
Task: Classification | Acc: 97.73% | Avg Loss: 0.0645
Task: Reconstruction | Avg Loss: 2.4014 
MoE Balancing Loss: 10429.7183
Mutual Information | Avg Loss: -0.00362
Total Loss: 3.2316
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329979.09
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329814.97
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330659.56
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330607.66
Time elapsed: 3:43:30.927390

 --- Epoch 123
Task: Classification | Acc: 97.69% | Avg Loss: 0.0642
Task: Reconstruction | Avg Loss: 2.3793 
MoE Balancing Loss: 10428.2598
Mutual Information | Avg Loss: -0.00368
Total Loss: 3.2552
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332231.41
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331031.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331132.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330346.69
Time elapsed: 3:45:21.802246

 --- Epoch 124
Task: Classification | Acc: 97.61% | Avg Loss: 0.0676
Task: Reconstruction | Avg Loss: 2.3927 
MoE Balancing Loss: 10428.9303
Mutual Information | Avg Loss: -0.00374
Total Loss: 3.3248
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330832.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329925.91
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330619.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330494.88
Time elapsed: 3:47:12.910036

 --- Epoch 125
Task: Classification | Acc: 97.70% | Avg Loss: 0.0675
Task: Reconstruction | Avg Loss: 2.3759 
MoE Balancing Loss: 10428.0407
Mutual Information | Avg Loss: -0.00384
Total Loss: 3.3630
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331110.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330885.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330993.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331291.69
Time elapsed: 3:49:04.566302

 --- Epoch 126
Task: Classification | Acc: 97.70% | Avg Loss: 0.0649
Task: Reconstruction | Avg Loss: 2.3807 
MoE Balancing Loss: 10425.7034
Mutual Information | Avg Loss: -0.00370
Total Loss: 3.2336
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329895.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329702.59
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329772.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330142.62
Time elapsed: 3:50:54.887423

 --- Epoch 127
Task: Classification | Acc: 97.77% | Avg Loss: 0.0629
Task: Reconstruction | Avg Loss: 2.3728 
MoE Balancing Loss: 10429.6974
Mutual Information | Avg Loss: -0.00364
Total Loss: 3.2564
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331973.03
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331359.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331512.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331620.12
Time elapsed: 3:52:45.688330

 --- Epoch 128
Task: Classification | Acc: 97.66% | Avg Loss: 0.0647
Task: Reconstruction | Avg Loss: 2.3562 
MoE Balancing Loss: 10430.0884
Mutual Information | Avg Loss: -0.00375
Total Loss: 3.2569
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332651.22
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331549.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331354.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331170.44
Time elapsed: 3:54:36.604445

 --- Epoch 129
Task: Classification | Acc: 97.80% | Avg Loss: 0.0635
Task: Reconstruction | Avg Loss: 2.3631 
MoE Balancing Loss: 10428.2299
Mutual Information | Avg Loss: -0.00371
Total Loss: 3.2116
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330450.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330921.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331342.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331879.34
Time elapsed: 3:56:27.132731

 --- Epoch 130
Task: Classification | Acc: 97.71% | Avg Loss: 0.0607
Task: Reconstruction | Avg Loss: 2.3560 
MoE Balancing Loss: 10426.2316
Mutual Information | Avg Loss: -0.00382
Total Loss: 3.2749
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332598.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331843.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331546.47
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331191.69
Time elapsed: 3:58:18.162526
Example 1 ---
Original text: kinnear doesn't aim for our sympathy, but rather delivers a performance of striking skill and depth.
Reconstructed text: the bourne that doesn't have up its engaging, is us power the sense of wonder in the future.
Original IDs: [101, 12631, 22084, 2099, 2515, 1050, 1005, 1056, 6614, 2005, 2256, 11883, 1010, 2021, 2738, 18058, 1037, 2836, 1997, 8478, 8066, 1998, 5995, 1012, 102]
Predicted IDs: [101, 1996, 15803, 2008, 2515, 1050, 1005, 1056, 2031, 2039, 2049, 11973, 1010, 2003, 2149, 2373, 1996, 3168, 1997, 4687, 1999, 1996, 2925, 1012, 102]
BLEU Score: 0.2381
Example 2 ---
Original text: it's slow - - very, very slow.
Reconstructed text: it's old - -, any, as the
Original IDs: [101, 2009, 1005, 1055, 4030, 1011, 1011, 2200, 1010, 2200, 4030, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 2214, 1011, 1011, 102, 1010, 2151, 1010, 2004, 1996]
BLEU Score: 0.5000
Example 3 ---
Original text: a grimly competent and stolid and earnest military courtroom drama.
Reconstructed text: itsustal and redcends and surprisinglyome tone
Original IDs: [101, 1037, 22561, 17824, 1998, 2358, 10893, 2094, 1998, 17300, 2510, 20747, 3689, 1012, 102]
Predicted IDs: [101, 2049, 19966, 2389, 1998, 2417, 23865, 2015, 1998, 10889, 8462, 4309, 102, 102, 102]
BLEU Score: 0.1449

 --- Epoch 131
Task: Classification | Acc: 97.72% | Avg Loss: 0.0626
Task: Reconstruction | Avg Loss: 2.3456 
MoE Balancing Loss: 10425.7877
Mutual Information | Avg Loss: -0.00375
Total Loss: 3.3015
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330456.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330586.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330347.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330468.34
Time elapsed: 4:00:09.245965

 --- Epoch 132
Task: Classification | Acc: 97.81% | Avg Loss: 0.0631
Task: Reconstruction | Avg Loss: 2.3413 
MoE Balancing Loss: 10425.3670
Mutual Information | Avg Loss: -0.00356
Total Loss: 3.1890
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332053.44
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330149.19
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331261.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330970.16
Time elapsed: 4:01:59.649415

 --- Epoch 133
Task: Classification | Acc: 97.66% | Avg Loss: 0.0634
Task: Reconstruction | Avg Loss: 2.3462 
MoE Balancing Loss: 10432.9045
Mutual Information | Avg Loss: -0.00364
Total Loss: 3.2268
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331665.59
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330908.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331118.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331144.75
Time elapsed: 4:03:50.431483

 --- Epoch 134
Task: Classification | Acc: 97.65% | Avg Loss: 0.0623
Task: Reconstruction | Avg Loss: 2.3215 
MoE Balancing Loss: 10429.5439
Mutual Information | Avg Loss: -0.00369
Total Loss: 3.2430
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331659.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331235.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332034.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332104.59
Time elapsed: 4:05:41.429673

 --- Epoch 135
Task: Classification | Acc: 97.77% | Avg Loss: 0.0614
Task: Reconstruction | Avg Loss: 2.3304 
MoE Balancing Loss: 10426.8778
Mutual Information | Avg Loss: -0.00365
Total Loss: 3.1565
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331810.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331310.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331024.22
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331298.94
Time elapsed: 4:07:31.550631

 --- Epoch 136
Task: Classification | Acc: 97.81% | Avg Loss: 0.0597
Task: Reconstruction | Avg Loss: 2.3293 
MoE Balancing Loss: 10425.5288
Mutual Information | Avg Loss: -0.00361
Total Loss: 3.1983
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331650.56
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332684.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331841.50
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332239.97
Time elapsed: 4:09:22.046413

 --- Epoch 137
Task: Classification | Acc: 97.76% | Avg Loss: 0.0616
Task: Reconstruction | Avg Loss: 2.3130 
MoE Balancing Loss: 10425.3385
Mutual Information | Avg Loss: -0.00377
Total Loss: 3.2454
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329355.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 328992.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330048.34
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329227.47
Time elapsed: 4:11:12.472931

 --- Epoch 138
Task: Classification | Acc: 97.80% | Avg Loss: 0.0610
Task: Reconstruction | Avg Loss: 2.3211 
MoE Balancing Loss: 10428.4279
Mutual Information | Avg Loss: -0.00379
Total Loss: 3.2539
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332142.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330317.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330775.69
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330971.78
Time elapsed: 4:13:03.346550

 --- Epoch 139
Task: Classification | Acc: 97.87% | Avg Loss: 0.0590
Task: Reconstruction | Avg Loss: 2.3125 
MoE Balancing Loss: 10424.9655
Mutual Information | Avg Loss: -0.00370
Total Loss: 3.2444
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331139.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332077.28
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332584.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332401.91
Time elapsed: 4:14:54.298346

 --- Epoch 140
Task: Classification | Acc: 97.72% | Avg Loss: 0.0625
Task: Reconstruction | Avg Loss: 2.3124 
MoE Balancing Loss: 10423.8142
Mutual Information | Avg Loss: -0.00373
Total Loss: 3.1858
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331192.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330638.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330517.91
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330957.66
Time elapsed: 4:16:44.546665
Example 1 ---
Original text: it's a remarkably solid and subtly satirical tour de force.
Reconstructed text: it's a very sincere work, deeply of hollywood enterprise.
Original IDs: [101, 2009, 1005, 1055, 1037, 17431, 5024, 1998, 28797, 17251, 2778, 2139, 2486, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 1037, 2200, 18006, 2147, 1010, 6171, 1997, 5365, 6960, 1012, 102]
BLEU Score: 0.3333
Example 2 ---
Original text: while its careful pace and seemingly opaque story may not satisfy every moviegoer's appetite, the film's final scene is soaringly, transparently moving.
Reconstructed text: is a transparent a ho nift, to even the daring as john ritter's interest, the three's end, and 20 gaze - s.
Original IDs: [101, 2096, 2049, 6176, 6393, 1998, 9428, 28670, 2466, 2089, 2025, 13225, 2296, 3185, 3995, 2121, 1005, 1055, 18923, 1010, 1996, 2143, 1005, 1055, 2345, 3496, 2003, 23990, 2135, 1010, 13338, 2135, 3048, 1012, 102]
Predicted IDs: [101, 2003, 1037, 13338, 1037, 7570, 9152, 6199, 1010, 2000, 2130, 1996, 15236, 2004, 2198, 23168, 1005, 1055, 3037, 1010, 1996, 2093, 1005, 1055, 2203, 1010, 1998, 2322, 3657, 1011, 102, 1055, 1012, 102, 102]
BLEU Score: 0.2857
Example 3 ---
Original text: this is human comedy at its most amusing, interesting and confirming.
Reconstructed text: this is a film that is very funny, amusing and unpredictable.
Original IDs: [101, 2023, 2003, 2529, 4038, 2012, 2049, 2087, 19142, 1010, 5875, 1998, 19195, 1012, 102]
Predicted IDs: [101, 2023, 2003, 1037, 2143, 2008, 2003, 2200, 6057, 1010, 19142, 1998, 21446, 1012, 102]
BLEU Score: 0.4615

 --- Epoch 141
Task: Classification | Acc: 97.89% | Avg Loss: 0.0571
Task: Reconstruction | Avg Loss: 2.2959 
MoE Balancing Loss: 10429.7879
Mutual Information | Avg Loss: -0.00386
Total Loss: 3.3365
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332506.44
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332082.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332296.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331799.53
Time elapsed: 4:18:36.138493

 --- Epoch 142
Task: Classification | Acc: 97.79% | Avg Loss: 0.0620
Task: Reconstruction | Avg Loss: 2.2986 
MoE Balancing Loss: 10428.1706
Mutual Information | Avg Loss: -0.00385
Total Loss: 3.2168
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328179.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329622.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329604.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329230.97
Time elapsed: 4:20:26.976160

 --- Epoch 143
Task: Classification | Acc: 97.94% | Avg Loss: 0.0577
Task: Reconstruction | Avg Loss: 2.3014 
MoE Balancing Loss: 10427.5552
Mutual Information | Avg Loss: -0.00389
Total Loss: 3.2198
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331334.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330771.75
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330804.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331355.09
Time elapsed: 4:22:17.555922

 --- Epoch 144
Task: Classification | Acc: 97.82% | Avg Loss: 0.0594
Task: Reconstruction | Avg Loss: 2.2849 
MoE Balancing Loss: 10427.1359
Mutual Information | Avg Loss: -0.00385
Total Loss: 3.2211
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332491.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331626.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331378.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331814.16
Time elapsed: 4:24:08.506145

 --- Epoch 145
Task: Classification | Acc: 97.83% | Avg Loss: 0.0575
Task: Reconstruction | Avg Loss: 2.2808 
MoE Balancing Loss: 10425.2346
Mutual Information | Avg Loss: -0.00390
Total Loss: 3.2384
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330143.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329571.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329752.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329543.97
Time elapsed: 4:25:59.737351

 --- Epoch 146
Task: Classification | Acc: 97.94% | Avg Loss: 0.0569
Task: Reconstruction | Avg Loss: 2.2694 
MoE Balancing Loss: 10430.7437
Mutual Information | Avg Loss: -0.00385
Total Loss: 3.1835
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331194.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330626.75
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331344.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331287.66
Time elapsed: 4:27:50.883158

 --- Epoch 147
Task: Classification | Acc: 97.87% | Avg Loss: 0.0599
Task: Reconstruction | Avg Loss: 2.2633 
MoE Balancing Loss: 10426.9115
Mutual Information | Avg Loss: -0.00381
Total Loss: 3.2319
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332007.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332739.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333034.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332593.22
Time elapsed: 4:29:42.244781

 --- Epoch 148
Task: Classification | Acc: 97.84% | Avg Loss: 0.0606
Task: Reconstruction | Avg Loss: 2.2672 
MoE Balancing Loss: 10425.4545
Mutual Information | Avg Loss: -0.00389
Total Loss: 3.2206
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331230.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330704.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331454.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331657.72
Time elapsed: 4:31:33.231677

 --- Epoch 149
Task: Classification | Acc: 97.82% | Avg Loss: 0.0593
Task: Reconstruction | Avg Loss: 2.2577 
MoE Balancing Loss: 10427.9032
Mutual Information | Avg Loss: -0.00387
Total Loss: 3.1991
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331832.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331915.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332178.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331690.84
Time elapsed: 4:33:24.534849

 --- Epoch 150
Task: Classification | Acc: 97.91% | Avg Loss: 0.0578
Task: Reconstruction | Avg Loss: 2.2681 
MoE Balancing Loss: 10425.7991
Mutual Information | Avg Loss: -0.00395
Total Loss: 3.2274
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332470.59
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332315.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332085.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332415.78
Time elapsed: 4:35:16.056179
Example 1 ---
Original text: pumpkin takes an admirable look at the hypocrisy of political correctness, but it does so with such an uneven tone that you never know when humor ends and tragedy begins.
Reconstructed text: despite a acrucnt film and a a soulcerater sense of obsessive, only, that a a period period, that as it is the to call...
Original IDs: [101, 25730, 3138, 2019, 4748, 14503, 3085, 2298, 2012, 1996, 1044, 22571, 10085, 6935, 2100, 1997, 2576, 6149, 2791, 1010, 2021, 2009, 2515, 2061, 2007, 2107, 2019, 17837, 4309, 2008, 2017, 2196, 2113, 2043, 8562, 4515, 1998, 10576, 4269, 1012, 102]
Predicted IDs: [101, 2750, 1037, 1037, 26775, 14194, 3372, 2143, 1998, 1037, 1037, 3969, 19357, 3334, 3168, 1997, 27885, 8583, 12742, 1010, 102, 2069, 1010, 2008, 1037, 1037, 2558, 2558, 1010, 2008, 2004, 2009, 2003, 1996, 2000, 2655, 1012, 102, 1012, 1012, 102]
BLEU Score: 0.1858
Example 2 ---
Original text: it's a work by an artist so in control of both his medium and his message that he can improvise like a jazzman.
Reconstructed text: it's a to to the the, the kind of the, that that we ever left out the twoerer with the good punch
Original IDs: [101, 2009, 1005, 1055, 1037, 2147, 2011, 2019, 3063, 2061, 1999, 2491, 1997, 2119, 2010, 5396, 1998, 2010, 4471, 2008, 2002, 2064, 17727, 12298, 5562, 2066, 1037, 4166, 2386, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 1037, 2000, 2000, 1996, 1996, 1010, 1996, 2785, 1997, 1996, 1010, 2008, 2008, 2057, 2412, 2187, 2041, 1996, 2048, 2121, 2121, 2007, 1996, 2204, 8595, 102, 102]
BLEU Score: 0.2000
Example 3 ---
Original text: it's a lovely film with lovely performances by buy and accorsi.
Reconstructed text: there's a best performance in the filmmaking with and and stu weird message,
Original IDs: [101, 2009, 1005, 1055, 1037, 8403, 2143, 2007, 8403, 4616, 2011, 4965, 1998, 16222, 5668, 2072, 1012, 102]
Predicted IDs: [101, 2045, 1005, 1055, 1037, 2190, 2836, 1999, 1996, 24466, 2007, 1998, 1998, 24646, 6881, 4471, 1010, 102]
BLEU Score: 0.2667

 --- Epoch 151
Task: Classification | Acc: 97.97% | Avg Loss: 0.0559
Task: Reconstruction | Avg Loss: 2.2519 
MoE Balancing Loss: 10431.5955
Mutual Information | Avg Loss: -0.00404
Total Loss: 3.2228
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330655.03
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328287.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328714.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328722.88
Time elapsed: 4:37:07.584459

 --- Epoch 152
Task: Classification | Acc: 97.78% | Avg Loss: 0.0605
Task: Reconstruction | Avg Loss: 2.2522 
MoE Balancing Loss: 10428.7148
Mutual Information | Avg Loss: -0.00392
Total Loss: 3.1924
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330197.09
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329858.88
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329889.47
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329736.34
Time elapsed: 4:38:58.556889

 --- Epoch 153
Task: Classification | Acc: 98.07% | Avg Loss: 0.0530
Task: Reconstruction | Avg Loss: 2.2499 
MoE Balancing Loss: 10425.1003
Mutual Information | Avg Loss: -0.00389
Total Loss: 3.2830
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330832.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331542.31
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331477.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331768.31
Time elapsed: 4:40:50.427558

 --- Epoch 154
Task: Classification | Acc: 97.96% | Avg Loss: 0.0574
Task: Reconstruction | Avg Loss: 2.2359 
MoE Balancing Loss: 10429.2145
Mutual Information | Avg Loss: -0.00380
Total Loss: 3.1470
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330181.00
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330956.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330973.31
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330531.09
Time elapsed: 4:42:41.079324

 --- Epoch 155
Task: Classification | Acc: 97.94% | Avg Loss: 0.0591
Task: Reconstruction | Avg Loss: 2.2429 
MoE Balancing Loss: 10429.0818
Mutual Information | Avg Loss: -0.00380
Total Loss: 3.2009
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332781.28
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331810.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332197.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331996.25
Time elapsed: 4:44:32.319734

 --- Epoch 156
Task: Classification | Acc: 97.93% | Avg Loss: 0.0565
Task: Reconstruction | Avg Loss: 2.2286 
MoE Balancing Loss: 10423.9195
Mutual Information | Avg Loss: -0.00376
Total Loss: 3.1960
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331131.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331412.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330969.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330684.09
Time elapsed: 4:46:23.608328

 --- Epoch 157
Task: Classification | Acc: 97.85% | Avg Loss: 0.0589
Task: Reconstruction | Avg Loss: 2.2276 
MoE Balancing Loss: 10425.6730
Mutual Information | Avg Loss: -0.00381
Total Loss: 3.1924
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330500.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330849.12
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330812.41
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330856.75
Time elapsed: 4:48:14.610111

 --- Epoch 158
Task: Classification | Acc: 98.00% | Avg Loss: 0.0580
Task: Reconstruction | Avg Loss: 2.2302 
MoE Balancing Loss: 10426.5494
Mutual Information | Avg Loss: -0.00394
Total Loss: 3.2416
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330425.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329917.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330081.84
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330513.94
Time elapsed: 4:50:06.007352

 --- Epoch 159
Task: Classification | Acc: 98.13% | Avg Loss: 0.0551
Task: Reconstruction | Avg Loss: 2.2231 
MoE Balancing Loss: 10425.6447
Mutual Information | Avg Loss: -0.00387
Total Loss: 3.1958
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333065.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332927.62
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333595.69
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333495.50
Time elapsed: 4:51:57.490269

 --- Epoch 160
Task: Classification | Acc: 98.04% | Avg Loss: 0.0571
Task: Reconstruction | Avg Loss: 2.2202 
MoE Balancing Loss: 10429.9366
Mutual Information | Avg Loss: -0.00389
Total Loss: 3.1180
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330226.47
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330615.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331127.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330334.72
Time elapsed: 4:53:48.288753
Example 1 ---
Original text: the vivid lead performances sustain interest and empathy, but the journey is far more interesting than the final destination.
Reconstructed text: a poorly story and built lacks, williams, and the little as far too much as a original.
Original IDs: [101, 1996, 14954, 2599, 4616, 15770, 3037, 1998, 26452, 1010, 2021, 1996, 4990, 2003, 2521, 2062, 5875, 2084, 1996, 2345, 7688, 1012, 102]
Predicted IDs: [101, 1037, 9996, 2466, 1998, 2328, 14087, 1010, 3766, 1010, 1998, 1996, 2210, 2004, 2521, 2205, 2172, 2004, 1037, 2434, 1012, 102, 102]
BLEU Score: 0.2378
Example 2 ---
Original text: the director knows how to apply textural gloss, but his portrait of sex - as - war is strictly sitcom.
Reconstructed text: the film of like and and punchless movie, about the sides of you - in's the comedies of oscar
Original IDs: [101, 1996, 2472, 4282, 2129, 2000, 6611, 3793, 11137, 27068, 1010, 2021, 2010, 6533, 1997, 3348, 1011, 2004, 1011, 2162, 2003, 9975, 13130, 1012, 102]
Predicted IDs: [101, 1996, 2143, 1997, 2066, 1998, 1998, 8595, 3238, 3185, 1010, 2055, 1996, 3903, 1997, 2017, 1011, 1999, 1005, 1055, 1996, 22092, 1997, 102, 7436]
BLEU Score: 0.1816
Example 3 ---
Original text: some of their jokes work, but most fail miserably and in the end, pumpkin is far more offensive than it is funny.
Reconstructed text: need of a cube canon so b deed,, but the project movie'is experiences as enjoyable as it is themselves.
Original IDs: [101, 2070, 1997, 2037, 13198, 2147, 1010, 2021, 2087, 8246, 28616, 6906, 6321, 1998, 1999, 1996, 2203, 1010, 25730, 2003, 2521, 2062, 5805, 2084, 2009, 2003, 6057, 1012, 102]
Predicted IDs: [101, 2342, 1997, 1037, 14291, 9330, 102, 102, 2061, 1038, 2139, 2098, 1010, 1010, 2021, 1996, 2622, 3185, 1005, 2003, 6322, 2004, 22249, 2004, 2009, 2003, 3209, 1012, 102]
BLEU Score: 0.3173

 --- Epoch 161
Task: Classification | Acc: 97.97% | Avg Loss: 0.0559
Task: Reconstruction | Avg Loss: 2.2347 
MoE Balancing Loss: 10428.7163
Mutual Information | Avg Loss: -0.00382
Total Loss: 3.1371
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331964.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331696.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331736.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331578.91
Time elapsed: 4:55:39.424374

 --- Epoch 162
Task: Classification | Acc: 97.99% | Avg Loss: 0.0554
Task: Reconstruction | Avg Loss: 2.2142 
MoE Balancing Loss: 10429.5314
Mutual Information | Avg Loss: -0.00399
Total Loss: 3.2116
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329718.59
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329360.97
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329175.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330082.12
Time elapsed: 4:57:31.004590

 --- Epoch 163
Task: Classification | Acc: 98.04% | Avg Loss: 0.0556
Task: Reconstruction | Avg Loss: 2.2072 
MoE Balancing Loss: 10426.4549
Mutual Information | Avg Loss: -0.00385
Total Loss: 3.0986
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330729.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331923.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332478.41
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332280.31
Time elapsed: 4:59:21.791082

 --- Epoch 164
Task: Classification | Acc: 97.94% | Avg Loss: 0.0574
Task: Reconstruction | Avg Loss: 2.1874 
MoE Balancing Loss: 10425.6851
Mutual Information | Avg Loss: -0.00391
Total Loss: 3.1907
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332489.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332568.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332967.91
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332777.91
Time elapsed: 5:01:13.341489

 --- Epoch 165
Task: Classification | Acc: 97.97% | Avg Loss: 0.0572
Task: Reconstruction | Avg Loss: 2.2070 
MoE Balancing Loss: 10429.7723
Mutual Information | Avg Loss: -0.00386
Total Loss: 3.1366
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332356.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330787.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331322.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331197.12
Time elapsed: 5:03:04.085573

 --- Epoch 166
Task: Classification | Acc: 98.00% | Avg Loss: 0.0562
Task: Reconstruction | Avg Loss: 2.1926 
MoE Balancing Loss: 10425.4363
Mutual Information | Avg Loss: -0.00395
Total Loss: 3.1802
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330224.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330661.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331040.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330291.09
Time elapsed: 5:04:55.272663

 --- Epoch 167
Task: Classification | Acc: 97.99% | Avg Loss: 0.0558
Task: Reconstruction | Avg Loss: 2.1916 
MoE Balancing Loss: 10428.4973
Mutual Information | Avg Loss: -0.00389
Total Loss: 3.1887
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332879.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332653.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332434.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332979.19
Time elapsed: 5:06:46.570283

 --- Epoch 168
Task: Classification | Acc: 97.93% | Avg Loss: 0.0586
Task: Reconstruction | Avg Loss: 2.1984 
MoE Balancing Loss: 10424.4671
Mutual Information | Avg Loss: -0.00387
Total Loss: 3.1686
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331672.72
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332322.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332698.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332210.47
Time elapsed: 5:08:37.834389

 --- Epoch 169
Task: Classification | Acc: 97.96% | Avg Loss: 0.0557
Task: Reconstruction | Avg Loss: 2.1966 
MoE Balancing Loss: 10430.4065
Mutual Information | Avg Loss: -0.00381
Total Loss: 3.1599
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333890.00
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332421.75
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332494.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332401.56
Time elapsed: 5:10:29.066709

 --- Epoch 170
Task: Classification | Acc: 98.04% | Avg Loss: 0.0560
Task: Reconstruction | Avg Loss: 2.1781 
MoE Balancing Loss: 10429.0555
Mutual Information | Avg Loss: -0.00399
Total Loss: 3.2455
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332520.94
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330654.75
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330968.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331271.97
Time elapsed: 5:12:20.744597
Example 1 ---
Original text: without ever becoming didactic, director carlos carrera expertly weaves this novelistic story of entangled interrelationships and complex morality.
Reconstructed text: is a a computers and his comedy, that bearss the moments to quieter the tract of p satirical prefloitism and drinks
Original IDs: [101, 2302, 2412, 3352, 2106, 28804, 1010, 2472, 5828, 12385, 6906, 6739, 2135, 25308, 2015, 2023, 9974, 2594, 2466, 1997, 4372, 27898, 6970, 16570, 10708, 19801, 1998, 3375, 16561, 1012, 102]
Predicted IDs: [101, 2003, 1037, 1037, 3274, 2015, 1998, 2010, 4038, 1010, 2008, 6468, 2015, 1996, 5312, 2000, 27486, 1996, 12859, 1997, 1052, 17251, 3653, 10258, 28100, 2964, 1998, 4392, 2015, 102, 102]
BLEU Score: 0.1364
Example 2 ---
Original text: the film's performances are thrilling.
Reconstructed text: the movie's relentless fiveved. '
Original IDs: [101, 1996, 2143, 1005, 1055, 4616, 2024, 26162, 1012, 102]
Predicted IDs: [101, 1996, 3185, 1005, 1055, 21660, 2274, 7178, 1012, 1005]
BLEU Score: 0.4286
Example 3 ---
Original text: more whiny downer than corruscating commentary.
Reconstructed text: an catgesrine bal in an untistint ending
Original IDs: [101, 2062, 1059, 10606, 2100, 2091, 2121, 2084, 2522, 12171, 2271, 18252, 8570, 1012, 102]
Predicted IDs: [101, 2019, 4937, 8449, 11467, 28352, 1999, 2019, 4895, 3775, 16643, 3372, 4566, 102, 102]
BLEU Score: 0.0000

 --- Epoch 171
Task: Classification | Acc: 97.92% | Avg Loss: 0.0576
Task: Reconstruction | Avg Loss: 2.1871 
MoE Balancing Loss: 10426.3968
Mutual Information | Avg Loss: -0.00381
Total Loss: 3.0988
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331318.94
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330664.69
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330812.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331047.72
Time elapsed: 5:14:11.667970

 --- Epoch 172
Task: Classification | Acc: 97.95% | Avg Loss: 0.0580
Task: Reconstruction | Avg Loss: 2.1776 
MoE Balancing Loss: 10427.1055
Mutual Information | Avg Loss: -0.00391
Total Loss: 3.1863
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331177.56
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331097.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331490.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331682.66
Time elapsed: 5:16:03.322950

 --- Epoch 173
Task: Classification | Acc: 98.07% | Avg Loss: 0.0534
Task: Reconstruction | Avg Loss: 2.1812 
MoE Balancing Loss: 10424.7446
Mutual Information | Avg Loss: -0.00382
Total Loss: 3.1216
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332144.34
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331397.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332330.44
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332261.50
Time elapsed: 5:17:54.524077

 --- Epoch 174
Task: Classification | Acc: 98.19% | Avg Loss: 0.0532
Task: Reconstruction | Avg Loss: 2.1726 
MoE Balancing Loss: 10424.6721
Mutual Information | Avg Loss: -0.00400
Total Loss: 3.1317
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333720.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332410.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332161.06
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331938.56
Time elapsed: 5:19:45.604685

 --- Epoch 175
Task: Classification | Acc: 98.09% | Avg Loss: 0.0536
Task: Reconstruction | Avg Loss: 2.1746 
MoE Balancing Loss: 10428.3984
Mutual Information | Avg Loss: -0.00397
Total Loss: 3.1097
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330536.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330837.62
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331642.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332192.22
Time elapsed: 5:21:36.471673

 --- Epoch 176
Task: Classification | Acc: 98.07% | Avg Loss: 0.0539
Task: Reconstruction | Avg Loss: 2.1648 
MoE Balancing Loss: 10425.0046
Mutual Information | Avg Loss: -0.00409
Total Loss: 3.1955
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333202.03
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331782.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332472.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331890.97
Time elapsed: 5:23:28.038682

 --- Epoch 177
Task: Classification | Acc: 97.85% | Avg Loss: 0.0547
Task: Reconstruction | Avg Loss: 2.1653 
MoE Balancing Loss: 10427.3425
Mutual Information | Avg Loss: -0.00412
Total Loss: 3.1844
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330010.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331620.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331294.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332189.75
Time elapsed: 5:25:19.653387

 --- Epoch 178
Task: Classification | Acc: 98.24% | Avg Loss: 0.0513
Task: Reconstruction | Avg Loss: 2.1638 
MoE Balancing Loss: 10428.2764
Mutual Information | Avg Loss: -0.00419
Total Loss: 3.2095
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331601.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330996.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331943.56
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331311.81
Time elapsed: 5:27:11.089518

 --- Epoch 179
Task: Classification | Acc: 98.05% | Avg Loss: 0.0530
Task: Reconstruction | Avg Loss: 2.1546 
MoE Balancing Loss: 10424.7928
Mutual Information | Avg Loss: -0.00408
Total Loss: 3.1540
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333211.50
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332510.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332232.56
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332609.00
Time elapsed: 5:29:02.549698

 --- Epoch 180
Task: Classification | Acc: 98.12% | Avg Loss: 0.0509
Task: Reconstruction | Avg Loss: 2.1560 
MoE Balancing Loss: 10424.4303
Mutual Information | Avg Loss: -0.00400
Total Loss: 3.1024
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330333.72
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330265.62
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330461.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330841.12
Time elapsed: 5:30:53.560098
Example 1 ---
Original text: it's a much more emotional journey than what shyamalan has given us in his past two movies, and gibson, stepping in for bruce willis, is the perfect actor to take us on the trip.
Reconstructed text: it's a a fan understandsage for of ta / greenams, gives to in his most job, but ultimately triumph, and as charmism and is is seems to feel it..
Original IDs: [101, 2009, 1005, 1055, 1037, 2172, 2062, 6832, 4990, 2084, 2054, 11004, 8067, 5802, 2038, 2445, 2149, 1999, 2010, 2627, 2048, 5691, 1010, 1998, 9406, 1010, 9085, 1999, 2005, 5503, 12688, 1010, 2003, 1996, 3819, 3364, 2000, 2202, 2149, 2006, 1996, 4440, 1012, 102]
Predicted IDs: [101, 2009, 1005, 1055, 1037, 1037, 5470, 19821, 4270, 2005, 1997, 11937, 1013, 2665, 13596, 1010, 3957, 2000, 1999, 2010, 2087, 3105, 1010, 2021, 4821, 10911, 1010, 1998, 2004, 11084, 2964, 1998, 102, 2003, 2003, 3849, 2000, 2514, 2009, 1012, 102, 102, 1012, 102]
BLEU Score: 0.3047
Example 2 ---
Original text: ... mafia, rap stars and hood rats butt their ugly heads in a regurgitation of cinematic violence that gives brutal birth to an unlikely, but likable, hero. '
Reconstructed text: the it's not the the or the the bands of his,, and thetinge of urban right and a the much of the back, thescreen personas..
Original IDs: [101, 1012, 1012, 1012, 13897, 1010, 9680, 3340, 1998, 7415, 11432, 10007, 2037, 9200, 4641, 1999, 1037, 19723, 12514, 18557, 1997, 21014, 4808, 2008, 3957, 12077, 4182, 2000, 2019, 9832, 1010, 2021, 5622, 2912, 3468, 1010, 5394, 1012, 1005, 102]
Predicted IDs: [101, 1996, 2009, 1005, 1055, 2025, 1996, 1996, 2030, 1996, 1996, 4996, 1997, 2010, 1010, 1010, 1998, 1996, 3436, 2063, 1997, 3923, 2157, 1998, 1037, 1996, 2172, 1997, 1996, 2067, 1010, 1996, 18182, 16115, 2015, 1012, 102, 102, 1012, 102]
BLEU Score: 0.1871
Example 3 ---
Original text: add yet another hat to a talented head, clooney's a good director.
Reconstructed text: a a bad much line the superficialfolding the cr sandn's second movie material at all
Original IDs: [101, 5587, 2664, 2178, 6045, 2000, 1037, 10904, 2132, 1010, 18856, 7828, 3240, 1005, 1055, 1037, 2204, 2472, 1012, 102]
Predicted IDs: [101, 1037, 1037, 2919, 2172, 2240, 1996, 23105, 21508, 1996, 13675, 5472, 2078, 1005, 1055, 2117, 3185, 3430, 2012, 2035]
BLEU Score: 0.1875

 --- Epoch 181
Task: Classification | Acc: 98.19% | Avg Loss: 0.0524
Task: Reconstruction | Avg Loss: 2.1523 
MoE Balancing Loss: 10427.6624
Mutual Information | Avg Loss: -0.00406
Total Loss: 3.0935
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330596.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329993.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330250.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330111.66
Time elapsed: 5:32:44.108739

 --- Epoch 182
Task: Classification | Acc: 98.13% | Avg Loss: 0.0539
Task: Reconstruction | Avg Loss: 2.1499 
MoE Balancing Loss: 10431.4030
Mutual Information | Avg Loss: -0.00406
Total Loss: 3.1536
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333328.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332337.59
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332520.69
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333120.00
Time elapsed: 5:34:35.496792

 --- Epoch 183
Task: Classification | Acc: 98.01% | Avg Loss: 0.0557
Task: Reconstruction | Avg Loss: 2.1420 
MoE Balancing Loss: 10428.7103
Mutual Information | Avg Loss: -0.00401
Total Loss: 3.1623
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331399.50
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331733.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332208.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331482.91
Time elapsed: 5:36:27.146918

 --- Epoch 184
Task: Classification | Acc: 98.08% | Avg Loss: 0.0533
Task: Reconstruction | Avg Loss: 2.1402 
MoE Balancing Loss: 10429.3010
Mutual Information | Avg Loss: -0.00408
Total Loss: 3.1714
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329063.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329358.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328982.31
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329245.69
Time elapsed: 5:38:18.698213

 --- Epoch 185
Task: Classification | Acc: 98.13% | Avg Loss: 0.0532
Task: Reconstruction | Avg Loss: 2.1381 
MoE Balancing Loss: 10428.4355
Mutual Information | Avg Loss: -0.00394
Total Loss: 3.0134
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331712.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331489.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331427.59
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330844.28
Time elapsed: 5:40:09.060146

 --- Epoch 186
Task: Classification | Acc: 98.16% | Avg Loss: 0.0513
Task: Reconstruction | Avg Loss: 2.1463 
MoE Balancing Loss: 10429.1749
Mutual Information | Avg Loss: -0.00414
Total Loss: 3.1492
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331104.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329440.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330086.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330828.72
Time elapsed: 5:42:00.689095

 --- Epoch 187
Task: Classification | Acc: 98.11% | Avg Loss: 0.0536
Task: Reconstruction | Avg Loss: 2.1229 
MoE Balancing Loss: 10429.1381
Mutual Information | Avg Loss: -0.00416
Total Loss: 3.1737
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331018.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330578.78
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330518.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330520.69
Time elapsed: 5:43:52.442293

 --- Epoch 188
Task: Classification | Acc: 98.18% | Avg Loss: 0.0494
Task: Reconstruction | Avg Loss: 2.1295 
MoE Balancing Loss: 10425.5623
Mutual Information | Avg Loss: -0.00404
Total Loss: 3.1559
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332083.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332210.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.06, 0.05, 0.04, 0.04, 0.04], std: 331918.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332141.84
Time elapsed: 5:45:44.234879

 --- Epoch 189
Task: Classification | Acc: 98.14% | Avg Loss: 0.0521
Task: Reconstruction | Avg Loss: 2.1258 
MoE Balancing Loss: 10428.8461
Mutual Information | Avg Loss: -0.00415
Total Loss: 3.0801
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330106.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330011.88
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330285.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330346.41
Time elapsed: 5:47:35.153040

 --- Epoch 190
Task: Classification | Acc: 98.21% | Avg Loss: 0.0522
Task: Reconstruction | Avg Loss: 2.1161 
MoE Balancing Loss: 10427.3299
Mutual Information | Avg Loss: -0.00415
Total Loss: 3.1692
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330895.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330389.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330886.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330796.31
Time elapsed: 5:49:26.790973
Example 1 ---
Original text: this is a shameless sham, calculated to cash in on the popularity of its stars.
Reconstructed text: one be a futuret for the way of vulgar writing than the edge of the delivery.
Original IDs: [101, 2023, 2003, 1037, 9467, 3238, 25850, 1010, 10174, 2000, 5356, 1999, 2006, 1996, 6217, 1997, 2049, 3340, 1012, 102]
Predicted IDs: [101, 2028, 2022, 1037, 2925, 2102, 2005, 1996, 2126, 1997, 29364, 3015, 2084, 1996, 3341, 1997, 1996, 6959, 1012, 102]
BLEU Score: 0.2353
Example 2 ---
Original text: the film tunes into a grief that could lead a man across centuries.
Reconstructed text: the why anyone for many two we can strike the film in free
Original IDs: [101, 1996, 2143, 13281, 2046, 1037, 9940, 2008, 2071, 2599, 1037, 2158, 2408, 4693, 1012, 102]
Predicted IDs: [101, 1996, 2339, 3087, 2005, 2116, 2048, 2057, 2064, 4894, 1996, 2143, 1999, 2489, 102, 102]
BLEU Score: 0.1425
Example 3 ---
Original text: one of the smartest takes on singles culture i've seen in a long time.
Reconstructed text: one of the most notable nomination in a long i've seen in the while.
Original IDs: [101, 2028, 1997, 1996, 6047, 4355, 3138, 2006, 3895, 3226, 1045, 1005, 2310, 2464, 1999, 1037, 2146, 2051, 1012, 102]
Predicted IDs: [101, 2028, 1997, 1996, 2087, 3862, 6488, 1999, 1037, 2146, 1045, 1005, 2310, 2464, 1999, 1996, 2096, 1012, 102, 102]
BLEU Score: 0.6250

 --- Epoch 191
Task: Classification | Acc: 98.03% | Avg Loss: 0.0517
Task: Reconstruction | Avg Loss: 2.1119 
MoE Balancing Loss: 10426.6326
Mutual Information | Avg Loss: -0.00419
Total Loss: 3.1311
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330080.47
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330205.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330338.09
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330409.16
Time elapsed: 5:51:18.369098

 --- Epoch 192
Task: Classification | Acc: 98.06% | Avg Loss: 0.0537
Task: Reconstruction | Avg Loss: 2.1227 
MoE Balancing Loss: 10426.4504
Mutual Information | Avg Loss: -0.00421
Total Loss: 3.1648
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330693.56
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329855.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.06, 0.05, 0.04, 0.04, 0.04], std: 330476.56
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330345.78
Time elapsed: 5:53:10.078098

 --- Epoch 193
Task: Classification | Acc: 98.03% | Avg Loss: 0.0545
Task: Reconstruction | Avg Loss: 2.1109 
MoE Balancing Loss: 10428.6570
Mutual Information | Avg Loss: -0.00424
Total Loss: 3.1280
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331172.50
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330684.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330939.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330692.03
Time elapsed: 5:55:01.843410

 --- Epoch 194
Task: Classification | Acc: 98.20% | Avg Loss: 0.0501
Task: Reconstruction | Avg Loss: 2.1077 
MoE Balancing Loss: 10427.3070
Mutual Information | Avg Loss: -0.00415
Total Loss: 3.1639
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331310.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331409.00
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331946.88
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331299.41
Time elapsed: 5:56:53.818619

 --- Epoch 195
Task: Classification | Acc: 98.14% | Avg Loss: 0.0502
Task: Reconstruction | Avg Loss: 2.1127 
MoE Balancing Loss: 10427.4391
Mutual Information | Avg Loss: -0.00417
Total Loss: 3.1546
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330852.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331107.19
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331049.88
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331324.09
Time elapsed: 5:58:45.705343

 --- Epoch 196
Task: Classification | Acc: 98.21% | Avg Loss: 0.0506
Task: Reconstruction | Avg Loss: 2.1003 
MoE Balancing Loss: 10427.7743
Mutual Information | Avg Loss: -0.00417
Total Loss: 3.1523
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331849.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331508.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331986.44
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331653.62
Time elapsed: 6:00:37.635219

 --- Epoch 197
Task: Classification | Acc: 98.10% | Avg Loss: 0.0527
Task: Reconstruction | Avg Loss: 2.0966 
MoE Balancing Loss: 10428.0593
Mutual Information | Avg Loss: -0.00419
Total Loss: 3.1397
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330603.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330661.25
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331642.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330951.50
Time elapsed: 6:02:29.755944

 --- Epoch 198
Task: Classification | Acc: 98.05% | Avg Loss: 0.0529
Task: Reconstruction | Avg Loss: 2.0960 
MoE Balancing Loss: 10426.7813
Mutual Information | Avg Loss: -0.00420
Total Loss: 3.1043
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331924.16
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332298.84
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332472.78
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332876.12
Time elapsed: 6:04:21.188094

 --- Epoch 199
Task: Classification | Acc: 98.20% | Avg Loss: 0.0512
Task: Reconstruction | Avg Loss: 2.0800 
MoE Balancing Loss: 10426.5269
Mutual Information | Avg Loss: -0.00410
Total Loss: 3.1041
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331326.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332052.75
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332126.22
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332275.91
Time elapsed: 6:06:12.661815

 --- Epoch 200
Task: Classification | Acc: 98.33% | Avg Loss: 0.0477
Task: Reconstruction | Avg Loss: 2.0963 
MoE Balancing Loss: 10427.8682
Mutual Information | Avg Loss: -0.00414
Total Loss: 3.1375
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330675.56
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330648.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330869.22
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329947.31
Time elapsed: 6:08:04.307196
Example 1 ---
Original text: every dance becomes about seduction, where backstabbing and betrayals are celebrated, and sex is currency.
Reconstructed text: the shot, surprisingfelt, more li smartous and sick sublimely amusing, and wicked and
Original IDs: [101, 2296, 3153, 4150, 2055, 26962, 1010, 2073, 10457, 2696, 23200, 1998, 14583, 2015, 2024, 6334, 1010, 1998, 3348, 2003, 9598, 1012, 102]
Predicted IDs: [101, 1996, 2915, 1010, 11341, 26675, 1010, 2062, 5622, 6047, 3560, 1998, 5305, 28341, 2135, 19142, 1010, 1998, 102, 10433, 102, 102, 1998]
BLEU Score: 0.2206
Example 2 ---
Original text: turns potentially forgettable formula into something strangely diverting.
Reconstructed text: the a women glossypes like you sears,
Original IDs: [101, 4332, 9280, 5293, 10880, 5675, 2046, 2242, 13939, 27345, 2075, 1012, 102]
Predicted IDs: [101, 1996, 1037, 2308, 19504, 10374, 2066, 2017, 1055, 14644, 2015, 1010, 102]
BLEU Score: 0.0000
Example 3 ---
Original text: villeneuve spends too much time wallowing in bibi's generic angst ( there are a lot of shots of her gazing out windows ).
Reconstructed text: the limited's good bravery about the for the film's reporters sakecating ar goes to his essence of art, awareness the. of own.
Original IDs: [101, 20184, 28104, 15970, 2205, 2172, 2051, 2813, 14138, 1999, 12170, 5638, 1005, 1055, 12391, 17076, 3367, 1006, 2045, 2024, 1037, 2843, 1997, 7171, 1997, 2014, 16448, 2041, 3645, 1007, 1012, 102]
Predicted IDs: [101, 1996, 3132, 1005, 1055, 2204, 16534, 2055, 1996, 2005, 1996, 2143, 1005, 1055, 12060, 8739, 18252, 12098, 3632, 2000, 2010, 11305, 1997, 2396, 1010, 7073, 1996, 1012, 1997, 102, 2219, 1012]
BLEU Score: 0.1481

 --- Epoch 201
Task: Classification | Acc: 98.12% | Avg Loss: 0.0531
Task: Reconstruction | Avg Loss: 2.0811 
MoE Balancing Loss: 10426.2807
Mutual Information | Avg Loss: -0.00418
Total Loss: 3.1510
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331197.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330553.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331132.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331281.41
Time elapsed: 6:09:56.184594

 --- Epoch 202
Task: Classification | Acc: 98.17% | Avg Loss: 0.0513
Task: Reconstruction | Avg Loss: 2.0804 
MoE Balancing Loss: 10429.0790
Mutual Information | Avg Loss: -0.00417
Total Loss: 3.0733
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332199.00
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331625.28
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332176.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332099.91
Time elapsed: 6:11:47.606070

 --- Epoch 203
Task: Classification | Acc: 98.09% | Avg Loss: 0.0514
Task: Reconstruction | Avg Loss: 2.0938 
MoE Balancing Loss: 10427.4750
Mutual Information | Avg Loss: -0.00419
Total Loss: 3.1065
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332690.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332735.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333209.81
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332903.56
Time elapsed: 6:13:39.242935

 --- Epoch 204
Task: Classification | Acc: 98.30% | Avg Loss: 0.0497
Task: Reconstruction | Avg Loss: 2.0838 
MoE Balancing Loss: 10427.9078
Mutual Information | Avg Loss: -0.00420
Total Loss: 3.1625
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331065.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332200.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332212.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332568.91
Time elapsed: 6:15:31.195306

 --- Epoch 205
Task: Classification | Acc: 98.34% | Avg Loss: 0.0481
Task: Reconstruction | Avg Loss: 2.0704 
MoE Balancing Loss: 10428.2767
Mutual Information | Avg Loss: -0.00415
Total Loss: 3.0938
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332796.75
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331203.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331562.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331031.19
Time elapsed: 6:17:22.630822

 --- Epoch 206
Task: Classification | Acc: 98.21% | Avg Loss: 0.0502
Task: Reconstruction | Avg Loss: 2.0722 
MoE Balancing Loss: 10426.8401
Mutual Information | Avg Loss: -0.00425
Total Loss: 3.0753
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 328746.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329736.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.06, 0.05, 0.04, 0.04, 0.04], std: 330027.53
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330354.34
Time elapsed: 6:19:13.870505

 --- Epoch 207
Task: Classification | Acc: 98.22% | Avg Loss: 0.0496
Task: Reconstruction | Avg Loss: 2.0699 
MoE Balancing Loss: 10428.7712
Mutual Information | Avg Loss: -0.00425
Total Loss: 3.1356
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330955.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331157.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332007.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331369.97
Time elapsed: 6:21:05.520482

 --- Epoch 208
Task: Classification | Acc: 98.10% | Avg Loss: 0.0538
Task: Reconstruction | Avg Loss: 2.0512 
MoE Balancing Loss: 10427.1744
Mutual Information | Avg Loss: -0.00420
Total Loss: 3.1130
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330269.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332499.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331543.75
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331815.69
Time elapsed: 6:22:57.148394

 --- Epoch 209
Task: Classification | Acc: 97.96% | Avg Loss: 0.0535
Task: Reconstruction | Avg Loss: 2.0564 
MoE Balancing Loss: 10430.7339
Mutual Information | Avg Loss: -0.00413
Total Loss: 3.1207
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333166.31
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332933.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333667.22
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332961.69
Time elapsed: 6:24:48.945353

 --- Epoch 210
Task: Classification | Acc: 98.39% | Avg Loss: 0.0471
Task: Reconstruction | Avg Loss: 2.0513 
MoE Balancing Loss: 10426.9167
Mutual Information | Avg Loss: -0.00432
Total Loss: 3.1275
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331885.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331232.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331344.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331558.91
Time elapsed: 6:26:40.610107
Example 1 ---
Original text: far more imaginative and ambitious than the trivial, cash - in features nickelodeon has made from its other animated tv series.
Reconstructed text: a the, ofies of a,, crowd - fisted goalsoons quickly drawn into a powerful film
Original IDs: [101, 2521, 2062, 28575, 1998, 12479, 2084, 1996, 20610, 1010, 5356, 1011, 1999, 2838, 20814, 2038, 2081, 2013, 2049, 2060, 6579, 2694, 2186, 1012, 102]
Predicted IDs: [101, 1037, 1996, 1010, 1997, 3111, 1997, 1037, 1010, 1010, 4306, 1011, 28273, 3289, 27174, 2855, 4567, 2046, 1037, 3928, 2143, 102, 102, 102, 102]
BLEU Score: 0.1262
Example 2 ---
Original text: it gets onto the screen just about as much of the novella as one could reasonably expect, and is engrossing and moving in its own right.
Reconstructed text: it less that a a was co - out of the romantic apparatus that might is nowhere fans,, unventing and ar asrar freshty.
Original IDs: [101, 2009, 4152, 3031, 1996, 3898, 2074, 2055, 2004, 2172, 1997, 1996, 20674, 2004, 2028, 2071, 16286, 5987, 1010, 1998, 2003, 25540, 25725, 2075, 1998, 3048, 1999, 2049, 2219, 2157, 1012, 102]
Predicted IDs: [101, 2009, 2625, 2008, 1037, 1037, 2001, 2522, 1011, 2041, 1997, 1996, 6298, 14709, 2008, 2453, 2003, 7880, 4599, 1010, 1010, 4895, 15338, 2075, 1998, 12098, 2004, 19848, 4840, 3723, 1012, 102]
BLEU Score: 0.2493
Example 3 ---
Original text: the film's tone and pacing are off almost from the get - go.
Reconstructed text: , it's not (ing and barbed enough and a self - fashioned,
Original IDs: [101, 1996, 2143, 1005, 1055, 4309, 1998, 15732, 2024, 2125, 2471, 2013, 1996, 2131, 1011, 2175, 1012, 102]
Predicted IDs: [101, 1010, 2009, 1005, 1055, 2025, 1006, 2075, 1998, 25007, 2438, 1998, 1037, 2969, 1011, 13405, 1010, 102]
BLEU Score: 0.2000

 --- Epoch 211
Task: Classification | Acc: 98.21% | Avg Loss: 0.0516
Task: Reconstruction | Avg Loss: 2.0664 
MoE Balancing Loss: 10426.8071
Mutual Information | Avg Loss: -0.00424
Total Loss: 3.1077
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329817.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330366.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330596.41
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330649.00
Time elapsed: 6:28:32.368622

 --- Epoch 212
Task: Classification | Acc: 98.21% | Avg Loss: 0.0505
Task: Reconstruction | Avg Loss: 2.0590 
MoE Balancing Loss: 10428.0981
Mutual Information | Avg Loss: -0.00420
Total Loss: 3.1079
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332493.94
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332002.09
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332959.50
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331823.59
Time elapsed: 6:30:23.996635

 --- Epoch 213
Task: Classification | Acc: 98.29% | Avg Loss: 0.0463
Task: Reconstruction | Avg Loss: 2.0426 
MoE Balancing Loss: 10428.4642
Mutual Information | Avg Loss: -0.00426
Total Loss: 3.0895
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330392.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330805.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330884.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331246.16
Time elapsed: 6:32:15.378010

 --- Epoch 214
Task: Classification | Acc: 98.35% | Avg Loss: 0.0480
Task: Reconstruction | Avg Loss: 2.0459 
MoE Balancing Loss: 10426.9810
Mutual Information | Avg Loss: -0.00416
Total Loss: 3.0585
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333538.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332411.25
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332748.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332426.03
Time elapsed: 6:34:06.856568

 --- Epoch 215
Task: Classification | Acc: 98.31% | Avg Loss: 0.0472
Task: Reconstruction | Avg Loss: 2.0643 
MoE Balancing Loss: 10424.0272
Mutual Information | Avg Loss: -0.00420
Total Loss: 3.1043
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330286.84
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331059.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330614.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330925.75
Time elapsed: 6:35:58.456686

 --- Epoch 216
Task: Classification | Acc: 98.23% | Avg Loss: 0.0485
Task: Reconstruction | Avg Loss: 2.0396 
MoE Balancing Loss: 10428.2271
Mutual Information | Avg Loss: -0.00427
Total Loss: 3.0813
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329917.38
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329881.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329968.22
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330339.44
Time elapsed: 6:37:49.905532

 --- Epoch 217
Task: Classification | Acc: 98.27% | Avg Loss: 0.0484
Task: Reconstruction | Avg Loss: 2.0483 
MoE Balancing Loss: 10426.3935
Mutual Information | Avg Loss: -0.00431
Total Loss: 3.0772
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330440.47
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331009.50
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331535.94
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331786.25
Time elapsed: 6:39:41.388558

 --- Epoch 218
Task: Classification | Acc: 98.22% | Avg Loss: 0.0506
Task: Reconstruction | Avg Loss: 2.0365 
MoE Balancing Loss: 10429.2580
Mutual Information | Avg Loss: -0.00430
Total Loss: 3.1259
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329757.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329819.78
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.06, 0.05, 0.04, 0.04, 0.04], std: 329760.44
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329863.91
Time elapsed: 6:41:32.918137

 --- Epoch 219
Task: Classification | Acc: 98.24% | Avg Loss: 0.0489
Task: Reconstruction | Avg Loss: 2.0423 
MoE Balancing Loss: 10427.4869
Mutual Information | Avg Loss: -0.00429
Total Loss: 3.1128
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331363.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331459.25
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331006.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331510.09
Time elapsed: 6:43:24.725349

 --- Epoch 220
Task: Classification | Acc: 98.11% | Avg Loss: 0.0526
Task: Reconstruction | Avg Loss: 2.0356 
MoE Balancing Loss: 10427.2745
Mutual Information | Avg Loss: -0.00436
Total Loss: 3.0841
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330528.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330372.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331188.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331178.19
Time elapsed: 6:45:16.416483
Example 1 ---
Original text: a misogynistic piece of filth that attempts to pass itself off as hip, young adult entertainment.
Reconstructed text: the chance chance, the close of its taking that us to taking, away in passion, b forget story and history
Original IDs: [101, 1037, 28616, 15707, 26942, 2594, 3538, 1997, 10882, 24658, 2008, 4740, 2000, 3413, 2993, 2125, 2004, 5099, 1010, 2402, 4639, 4024, 1012, 102]
Predicted IDs: [101, 1996, 3382, 3382, 1010, 1996, 2485, 1997, 2049, 2635, 2008, 2149, 2000, 2635, 1010, 2185, 1999, 6896, 1010, 1038, 5293, 2466, 1998, 2381]
BLEU Score: 0.1739
Example 2 ---
Original text: liotta put on 30 pounds for the role, and has completely transformed himself from his smooth, goodfellas image.
Reconstructed text: is the intelligence that the fireworks do by the actress, but the he only come in a flat and beautiful storylinequent note
Original IDs: [101, 5622, 14517, 2050, 2404, 2006, 2382, 7038, 2005, 1996, 2535, 1010, 1998, 2038, 3294, 8590, 2370, 2013, 2010, 5744, 1010, 2204, 23510, 3022, 3746, 1012, 102]
Predicted IDs: [101, 2003, 1996, 4454, 2008, 1996, 16080, 2079, 2011, 1996, 3883, 1010, 2021, 1996, 2002, 2069, 2272, 1999, 1037, 4257, 1998, 3376, 9994, 15417, 3602, 102, 102]
BLEU Score: 0.1304
Example 3 ---
Original text: the weight of the piece, the unerring professionalism of the chilly production, and the fascination embedded in the lurid topic prove recommendation enough.
Reconstructed text: is tod the love and to theiveive themes of the rowdy and, and it up in a cly love.
Original IDs: [101, 1996, 3635, 1997, 1996, 3538, 1010, 1996, 16655, 18807, 2658, 2964, 1997, 1996, 24222, 2537, 1010, 1998, 1996, 18987, 11157, 1999, 1996, 11320, 14615, 8476, 6011, 12832, 2438, 1012, 102]
Predicted IDs: [101, 2003, 2000, 2094, 1996, 2293, 1998, 2000, 1996, 3512, 3512, 6991, 1997, 1996, 5216, 5149, 1998, 1010, 1998, 2009, 2039, 1999, 1037, 18856, 2100, 2293, 102, 102, 102, 1012, 102]
BLEU Score: 0.2627

 --- Epoch 221
Task: Classification | Acc: 98.28% | Avg Loss: 0.0479
Task: Reconstruction | Avg Loss: 2.0229 
MoE Balancing Loss: 10426.9392
Mutual Information | Avg Loss: -0.00440
Total Loss: 3.1612
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332383.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332391.97
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331933.66
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331960.91
Time elapsed: 6:47:08.536900

 --- Epoch 222
Task: Classification | Acc: 98.28% | Avg Loss: 0.0483
Task: Reconstruction | Avg Loss: 2.0406 
MoE Balancing Loss: 10429.7697
Mutual Information | Avg Loss: -0.00423
Total Loss: 3.0294
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332288.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331067.03
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330772.91
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331150.56
Time elapsed: 6:48:59.872872

 --- Epoch 223
Task: Classification | Acc: 98.35% | Avg Loss: 0.0463
Task: Reconstruction | Avg Loss: 2.0325 
MoE Balancing Loss: 10430.0232
Mutual Information | Avg Loss: -0.00433
Total Loss: 3.1669
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332537.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331117.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332447.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331756.50
Time elapsed: 6:50:52.037027

 --- Epoch 224
Task: Classification | Acc: 98.20% | Avg Loss: 0.0491
Task: Reconstruction | Avg Loss: 2.0346 
MoE Balancing Loss: 10430.8399
Mutual Information | Avg Loss: -0.00427
Total Loss: 3.1287
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329208.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329975.12
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330462.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330776.94
Time elapsed: 6:52:44.078496

 --- Epoch 225
Task: Classification | Acc: 98.30% | Avg Loss: 0.0463
Task: Reconstruction | Avg Loss: 2.0118 
MoE Balancing Loss: 10428.1202
Mutual Information | Avg Loss: -0.00435
Total Loss: 3.0991
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330132.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330486.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329954.31
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330181.47
Time elapsed: 6:54:35.864649

 --- Epoch 226
Task: Classification | Acc: 98.31% | Avg Loss: 0.0461
Task: Reconstruction | Avg Loss: 2.0133 
MoE Balancing Loss: 10429.9110
Mutual Information | Avg Loss: -0.00422
Total Loss: 3.0567
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331502.59
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331617.22
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331985.34
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332456.22
Time elapsed: 6:56:27.513981

 --- Epoch 227
Task: Classification | Acc: 98.35% | Avg Loss: 0.0457
Task: Reconstruction | Avg Loss: 2.0057 
MoE Balancing Loss: 10426.7337
Mutual Information | Avg Loss: -0.00422
Total Loss: 3.0707
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329964.06
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329268.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329714.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329665.34
Time elapsed: 6:58:18.939054

 --- Epoch 228
Task: Classification | Acc: 98.38% | Avg Loss: 0.0463
Task: Reconstruction | Avg Loss: 2.0315 
MoE Balancing Loss: 10427.3931
Mutual Information | Avg Loss: -0.00424
Total Loss: 3.0424
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331927.88
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332140.31
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331435.69
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331268.06
Time elapsed: 7:00:10.588385

 --- Epoch 229
Task: Classification | Acc: 98.40% | Avg Loss: 0.0460
Task: Reconstruction | Avg Loss: 2.0344 
MoE Balancing Loss: 10425.7572
Mutual Information | Avg Loss: -0.00423
Total Loss: 3.0019
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331588.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331901.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332056.88
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332440.12
Time elapsed: 7:02:01.943479

 --- Epoch 230
Task: Classification | Acc: 98.31% | Avg Loss: 0.0495
Task: Reconstruction | Avg Loss: 2.0126 
MoE Balancing Loss: 10425.3626
Mutual Information | Avg Loss: -0.00438
Total Loss: 3.1412
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329261.53
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328907.31
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329481.16
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330153.88
Time elapsed: 7:03:54.349749
Example 1 ---
Original text: paid in full is so stale, in fact, that its most vibrant scene is one that uses clips from brian de palma's scarface.
Reconstructed text: the halfa award ) award half actresses place help as a good gr images is, haunting the film in ka wiseor's life intense.
Original IDs: [101, 3825, 1999, 2440, 2003, 2061, 26729, 1010, 1999, 2755, 1010, 2008, 2049, 2087, 17026, 3496, 2003, 2028, 2008, 3594, 15281, 2013, 4422, 2139, 23985, 1005, 1055, 18982, 10732, 1012, 102]
Predicted IDs: [101, 1996, 2431, 2050, 2400, 1007, 2400, 2431, 19910, 2173, 2393, 2004, 1037, 2204, 24665, 4871, 2003, 1010, 20161, 1996, 2143, 1999, 10556, 7968, 2953, 1005, 1055, 2166, 6387, 1012, 102]
BLEU Score: 0.1851
Example 2 ---
Original text: the best film about baseball to hit theaters since field of dreams.
Reconstructed text: spy movie just is slight to induce animation than fright drama.
Original IDs: [101, 1996, 2190, 2143, 2055, 3598, 2000, 2718, 12370, 2144, 2492, 1997, 5544, 1012, 102]
Predicted IDs: [101, 8645, 3185, 2074, 2003, 7263, 2000, 19653, 7284, 2084, 25966, 3689, 1012, 102, 102]
BLEU Score: 0.1533
Example 3 ---
Original text: the experience of going to a film festival is a rewarding one ; the experiencing of sampling one through this movie is not.
Reconstructed text: the ( of those of jar, and than the moviesless.... the better better for this independent was..
Original IDs: [101, 1996, 3325, 1997, 2183, 2000, 1037, 2143, 2782, 2003, 1037, 10377, 2075, 2028, 1025, 1996, 13417, 1997, 16227, 2028, 2083, 2023, 3185, 2003, 2025, 1012, 102]
Predicted IDs: [101, 1996, 1006, 1997, 2216, 1997, 15723, 1010, 1998, 2084, 1996, 5691, 3238, 1012, 1012, 1012, 1012, 1996, 2488, 2488, 2005, 2023, 2981, 2001, 1012, 1012, 102]
BLEU Score: 0.2047

 --- Epoch 231
Task: Classification | Acc: 98.36% | Avg Loss: 0.0466
Task: Reconstruction | Avg Loss: 1.9979 
MoE Balancing Loss: 10426.7703
Mutual Information | Avg Loss: -0.00430
Total Loss: 3.0739
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330719.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330809.16
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330707.41
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330607.28
Time elapsed: 7:05:46.682291

 --- Epoch 232
Task: Classification | Acc: 98.32% | Avg Loss: 0.0478
Task: Reconstruction | Avg Loss: 2.0118 
MoE Balancing Loss: 10427.8766
Mutual Information | Avg Loss: -0.00444
Total Loss: 3.0691
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331079.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330458.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330831.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330699.03
Time elapsed: 7:07:38.672059

 --- Epoch 233
Task: Classification | Acc: 98.31% | Avg Loss: 0.0486
Task: Reconstruction | Avg Loss: 2.0140 
MoE Balancing Loss: 10425.9283
Mutual Information | Avg Loss: -0.00435
Total Loss: 3.1084
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329826.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330266.56
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330783.88
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330572.44
Time elapsed: 7:09:31.231557

 --- Epoch 234
Task: Classification | Acc: 98.21% | Avg Loss: 0.0505
Task: Reconstruction | Avg Loss: 2.0022 
MoE Balancing Loss: 10426.3053
Mutual Information | Avg Loss: -0.00444
Total Loss: 3.0691
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331608.91
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331735.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332027.88
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331793.62
Time elapsed: 7:11:23.558786

 --- Epoch 235
Task: Classification | Acc: 98.29% | Avg Loss: 0.0484
Task: Reconstruction | Avg Loss: 1.9779 
MoE Balancing Loss: 10429.2754
Mutual Information | Avg Loss: -0.00438
Total Loss: 3.0826
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 334000.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333709.16
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 333407.62
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333468.88
Time elapsed: 7:13:15.857350

 --- Epoch 236
Task: Classification | Acc: 98.34% | Avg Loss: 0.0460
Task: Reconstruction | Avg Loss: 1.9958 
MoE Balancing Loss: 10424.6827
Mutual Information | Avg Loss: -0.00442
Total Loss: 3.0598
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332005.97
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332438.56
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332499.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332113.53
Time elapsed: 7:15:07.991776

 --- Epoch 237
Task: Classification | Acc: 98.40% | Avg Loss: 0.0462
Task: Reconstruction | Avg Loss: 1.9915 
MoE Balancing Loss: 10429.4845
Mutual Information | Avg Loss: -0.00443
Total Loss: 3.1583
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 331962.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331470.47
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331104.31
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331771.59
Time elapsed: 7:17:00.834266

 --- Epoch 238
Task: Classification | Acc: 98.29% | Avg Loss: 0.0482
Task: Reconstruction | Avg Loss: 1.9955 
MoE Balancing Loss: 10426.0663
Mutual Information | Avg Loss: -0.00445
Total Loss: 3.0644
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330398.62
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331180.81
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331569.44
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331641.34
Time elapsed: 7:18:52.590709

 --- Epoch 239
Task: Classification | Acc: 98.43% | Avg Loss: 0.0456
Task: Reconstruction | Avg Loss: 1.9806 
MoE Balancing Loss: 10427.5652
Mutual Information | Avg Loss: -0.00436
Total Loss: 3.0202
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331101.25
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331238.34
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331095.03
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330705.31
Time elapsed: 7:20:44.067950

 --- Epoch 240
Task: Classification | Acc: 98.31% | Avg Loss: 0.0498
Task: Reconstruction | Avg Loss: 1.9780 
MoE Balancing Loss: 10427.5370
Mutual Information | Avg Loss: -0.00446
Total Loss: 3.0932
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331345.66
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330590.28
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332308.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331874.47
Time elapsed: 7:22:36.041254
Example 1 ---
Original text: looks and feels like a project better suited for the small screen.
Reconstructed text: still is more as a littleera industry as its feature performance
Original IDs: [101, 3504, 1998, 5683, 2066, 1037, 2622, 2488, 10897, 2005, 1996, 2235, 3898, 1012, 102]
Predicted IDs: [101, 2145, 2003, 2062, 2004, 1037, 2210, 6906, 3068, 2004, 2049, 3444, 2836, 102, 102]
BLEU Score: 0.0758
Example 2 ---
Original text: the film's hackneyed message is not helped by the thin characterizations, nonexistent plot and pretentious visual style.
Reconstructed text: ' s in the the de heavy of the exquisite acting, cl computers, inventruous violence and mubas love action
Original IDs: [101, 1996, 2143, 1005, 1055, 28425, 2098, 4471, 2003, 2025, 3271, 2011, 1996, 4857, 23191, 2015, 1010, 3904, 9048, 16173, 2102, 5436, 1998, 3653, 6528, 20771, 5107, 2806, 1012, 102]
Predicted IDs: [101, 1005, 1055, 1999, 1996, 1996, 2139, 3082, 1997, 1996, 19401, 3772, 1010, 18856, 3274, 2015, 1010, 1999, 15338, 6820, 3560, 4808, 1998, 14163, 22083, 2293, 2895, 102, 102, 102]
BLEU Score: 0.1905
Example 3 ---
Original text: a solid film... but more conscientious than it is truly stirring.
Reconstructed text: the mu direction... and often shakesmstb reachhedly floorhunter,.
Original IDs: [101, 1037, 5024, 2143, 1012, 1012, 1012, 2021, 2062, 9530, 11020, 11638, 6313, 2084, 2009, 2003, 5621, 18385, 1012, 102]
Predicted IDs: [101, 1996, 14163, 3257, 1012, 1012, 1012, 1998, 2411, 10854, 2213, 3367, 2497, 3362, 9072, 2135, 2723, 25629, 1010, 1012]
BLEU Score: 0.1516

 --- Epoch 241
Task: Classification | Acc: 98.27% | Avg Loss: 0.0477
Task: Reconstruction | Avg Loss: 1.9863 
MoE Balancing Loss: 10426.5718
Mutual Information | Avg Loss: -0.00436
Total Loss: 3.0643
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331535.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331989.66
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332460.00
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331770.81
Time elapsed: 7:24:28.030175

 --- Epoch 242
Task: Classification | Acc: 98.40% | Avg Loss: 0.0464
Task: Reconstruction | Avg Loss: 1.9848 
MoE Balancing Loss: 10430.9588
Mutual Information | Avg Loss: -0.00451
Total Loss: 3.1211
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 329154.28
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328501.72
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 328825.19
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329894.53
Time elapsed: 7:26:20.555548

 --- Epoch 243
Task: Classification | Acc: 98.40% | Avg Loss: 0.0450
Task: Reconstruction | Avg Loss: 1.9740 
MoE Balancing Loss: 10426.7895
Mutual Information | Avg Loss: -0.00435
Total Loss: 3.0715
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330641.09
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331149.44
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330716.25
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330884.53
Time elapsed: 7:28:12.429442

 --- Epoch 244
Task: Classification | Acc: 98.38% | Avg Loss: 0.0453
Task: Reconstruction | Avg Loss: 1.9771 
MoE Balancing Loss: 10426.6149
Mutual Information | Avg Loss: -0.00437
Total Loss: 2.9446
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331431.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331015.91
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331024.12
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330851.22
Time elapsed: 7:30:03.692519

 --- Epoch 245
Task: Classification | Acc: 98.31% | Avg Loss: 0.0480
Task: Reconstruction | Avg Loss: 1.9784 
MoE Balancing Loss: 10428.8121
Mutual Information | Avg Loss: -0.00442
Total Loss: 3.1665
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332755.50
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332833.28
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333033.97
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332650.91
Time elapsed: 7:31:56.626769

 --- Epoch 246
Task: Classification | Acc: 98.24% | Avg Loss: 0.0480
Task: Reconstruction | Avg Loss: 1.9615 
MoE Balancing Loss: 10426.7214
Mutual Information | Avg Loss: -0.00440
Total Loss: 3.0588
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332997.78
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 332231.31
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332925.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 333242.03
Time elapsed: 7:33:48.907139

 --- Epoch 247
Task: Classification | Acc: 98.36% | Avg Loss: 0.0478
Task: Reconstruction | Avg Loss: 1.9763 
MoE Balancing Loss: 10424.9485
Mutual Information | Avg Loss: -0.00444
Total Loss: 3.0984
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330812.22
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330506.38
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331165.84
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330595.47
Time elapsed: 7:35:41.245826

 --- Epoch 248
Task: Classification | Acc: 98.41% | Avg Loss: 0.0458
Task: Reconstruction | Avg Loss: 1.9513 
MoE Balancing Loss: 10427.9947
Mutual Information | Avg Loss: -0.00440
Total Loss: 3.0853
Layer 0:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 334710.12
Layer 1:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332410.94
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331957.28
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331802.88
Time elapsed: 7:37:33.639828

 --- Epoch 249
Task: Classification | Acc: 98.40% | Avg Loss: 0.0435
Task: Reconstruction | Avg Loss: 1.9634 
MoE Balancing Loss: 10433.5517
Mutual Information | Avg Loss: -0.00450
Total Loss: 3.1636
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 331386.81
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 329393.53
Layer 2:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330548.84
Layer 3:
-- Expert usage: [0.21, 0.15, 0.12, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330543.72
Time elapsed: 7:39:26.536685

 --- Epoch 250
Task: Classification | Acc: 98.30% | Avg Loss: 0.0489
Task: Reconstruction | Avg Loss: 1.9472 
MoE Balancing Loss: 10427.8227
Mutual Information | Avg Loss: -0.00445
Total Loss: 3.0482
Layer 0:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 332068.69
Layer 1:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330469.41
Layer 2:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.04, 0.04, 0.04], std: 330797.38
Layer 3:
-- Expert usage: [0.21, 0.15, 0.11, 0.09, 0.08, 0.07, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04], std: 330483.84
Time elapsed: 7:41:18.751880
Example 1 ---
Original text: writer / director joe carnahan's grimy crime drama is a manual of precinct cliches, but it moves fast enough to cover its clunky dialogue and lapses in logic.
Reconstructed text: as the sing sc thevey's boro in, is lacking director of inc st gags, but consciousness never are enough to be their ` aic failure...
Original IDs: [101, 3213, 1013, 2472, 3533, 2482, 15272, 2319, 1005, 1055, 11844, 2100, 4126, 3689, 2003, 1037, 6410, 1997, 18761, 18856, 17322, 2015, 1010, 2021, 2009, 5829, 3435, 2438, 2000, 3104, 2049, 18856, 16814, 2100, 7982, 1998, 10876, 2229, 1999, 7961, 1012, 102]
Predicted IDs: [101, 2004, 1996, 1055, 2075, 8040, 1996, 12417, 1005, 1055, 8945, 3217, 1999, 1010, 2003, 11158, 2472, 1997, 4297, 2358, 18201, 2015, 1010, 2021, 8298, 2196, 2024, 2438, 2000, 2022, 2037, 1036, 1037, 2594, 4945, 1012, 102, 102, 102, 1012, 1012, 102]
BLEU Score: 0.2897
Example 2 ---
Original text: like you couldn't smell this turkey rotting from miles away.
Reconstructed text: like they don't have an percentage bone in day minutes but
Original IDs: [101, 2066, 2017, 2071, 1050, 1005, 1056, 5437, 2023, 4977, 22005, 2013, 2661, 2185, 1012, 102]
Predicted IDs: [101, 2066, 2027, 2079, 1050, 1005, 1056, 2031, 2019, 7017, 5923, 1999, 2154, 2781, 102, 2021]
BLEU Score: 0.1667
Example 3 ---
Original text: a gorgeous, high - spirited musical from india that exquisitely blends music, dance, song, and high drama.
Reconstructed text: 's a well - fashioned drama, of a pitch - flippans, lock, performances,, smokingtic, mu
Original IDs: [101, 1037, 9882, 1010, 2152, 1011, 24462, 3315, 2013, 2634, 2008, 19401, 2135, 12586, 2015, 2189, 1010, 3153, 1010, 2299, 1010, 1998, 2152, 3689, 1012, 102]
Predicted IDs: [101, 1005, 2015, 1037, 2092, 1011, 13405, 3689, 1010, 1997, 1037, 6510, 1011, 11238, 9739, 2015, 1010, 5843, 1010, 4616, 1010, 1010, 9422, 4588, 1010, 14163]
BLEU Score: 0.3178

--- Final Test BLEU Score ---
Avg BLEU Score: 0.1699
